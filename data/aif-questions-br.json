[
  {
    "id": "aif-c01-responsible_ai-001",
    "certification_id": "AIF-C01",
    "domain": "RESPONSIBLE_AI",
    "difficulty": "medium",
    "tier": "FREE",
    "required_selection_count": 1,
    "active": true,
    "question_text": "Um praticante de IA deve escrever um relatório de transparência e explicabilidade para modelos de ML de previsão de demanda trimestral. O que deve ser incluído para atender aos requisitos de explicabilidade para as partes interessadas?",
    "option_a": "Código de treinamento.",
    "option_b": "Gráficos de Dependência Parcial (PDPs).",
    "option_c": "Dados de treinamento de amostra.",
    "option_d": "Tabelas de convergência do modelo.",
    "correct_answers": [
      "B"
    ],
    "explanation_detailed": "Os Gráficos de Dependência Parcial (PDPs) visualizam o efeito marginal de uma ou duas características de entrada na previsão de um modelo, enquanto mediam a influência de outras variáveis. Eles fornecem uma visão da característica para o resultado que as partes interessadas não técnicas podem entender, apoiando a transparência e a interpretabilidade sem divulgar dados proprietários ou detalhes internos complexos. Os PDPs ajudam a revelar monotonicidade, retornos decrescentes e dicas de interação. Na AWS, você pode gerar artefatos de atribuição de características com o Amazon SageMaker Clarify e complementá-los com análises do tipo PDP em notebooks. Juntamente com a documentação da linhagem de dados e métricas de avaliação, os PDPs deixam claro como impulsionadores específicos (preço, sazonalidade, promoções) movem a previsão, o que é central para relatórios responsáveis e gerenciamento de risco do modelo.",
    "incorrect_explanations": {
      "A": "Publicar apenas o código de treinamento bruto não explica como as características individuais influenciam as previsões. Ajuda na reprodutibilidade, mas oferece interpretabilidade limitada no nível das partes interessadas sem visualizações do comportamento do modelo.",
      "C": "Os dados de amostra mostram as entradas, mas não sua contribuição causal ou marginal para os resultados. Podem levantar preocupações com a privacidade e ainda deixam as partes interessadas sem saber como o modelo usa as características.",
      "D": "As tabelas de convergência focam no progresso da otimização ou nas curvas de perda, não no impacto das características. Elas ajudam os engenheiros, mas não se traduzem em explicações amigáveis para as partes interessadas."
    }
  },
  {
    "id": "aif-c01-ai_services-002",
    "certification_id": "AIF-C01",
    "domain": "AI_SERVICES",
    "difficulty": "medium",
    "tier": "FREE",
    "required_selection_count": 1,
    "active": true,
    "question_text": "Um escritório de advocacia deseja uma aplicação de IA usando modelos de linguagem grandes para ler documentos legais e extrair pontos-chave. Qual solução se encaixa?",
    "option_a": "Construir apenas uma automação de reconhecimento de entidade nomeada (NER).",
    "option_b": "Criar um motor de recomendação de conteúdo.",
    "option_c": "Desenvolver um chatbot de sumarização.",
    "option_d": "Desenvolver um sistema de tradução multilíngue.",
    "correct_answers": [
      "C"
    ],
    "explanation_detailed": "Um chatbot de sumarização alimentado por um LLM é projetado para condensar textos longos e técnicos em pontos concisos e salientes, preservando a nuance legal. Usando o Amazon Bedrock, você pode invocar modelos de fundação para realizar sumarização extrativa ou abstrativa e sobrepor a recuperação em repositórios de casos, se necessário. As guardrails podem impor o tom e a política, enquanto o Amazon Kendra ou uma base de conhecimento do Bedrock podem melhorar o embasamento em documentos da empresa. Em comparação com um pipeline de NER puro, a sumarização abrange insights mais amplos, como identificação de questões, decisões e obrigações, não apenas entidades. Isso se alinha com a necessidade da empresa de ler documentos e destacar pontos-chave interativamente, com governança e registro via CloudWatch e log de invocação do Bedrock.",
    "incorrect_explanations": {
      "A": "O NER extrai entidades (nomes, datas, estatutos), mas perde pontos-chave, argumentos ou obrigações de nível superior que a sumarização captura.",
      "B": "Os motores de recomendação classificam ou sugerem conteúdo; eles não leem um documento e extraem seus pontos salientes.",
      "D": "A tradução muda o idioma, não a abstração do conteúdo. Não identifica ideias legais importantes no documento original."
    }
  },
  {
    "id": "aif-c01-ai_fundamentals-003",
    "certification_id": "AIF-C01",
    "domain": "AI_FUNDAMENTALS",
    "difficulty": "medium",
    "tier": "FREE",
    "required_selection_count": 1,
    "active": true,
    "question_text": "Uma empresa precisa classificar genes humanos em 20 categorias e quer documentar como a mecânica interna do modelo afeta os resultados. Qual algoritmo se encaixa melhor?",
    "option_a": "Árvores de Decisão.",
    "option_b": "Regressão Linear.",
    "option_c": "Regressão Logística.",
    "option_d": "Redes Neurais.",
    "correct_answers": [
      "A"
    ],
    "explanation_detailed": "As árvores de decisão fornecem interpretabilidade nativa ao expor as divisões hierárquicas que levam a uma previsão. A condição de cada nó mostra quais características do gene impulsionam o limite de decisão, permitindo explicações diretas e auditáveis, adequadas para contextos regulamentados ou científicos. Em comparação com as redes neurais, as árvores não exigem explicação pós-hoc para entender a mecânica. Elas também escalam naturalmente para problemas de múltiplas classes. Na AWS, você pode treinar modelos baseados em árvores com o Amazon SageMaker (por exemplo, XGBoost ou algoritmos integrados) e usar o SageMaker Clarify para verificações de importância de características e viés. Embora a regressão logística ofereça interpretabilidade, ela assume limites de decisão lineares; os modelos de árvore capturam interações não lineares entre as características do gene, mantendo-se transparentes.",
    "incorrect_explanations": {
      "B": "A regressão linear visa resultados contínuos e assume linearidade, o que é inadequado para saídas categóricas de múltiplas classes e interações complexas.",
      "C": "A regressão logística pode ser interpretável, mas torna-se menos prática com 20 classes e interações de características não lineares comuns em genômica.",
      "D": "As redes neurais podem ser precisas, mas são opacas por padrão; documentar a mecânica interna requer ferramentas e conhecimentos extras."
    }
  },
  {
    "id": "aif-c01-ai_fundamentals-004",
    "certification_id": "AIF-C01",
    "domain": "AI_FUNDAMENTALS",
    "difficulty": "easy",
    "tier": "FREE",
    "required_selection_count": 1,
    "active": true,
    "question_text": "Um classificador de imagens de doenças de plantas precisa de uma métrica que mostre quantas imagens foram classificadas corretamente. Qual métrica deve ser usada?",
    "option_a": "Pontuação R-quadrado.",
    "option_b": "Acurácia.",
    "option_c": "Raiz do Erro Quadrático Médio (RMSE).",
    "option_d": "Taxa de aprendizado.",
    "correct_answers": [
      "B"
    ],
    "explanation_detailed": "A acurácia mede a proporção de previsões corretas sobre todas as previsões, o que responde diretamente a “quantas imagens foram classificadas corretamente”. Para problemas de múltiplas classes com rótulo único e balanceados, a acurácia é uma métrica primária intuitiva. Na AWS, você pode computá-la durante o treinamento do SageMaker e acompanhá-la no CloudWatch ou SageMaker Experiments. Se as classes estiverem desbalanceadas, complemente a acurácia com precisão, recall e F1, e inspecione as matrizes de confusão para entender os erros específicos de cada classe. Métricas como RMSE ou R-quadrado são orientadas para regressão, enquanto a taxa de aprendizado é um hiperparâmetro, não uma métrica de avaliação. Assim, a acurácia é o indicador de nível superior mais claro para a correção de um classificador de doenças de folhas de plantas.",
    "incorrect_explanations": {
      "A": "O R-quadrado quantifica a variância explicada para regressão, não a correção da classificação em rótulos discretos.",
      "C": "O RMSE é uma métrica de erro de regressão; não representa resultados categóricos de acerto/erro.",
      "D": "A taxa de aprendizado é um hiperparâmetro de treinamento, não uma métrica de desempenho."
    }
  },
  {
    "id": "aif-c01-ai_services-005",
    "certification_id": "AIF-C01",
    "domain": "AI_SERVICES",
    "difficulty": "easy",
    "tier": "FREE",
    "required_selection_count": 1,
    "active": true,
    "question_text": "Uma empresa usa um LLM pré-treinado para um chatbot de recomendação de produtos. Eles precisam de saídas curtas em um idioma específico. O que alinha as respostas do LLM com as expectativas?",
    "option_a": "Ajuste de prompt e refinamento de instruções.",
    "option_b": "Escolher um tamanho de LLM diferente.",
    "option_c": "Aumentar a temperatura.",
    "option_d": "Aumentar o Top-K.",
    "correct_answers": [
      "A"
    ],
    "explanation_detailed": "Prompts claros e orientados a restrições direcionam o estilo de geração: idioma alvo, máximo de tokens, formato de lista, tom e exemplos de respostas desejadas. No Amazon Bedrock, você pode especificar mensagens de sistema e de usuário para impor brevidade e idioma, adicionar exemplos de poucas tentativas (few-shot) e definir o máximo de tokens de saída. Temperatura mais baixa e limites de decodificação podem reduzir ainda mais a verbosidade, mas a maior alavanca é a qualidade da instrução. O tamanho do modelo altera a capacidade, não a aderência ao estilo. Top-K e temperatura controlam a aleatoriedade, não a política de alto nível. Combine instruções explícitas com guardrails e parâmetros de invocação para produzir respostas concisas, alinhadas à marca e específicas do idioma de forma confiável, sem treinar novamente o modelo subjacente.",
    "incorrect_explanations": {
      "B": "O tamanho do modelo afeta a capacidade e a latência, não a aderência a restrições estilísticas que você pode impor com instruções claras.",
      "C": "Temperatura mais alta aumenta o risco de aleatoriedade e verbosidade; funciona contra a consistência e a brevidade.",
      "D": "O Top-K ajusta a diversidade de amostragem, não o controle geral do estilo ou as restrições de idioma necessárias."
    }
  },
  {
    "id": "aif-c01-ai_services-006",
    "certification_id": "AIF-C01",
    "domain": "AI_SERVICES",
    "difficulty": "medium",
    "tier": "FREE",
    "required_selection_count": 1,
    "active": true,
    "question_text": "Usando o Amazon SageMaker em produção, as entradas podem ter até 1 GB e o processamento até 1 hora, com latência quase em tempo real necessária. Qual opção de inferência se encaixa?",
    "option_a": "Inferência em tempo real.",
    "option_b": "Inferência sem servidor.",
    "option_c": "Inferência assíncrona.",
    "option_d": "Transformação em lote.",
    "correct_answers": [
      "C"
    ],
    "explanation_detailed": "A Inferência Assíncrona do SageMaker lida com grandes cargas úteis e longos tempos de processamento sem manter uma conexão HTTP aberta. Os clientes enviam solicitações e posteriormente recuperam os resultados de um local de saída do S3, alcançando responsividade quase em tempo real em escala, enquanto desacopla a latência do front-end do processamento do back-end. Os endpoints em tempo real são melhores para cargas úteis de subsegundos. A inferência sem servidor visa tráfego com picos e baixa latência com cargas úteis menores. A transformação em lote é para processamento offline e adiado de grandes conjuntos de dados sem requisito de imediatismo. A Inferência Assíncrona oferece o melhor equilíbrio para grandes entradas, longas janelas de computação, enfileiramento e entrega quase em tempo real.",
    "incorrect_explanations": {
      "A": "Os endpoints em tempo real mantêm as conexões abertas e não são otimizados para cargas úteis muito grandes ou processamento de uma hora.",
      "B": "A inferência sem servidor é adequada para cargas de trabalho de baixa latência e com picos, com cargas úteis menores, não para execuções de 1 GB e uma hora.",
      "D": "A transformação em lote é offline e não foi projetada para responsividade quase em tempo real."
    }
  },
  {
    "id": "aif-c01-ai_fundamentals-007",
    "certification_id": "AIF-C01",
    "domain": "AI_FUNDAMENTALS",
    "difficulty": "easy",
    "tier": "FREE",
    "required_selection_count": 1,
    "active": true,
    "question_text": "Uma equipe quer evitar o treinamento do zero e adaptar modelos pré-treinados e específicos de domínio para tarefas relacionadas. Qual estratégia eles devem usar?",
    "option_a": "Aumentar o número de épocas.",
    "option_b": "Aprendizado por transferência.",
    "option_c": "Diminuir o número de épocas.",
    "option_d": "Aprendizado não supervisionado.",
    "correct_answers": [
      "B"
    ],
    "explanation_detailed": "O aprendizado por transferência parte das representações aprendidas de um modelo pré-treinado e as ajusta em uma tarefa downstream relacionada, reduzindo drasticamente dados, computação e tempo. Isso é eficaz quando os domínios de origem e destino compartilham estrutura (por exemplo, de resumos legais para contratos, de imagens médicas para modalidades relacionadas). Na AWS, use o treinamento do Amazon SageMaker ou os endpoints de ajuste fino do Amazon Bedrock para adaptar modelos de fundação e aproveite os recursos de MLOps — Registro de Modelos, Pipelines — para repetibilidade. Ajuste as taxas de aprendizado, descongele camadas seletivamente e monitore o overfitting. O aprendizado por transferência geralmente supera o treinamento do zero quando os dados rotulados são limitados e acelera a implantação sem sacrificar a precisão.",
    "incorrect_explanations": {
      "A": "Mais épocas em um modelo inicializado aleatoriamente aumenta o custo e o risco de overfitting sem aproveitar o conhecimento prévio.",
      "C": "Menos épocas não permitem o reuso do conhecimento; simplesmente trunca o tempo de treinamento.",
      "D": "O aprendizado não supervisionado encontra estrutura sem rótulos e não adapta diretamente um modelo supervisionado pré-treinado a uma nova tarefa rotulada."
    }
  },
  {
    "id": "aif-c01-ai_services-008",
    "certification_id": "AIF-C01",
    "domain": "AI_SERVICES",
    "difficulty": "medium",
    "tier": "FREE",
    "required_selection_count": 1,
    "active": true,
    "question_text": "Uma solução precisa gerar imagens para óculos de proteção com alta precisão de anotação e risco mínimo de rotulagem incorreta. O que deve ser usado?",
    "option_a": "Validação com intervenção humana (human-in-the-loop) com o Amazon SageMaker Ground Truth Plus.",
    "option_b": "Aumento de dados usando uma base de conhecimento do Amazon Bedrock.",
    "option_c": "Reconhecimento de imagem com o Amazon Rekognition.",
    "option_d": "Sumarização de dados usando o Amazon QuickSight Q.",
    "correct_answers": [
      "A"
    ],
    "explanation_detailed": "Para conjuntos de dados de visão computacional de alto risco, os fluxos de trabalho de anotação com intervenção humana (HITL) reduzem erros. O Amazon SageMaker Ground Truth Plus fornece rotulagem gerenciada com forças de trabalho especializadas, revisão em vários estágios e controles de qualidade integrados. Você pode definir taxonomias de rótulos, estratégias de consenso e processos de auditoria, o que é crucial quando as imagens de equipamentos de segurança exigem caixas delimitadoras ou segmentação precisas. O Rekognition oferece detecção pré-construída, mas não resolve a qualidade da criação do conjunto de dados. Uma base de conhecimento do Bedrock aumenta o contexto do LLM, não os rótulos para imagens. O QuickSight Q suporta perguntas e respostas de BI, não anotação de visão. O Ground Truth Plus oferece controles de precisão rigorosos e métricas necessárias para minimizar o risco de rotulagem incorreta.",
    "incorrect_explanations": {
      "B": "Uma base de conhecimento do Bedrock aumenta o contexto do LLM para recuperação, não a precisão da rotulagem de imagens ou a criação de conjuntos de dados.",
      "C": "O Rekognition detecta objetos e cenas, mas não gerencia fluxos de trabalho de anotação personalizados ou garantia de qualidade de rótulos.",
      "D": "O QuickSight Q é um recurso de linguagem natural de inteligência de negócios, não relacionado à anotação de conjuntos de dados de imagem."
    }
  },
  {
    "id": "aif-c01-ai_services-009",
    "certification_id": "AIF-C01",
    "domain": "AI_SERVICES",
    "difficulty": "medium",
    "tier": "FREE",
    "required_selection_count": 1,
    "active": true,
    "question_text": "Um modelo de fundação do Bedrock precisa acessar objetos criptografados no Amazon S3 que usam SSE-S3. O modelo falha ao acessar o bucket. O que deve ser garantido?",
    "option_a": "A função assumida pelo Amazon Bedrock tem permissões para descriptografar usando a chave de criptografia correta.",
    "option_b": "Definir os buckets do S3 como públicos e habilitar o acesso à internet.",
    "option_c": "Usar engenharia de prompt para instruir o modelo a procurar no S3.",
    "option_d": "Garantir que os dados do S3 não contenham informações sensíveis.",
    "correct_answers": [
      "A"
    ],
    "explanation_detailed": "O acesso a dados criptografados do S3 requer que a função de serviço invocadora tenha as permissões apropriadas para ler e descriptografar os objetos. Com serviços da AWS como o Amazon Bedrock se integrando ao S3, a função do IAM deve incluir as ações necessárias do S3 (por exemplo, s3:GetObject) e permissões de descriptografia consistentes com as configurações de criptografia do bucket. Em ambientes rigidamente controlados, combine isso com endpoints da VPC e políticas de bucket restritivas com escopo para a função. Embora os detalhes do modo de criptografia difiram (KMS vs. gerenciado pelo S3), o princípio governante permanece: conceda à função de serviço os direitos de menor privilégio para ler e descriptografar. Não torne os buckets públicos nem confie em prompts para contornar a autorização.",
    "incorrect_explanations": {
      "B": "O acesso público enfraquece a postura de segurança e viola o menor privilégio sem corrigir as permissões de função ausentes.",
      "C": "As instruções de prompt não podem substituir a autorização do IAM ou do S3; o controle de acesso é imposto pela AWS, não pelo modelo.",
      "D": "A higienização do conteúdo não resolve a falha de autorização ao ler objetos criptografados."
    }
  },
  {
    "id": "aif-c01-ai_services-010",
    "certification_id": "AIF-C01",
    "domain": "AI_SERVICES",
    "difficulty": "medium",
    "tier": "FREE",
    "required_selection_count": 1,
    "active": true,
    "question_text": "Uma empresa construirá um modelo de ML no Amazon SageMaker e precisa compartilhar e gerenciar características entre várias equipes durante o desenvolvimento. O que eles devem usar?",
    "option_a": "Amazon SageMaker Feature Store",
    "option_b": "Amazon SageMaker Data Wrangler",
    "option_c": "Amazon SageMaker Clarify",
    "option_d": "Amazon SageMaker Model Cards",
    "correct_answers": [
      "A"
    ],
    "explanation_detailed": "O Amazon SageMaker Feature Store fornece um repositório centralizado e versionado para características online e offline. Ele permite definições consistentes de características entre as equipes, reduz a distorção entre treinamento e serviço e suporta a recuperação online de baixa latência para inferência. As lojas offline apoiam experimentos e treinamento em lote. A integração com o SageMaker Pipelines e o Model Registry suporta a governança de MLOps. O Data Wrangler foca na preparação e transformação, o Clarify no viés/explicabilidade e os Model Cards na documentação. O uso do Feature Store garante que cientistas de dados e engenheiros reutilizem características aprovadas, melhorem a reprodutibilidade e acelerem a entrega do modelo, mantendo a linhagem e o controle de acesso por meio do IAM e da criptografia em repouso.",
    "incorrect_explanations": {
      "B": "O Data Wrangler simplifica a preparação e a visualização de dados; não fornece um repositório compartilhado e versionado para o reuso de características entre equipes.",
      "C": "O Clarify explica modelos e detecta viés; não gerencia o armazenamento de características e o serviço online.",
      "D": "Os Model Cards documentam os detalhes do modelo para governança; não são um repositório de características."
    }
  },
  {
    "id": "aif-c01-ai_services-011",
    "certification_id": "AIF-C01",
    "domain": "AI_SERVICES",
    "difficulty": "easy",
    "tier": "FREE",
    "required_selection_count": 1,
    "active": true,
    "question_text": "Uma empresa quer IA generativa para aumentar a produtividade do desenvolvedor e usará o Amazon Q Developer. Qual capacidade está alinhada?",
    "option_a": "Criar trechos de código, rastrear referências e gerenciar obrigações de licença de código aberto.",
    "option_b": "Executar uma aplicação sem gerenciar servidores.",
    "option_c": "Habilitar comandos de voz para codificação e pesquisa em linguagem natural.",
    "option_d": "Converter arquivos de áudio em documentos de texto com modelos de ML.",
    "correct_answers": [
      "A"
    ],
    "explanation_detailed": "O Amazon Q Developer é um assistente de IA generativa que acelera tarefas de codificação: elaboração de funções, testes unitários e refatorações, enquanto apresenta referências e considerações de licença de dependências. Ele se integra a IDEs e serviços da AWS para recomendar código e explicações em contexto. A execução sem servidor é um padrão de computação da AWS (por exemplo, Lambda), não uma função de assistente de desenvolvedor. A conversão de fala para texto pertence ao Amazon Transcribe, e os assistentes de voz são capacidades separadas. O valor do Q Developer é a produtividade direcionada na autoria, revisão e compreensão de código, preservando a conformidade ao rastrear licenças de OSS e usar o contexto de repositórios e tíquetes.",
    "incorrect_explanations": {
      "B": "A computação sem servidor (por exemplo, AWS Lambda) aborda o gerenciamento de tempo de execução, não a geração de código e a assistência ao desenvolvedor.",
      "C": "O controle de voz não é a capacidade principal; o Q Developer foca na geração de código, explicações e integração com ferramentas de desenvolvedor.",
      "D": "A transcrição de áudio é o papel do Amazon Transcribe, não a função de produtividade de codificação do Q Developer."
    }
  },
  {
    "id": "aif-c01-ai_services-012",
    "certification_id": "AIF-C01",
    "domain": "AI_SERVICES",
    "difficulty": "medium",
    "tier": "FREE",
    "required_selection_count": 1,
    "active": true,
    "question_text": "Uma instituição financeira usa o Amazon Bedrock dentro de uma VPC sem saída para a internet devido à conformidade. Qual opção permite a conectividade de serviço privado?",
    "option_a": "AWS PrivateLink",
    "option_b": "Amazon Macie",
    "option_c": "Amazon CloudFront",
    "option_d": "Gateway da internet",
    "correct_answers": [
      "A"
    ],
    "explanation_detailed": "O AWS PrivateLink fornece conectividade privada entre VPCs e serviços da AWS suportados sem atravessar a internet pública. Ele estabelece endpoints de VPC de interface para o Bedrock, quando disponíveis, mantendo o tráfego dentro da rede da AWS para atender aos requisitos de conformidade. O Macie é para descoberta de privacidade de dados no S3. O CloudFront é uma CDN que depende de endpoints públicos. Um Gateway da Internet quebraria a política de “sem saída para a internet”. Com o PrivateLink e o IAM restrito, você mantém o acesso de menor privilégio enquanto registra via CloudWatch e CloudTrail. Combine com políticas de endpoint da VPC e endpoints de gateway do S3 para um caminho de interação de dados e modelo totalmente privado.",
    "incorrect_explanations": {
      "B": "O Macie classifica dados sensíveis no S3; não fornece conectividade de rede privada.",
      "C": "O CloudFront acelera a entrega de conteúdo pela internet e não é um mecanismo de link privado.",
      "D": "Um Gateway da Internet permite o acesso à internet, violando a restrição de não saída."
    }
  },
  {
    "id": "aif-c01-ai_fundamentals-013",
    "certification_id": "AIF-C01",
    "domain": "AI_FUNDAMENTALS",
    "difficulty": "easy",
    "tier": "FREE",
    "required_selection_count": 1,
    "active": true,
    "question_text": "Um jogo educativo faz perguntas de probabilidade como tirar uma bola verde de uma jarra com contagens conhecidas. Qual é a abordagem mais simples?",
    "option_a": "Usar aprendizado supervisionado para treinar um modelo de regressão.",
    "option_b": "Usar aprendizado por reforço para retornar a probabilidade.",
    "option_c": "Calcular a probabilidade com regras e cálculos diretos.",
    "option_d": "Usar aprendizado não supervisionado para estimar a densidade.",
    "correct_answers": [
      "C"
    ],
    "explanation_detailed": "Este é um problema de probabilidade de forma fechada: P(verde) = contagem(verde)/contagem(total). Codificar a regra é mais simples, mais barato e perfeitamente preciso, evitando qualquer sobrecarga de ML. Na AWS, essa lógica pode ser executada no AWS Lambda ou dentro de um backend de aplicação sem treinamento ou hospedagem de modelo. O ML é útil quando as relações são desconhecidas ou complexas; aqui, a relação é explícita e determinística. Treinar um modelo para aproximar uma fórmula trivial adiciona custo, latência e risco de erro. Mantenha o ML para problemas onde ele agrega valor e aplique o cálculo direto para tarefas combinatórias ou probabilísticas elementares.",
    "incorrect_explanations": {
      "A": "A regressão aprenderia a aproximar uma fórmula conhecida, adicionando custo de treinamento e possível erro para uma tarefa trivial.",
      "B": "O aprendizado por reforço é para problemas de decisão sequencial com recompensas, não para cálculos de probabilidade estáticos de forma fechada.",
      "D": "A estimativa de densidade não supervisionada é desnecessária quando as contagens são explícitas e formuláricas."
    }
  },
  {
    "id": "aif-c01-ai_fundamentals-014",
    "certification_id": "AIF-C01",
    "domain": "AI_FUNDAMENTALS",
    "difficulty": "easy",
    "tier": "FREE",
    "required_selection_count": 1,
    "active": true,
    "question_text": "Qual métrica avalia a eficiência de tempo de execução dos modelos de IA?",
    "option_a": "Satisfação do cliente (CSAT).",
    "option_b": "Tempo de treinamento por época.",
    "option_c": "Tempo médio de resposta.",
    "option_d": "Número de instâncias de treinamento.",
    "correct_answers": [
      "C"
    ],
    "explanation_detailed": "O tempo médio de resposta (ou latência) mede a rapidez com que um modelo serve previsões, refletindo a eficiência do tempo de execução. Ele é rastreado nos endpoints de inferência (por exemplo, inferência em tempo real ou sem servidor do Amazon SageMaker) e visualizado no CloudWatch. O tempo de treinamento por época é uma métrica de treinamento, não de eficiência de serviço. A contagem de instâncias descreve a capacidade, não a eficiência. O CSAT é uma métrica de resultado de negócios e pode se correlacionar com a latência, mas não a mede. A otimização da latência envolve compressão de modelo, hardware melhor, dimensionamento de lote e políticas de auto scaling, tudo observável por meio de métricas e logs de endpoint.",
    "incorrect_explanations": {
      "A": "O CSAT é uma métrica de percepção e não quantifica diretamente a latência de serviço.",
      "B": "O tempo de época se relaciona à velocidade de treinamento, não à eficiência da inferência de produção.",
      "D": "A contagem de instâncias indica capacidade; a eficiência requer medições de latência e taxa de transferência."
    }
  },
  {
    "id": "aif-c01-ai_services-015",
    "certification_id": "AIF-C01",
    "domain": "AI_SERVICES",
    "difficulty": "easy",
    "tier": "FREE",
    "required_selection_count": 1,
    "active": true,
    "question_text": "Uma aplicação de contact center precisa de insights de conversas com clientes, analisando e extraindo informações-chave do áudio das chamadas. Qual serviço da AWS deve ser usado primeiro?",
    "option_a": "Amazon Lex para construir um bot conversacional.",
    "option_b": "Amazon Transcribe para transcrever as gravações das chamadas.",
    "option_c": "Amazon SageMaker Model Monitor para extrair informações.",
    "option_d": "Amazon Comprehend para criar rótulos de classificação.",
    "correct_answers": [
      "B"
    ],
    "explanation_detailed": "Comece convertendo o áudio em texto usando o Amazon Transcribe, que fornece conversão de fala para texto precisa com carimbos de data/hora, separação de canais e redação de PII. Uma vez que as transcrições estejam disponíveis, você pode aplicar o Amazon Comprehend para sentimento, frases-chave, entidades e classificação personalizada. Para resumos gerados, invoque modelos de fundação via Amazon Bedrock. O Lex é usado para construir bots interativos, não para análise offline de áudio gravado. O Model Monitor rastreia o desvio de dados/qualidade para modelos implantados, não a transcrição básica. Este pipeline — Transcribe e depois NLP/LLM — desbloqueia insights estruturados para QA, conformidade e treinamento.",
    "incorrect_explanations": {
      "A": "O Lex é para construir interfaces conversacionais ao vivo; ele não processa gravações de áudio históricas em texto.",
      "C": "O Model Monitor detecta desvios em modelos implantados; não extrai informações do áudio.",
      "D": "O Comprehend analisa texto; você primeiro precisa de transcrições criadas pelo Transcribe."
    }
  },
  {
    "id": "aif-c01-ai_fundamentals-016",
    "certification_id": "AIF-C01",
    "domain": "AI_FUNDAMENTALS",
    "difficulty": "easy",
    "tier": "FREE",
    "required_selection_count": 1,
    "active": true,
    "question_text": "Uma empresa tem petabytes de dados de clientes não rotulados para segmentar para marketing. Qual abordagem de aprendizado se encaixa?",
    "option_a": "Aprendizado supervisionado",
    "option_b": "Aprendizado não supervisionado",
    "option_c": "Aprendizado por reforço",
    "option_d": "Aprendizado por reforço com feedback humano (RLHF)",
    "correct_answers": [
      "B"
    ],
    "explanation_detailed": "O aprendizado não supervisionado descobre padrões sem rótulos, tornando-o ideal para segmentação de clientes quando existem apenas atributos brutos. O agrupamento (por exemplo, k-means) ou a redução de dimensionalidade podem agrupar clientes semelhantes para campanhas direcionadas. Na AWS, use o SageMaker para treinar algoritmos não supervisionados e avaliar segmentos com KPIs downstream. Os métodos supervisionados exigem alvos rotulados, o RL aborda a tomada de decisão sequencial e o RLHF molda funções de recompensa para LLMs. Começar com a segmentação não supervisionada permite a geração de hipóteses que mais tarde podem ser validadas com testes A/B e modelagem de uplift.",
    "incorrect_explanations": {
      "A": "O aprendizado supervisionado requer alvos rotulados (por exemplo, churn sim/não), que estão ausentes aqui.",
      "C": "O aprendizado por reforço é para decisões sequenciais com recompensas, não para segmentação estática.",
      "D": "O RLHF é uma técnica especializada para alinhar modelos gerativos, não para segmentação de clientes."
    }
  },
  {
    "id": "aif-c01-ai_services-017",
    "certification_id": "AIF-C01",
    "domain": "AI_SERVICES",
    "difficulty": "medium",
    "tier": "FREE",
    "required_selection_count": 1,
    "active": true,
    "question_text": "Um aplicativo de busca de IA deve lidar com consultas contendo texto e imagens. Qual tipo de modelo deve ser usado?",
    "option_a": "Modelo de incorporação multimodal",
    "option_b": "Modelo de incorporação somente de texto",
    "option_c": "Modelo de geração multimodal",
    "option_d": "Modelo de geração de imagem",
    "correct_answers": [
      "A"
    ],
    "explanation_detailed": "Modelos de incorporação multimodal mapeiam entradas heterogêneas — texto e imagens — em um espaço vetorial compartilhado, permitindo a busca por similaridade entre modalidades. Isso suporta casos de uso como “encontrar imagens como esta legenda” ou “corresponder esta foto a documentos relacionados”. Com os índices k-NN e vetoriais do Amazon OpenSearch Service, você pode armazenar incorporações e consultar os vizinhos mais próximos de forma eficiente. As incorporações somente de texto perdem a semântica da imagem, os modelos de geração sintetizam saídas em vez de indexar similaridade. As incorporações multimodais, juntamente com codificadores hospedados no Bedrock e bancos de dados vetoriais, sustentam a recuperação robusta entre modalidades e os pipelines de RAG.",
    "incorrect_explanations": {
      "B": "As incorporações somente de texto não podem codificar imagens; elas quebram os requisitos de recuperação entre modalidades.",
      "C": "A geração foca na criação de conteúdo, não na produção de vetores comparáveis para recuperação.",
      "D": "Os geradores de imagem sintetizam imagens; eles não são construídos para busca por similaridade entre texto e imagens."
    }
  },
  {
    "id": "aif-c01-ai_services-018",
    "certification_id": "AIF-C01",
    "domain": "AI_SERVICES",
    "difficulty": "medium",
    "tier": "FREE",
    "required_selection_count": 1,
    "active": true,
    "question_text": "Um modelo de fundação do Bedrock que alimenta a busca precisa ser mais preciso usando dados da empresa. O que deve ser fornecido para o ajuste fino?",
    "option_a": "Dados rotulados com campos de prompt e conclusão",
    "option_b": "Um arquivo .txt contendo várias linhas em formato .csv",
    "option_c": "Taxa de transferência provisionada para o Amazon Bedrock",
    "option_d": "Treinar em jornais e livros didáticos",
    "correct_answers": [
      "A"
    ],
    "explanation_detailed": "Para o ajuste fino supervisionado de um modelo de fundação, forneça pares alinhados de prompt-conclusão que reflitam seu domínio e as saídas desejadas. Isso ensina o modelo a mapear consultas no estilo empresarial para respostas, formatos e restrições precisas. No Amazon Bedrock, os modelos suportados expõem APIs de ajuste fino e esperam JSONL estruturado com campos como instrução, entrada e saída. A taxa de transferência provisionada afeta a capacidade, não a qualidade. Os corpora públicos genéricos diluem a especificidade do domínio. A rotulagem adequada, as divisões de validação e a avaliação com métricas relevantes para o negócio garantem uma precisão aprimorada alinhada às necessidades da empresa.",
    "incorrect_explanations": {
      "B": "Arquivos malformados ou texto arbitrário não fornecem o sinal supervisionado necessário para o ajuste fino.",
      "C": "A taxa de transferência afeta a escalabilidade e a latência, não a precisão das saídas.",
      "D": "O treinamento em dados genéricos ignora o contexto da empresa, reduzindo o alinhamento do domínio."
    }
  },
  {
    "id": "aif-c01-ai_fundamentals-019",
    "certification_id": "AIF-C01",
    "domain": "AI_FUNDAMENTALS",
    "difficulty": "medium",
    "tier": "FREE",
    "required_selection_count": 1,
    "active": true,
    "question_text": "Uma solução de IA deve verificar se um endereço IP é suspeito para proteger uma aplicação. Qual abordagem se encaixa?",
    "option_a": "Construir um sistema de reconhecimento de fala.",
    "option_b": "Criar um sistema de reconhecimento de entidade nomeada (NLP).",
    "option_c": "Desenvolver um sistema de detecção de anomalias.",
    "option_d": "Criar um sistema de previsão de fraude.",
    "correct_answers": [
      "C"
    ],
    "explanation_detailed": "A detecção de anomalias identifica padrões que se desviam das linhas de base de tráfego normal, sinalizando IPs suspeitos que exibem taxas de solicitação anormais, mudanças de geolocalização ou comportamento. Isso pode ser feito com algoritmos não supervisionados (por exemplo, Isolation Forest) quando os dados de ataque rotulados são limitados. Na AWS, use o SageMaker para treinar e implantar detectores de anomalias; combine com as descobertas do AWS WAF, CloudFront e GuardDuty para defesa em profundidade. Os modelos de fraude são normalmente supervisionados em transações rotuladas. O NER e o reconhecimento de fala não se aplicam à telemetria de rede. A detecção de anomalias oferece aviso prévio e generaliza para novas assinaturas de ameaças.",
    "incorrect_explanations": {
      "A": "O reconhecimento de fala transforma áudio em texto e não está relacionado à detecção de ameaças de rede.",
      "B": "O NER extrai entidades do texto; não avalia anomalias comportamentais de IP.",
      "D": "A previsão de fraude visa padrões transacionais; a reputação de IP geralmente se beneficia de linhas de base de anomalias não supervisionadas."
    }
  },
  {
    "id": "aif-c01-ai_services-020",
    "certification_id": "AIF-C01",
    "domain": "AI_SERVICES",
    "difficulty": "medium",
    "tier": "FREE",
    "required_selection_count": 1,
    "active": true,
    "question_text": "Qual capacidade do Amazon OpenSearch Service permite aplicações de banco de dados vetorial?",
    "option_a": "Integração com o armazenamento de objetos do Amazon S3",
    "option_b": "Indexação e consultas geoespaciais",
    "option_c": "Gerenciamento de índice escalável e busca de vizinho mais próximo k-NN",
    "option_d": "Análise em tempo real de dados de streaming",
    "correct_answers": [
      "C"
    ],
    "explanation_detailed": "O Amazon OpenSearch Service suporta índices vetoriais k-NN que armazenam incorporações de alta dimensão e realizam busca de vizinho mais próximo aproximado, permitindo aplicações de similaridade semântica como RAG, busca entre modalidades e recomendação. Você pode escalar shards e réplicas, ajustar as trocas entre recall e latência e combinar filtros vetoriais e de palavras-chave. A integração com o S3 lida com snapshots, o geoespacial é ortogonal aos vetores e os casos de uso de análise de streaming dependem da ingestão do OpenSearch — não da similaridade vetorial. Com o Bedrock para incorporações e o OpenSearch k-NN, você pode construir bancos de dados vetoriais robustos para cargas de trabalho de produção.",
    "incorrect_explanations": {
      "A": "A integração de snapshots do S3 é para backups e restaurações, não para busca de similaridade vetorial.",
      "B": "Os recursos geoespaciais visam consultas de localização, não a busca de incorporação de alta dimensão.",
      "D": "A análise de streaming é valiosa, mas não está relacionada à recuperação de vetores de vizinho mais próximo."
    }
  },
  {
    "id": "aif-c01-ai_fundamentals-021",
    "certification_id": "AIF-C01",
    "domain": "AI_FUNDAMENTALS",
    "difficulty": "easy",
    "tier": "FREE",
    "required_selection_count": 1,
    "active": true,
    "question_text": "Qual é um caso de uso válido para modelos de IA generativa?",
    "option_a": "Melhorar a segurança da rede usando sistemas de detecção de intrusão.",
    "option_b": "Criar imagens fotorrealistas a partir de descrições textuais para marketing digital.",
    "option_c": "Melhorar o desempenho do banco de dados usando indexação otimizada.",
    "option_d": "Analisar dados financeiros para prever tendências do mercado de ações com ARIMA.",
    "correct_answers": [
      "B"
    ],
    "explanation_detailed": "Os modelos generativos sintetizam conteúdo — imagens, texto, áudio ou código — a partir de prompts. Para marketing, a geração de texto para imagem pode produzir visuais de produtos, cenas de estilo de vida e variantes rapidamente. Com o Amazon Bedrock, você pode invocar modelos de geração de imagem por meio de APIs gerenciadas e adicionar guardrails para impor as políticas da marca. Embora a IA ajude na segurança, ajuste de banco de dados ou previsão de séries temporais, essas não são tarefas primariamente generativas. Os pipelines generativos podem ser combinados com a recuperação para ativos de marca e endpoints do SageMaker para fluxos de trabalho de aprovação. Isso acelera os ciclos criativos, mantendo a governança e os controles de custos.",
    "incorrect_explanations": {
      "A": "A detecção de intrusão foca na detecção de anomalias ou assinaturas, não na geração de conteúdo.",
      "C": "A otimização de índice é uma tarefa de engenharia de banco de dados, não de síntese generativa.",
      "D": "A previsão de ações é modelagem preditiva; os modelos generativos visam a criação de conteúdo."
    }
  },
  {
    "id": "aif-c01-ai_fundamentals-022",
    "certification_id": "AIF-C01",
    "domain": "AI_FUNDAMENTALS",
    "difficulty": "easy",
    "tier": "FREE",
    "required_selection_count": 1,
    "active": true,
    "question_text": "Ao escolher um modelo de fundação no Amazon Bedrock, qual conceito determina quanta informação pode caber em um único prompt?",
    "option_a": "Temperatura",
    "option_b": "Janela de contexto",
    "option_c": "Tamanho do lote (batch size)",
    "option_d": "Tamanho do modelo",
    "correct_answers": [
      "B"
    ],
    "explanation_detailed": "A janela de contexto (comprimento do contexto) determina o número máximo de tokens aos quais o modelo pode prestar atenção na entrada e na saída. Janelas maiores permitem prompts mais longos, mais contexto recuperado ou conversas com vários documentos. No Bedrock, selecione modelos com contexto suficiente para sua estratégia de embasamento (por exemplo, RAG). A temperatura controla a aleatoriedade, não a capacidade. O tamanho do lote se aplica à taxa de transferência durante o treinamento/inferência, não à capacidade de contexto por solicitação. O tamanho do modelo se correlaciona com a capacidade, mas não garante um contexto maior. Alinhe o tamanho da janela com seus orçamentos de fragmentação de recuperação e de tokens para evitar o truncamento.",
    "incorrect_explanations": {
      "A": "A temperatura altera a aleatoriedade da saída; não aumenta quantos tokens cabem no prompt.",
      "C": "O tamanho do lote afeta o paralelismo/taxa de transferência, não a capacidade de comprimento do prompt.",
      "D": "Modelos maiores podem ou não ter janelas de contexto maiores; o comprimento do contexto é um parâmetro distinto."
    }
  },
  {
    "id": "aif-c01-ai_services-023",
    "certification_id": "AIF-C01",
    "domain": "AI_SERVICES",
    "difficulty": "easy",
    "tier": "FREE",
    "required_selection_count": 1,
    "active": true,
    "question_text": "Uma empresa está construindo um chatbot de suporte ao cliente com um modelo de fundação. O bot precisa responder no tom da empresa. O que a equipe deve fazer?",
    "option_a": "Definir um limite de tokens muito baixo para as saídas.",
    "option_b": "Usar inferência em lote para processar respostas detalhadas.",
    "option_c": "Experimentar e refinar iterativamente o prompt até que o FM produza o estilo desejado.",
    "option_d": "Aumentar o parâmetro de temperatura.",
    "correct_answers": [
      "C"
    ],
    "explanation_detailed": "O alinhamento de estilo é alcançado principalmente por meio de prompts fortes: um prompt de sistema que especifica tom, persona, vocabulário da marca e formato, além de exemplos de poucas tentativas (few-shot) mostrando respostas corretas e incorretas. No Amazon Bedrock, você pode impor isso com guardrails e configurações de máximo de tokens para manter as saídas concisas. A temperatura controla a aleatoriedade, não o tom da marca. A inferência em lote é sobre taxa de transferência, não estilo. Os limites de tokens evitam a verbosidade, mas não estabelecem a voz. Combine a iteração de prompts com a avaliação e, se necessário, o ajuste fino em transcrições de conversas aprovadas pela marca para obter a melhor consistência.",
    "incorrect_explanations": {
      "A": "Reduzir os limites de tokens reduz o comprimento, não o alinhamento do tom ou a consistência estilística.",
      "B": "A inferência em lote otimiza a taxa de transferência; não molda o estilo ou o tom.",
      "D": "Temperatura mais alta aumenta a aleatoriedade e pode se desviar do tom alvo."
    }
  },
  {
    "id": "aif-c01-ai_services-024",
    "certification_id": "AIF-C01",
    "domain": "AI_SERVICES",
    "difficulty": "easy",
    "tier": "FREE",
    "required_selection_count": 1,
    "active": true,
    "question_text": "Usando um LLM no Amazon Bedrock para análise de sentimento, como o prompt deve ser estruturado para classificar trechos como positivos ou negativos?",
    "option_a": "Fornecer exemplos rotulados como positivos ou negativos, seguidos pelo novo trecho a ser classificado.",
    "option_b": "Fornecer uma explicação detalhada da análise de sentimento e como os LLMs funcionam.",
    "option_c": "Fornecer apenas o novo trecho sem contexto.",
    "option_d": "Fornecer o novo trecho mais exemplos de tarefas não relacionadas, como sumarização.",
    "correct_answers": [
      "A"
    ],
    "explanation_detailed": "O prompting de poucas tentativas (few-shot) com exemplos rotulados estabelece a tarefa, o formato e os critérios de decisão. Mostrar amostras positivas e negativas ensina ao modelo o limite em seu domínio e melhora a consistência. No Bedrock, combine isso com uma instrução clara, esquema de saída e baixa temperatura para determinismo. Explicações da teoria de ML não adicionam sinal. Prompts não contextualizados geram saídas variáveis. Tarefas não relacionadas confundem o espaço de instrução. Para maior precisão, avalie com conjuntos de dados rotulados e considere o ajuste fino ou o uso do Amazon Comprehend para sentimento pronto quando suas necessidades forem padrão.",
    "incorrect_explanations": {
      "B": "As descrições da teoria não fornecem estrutura de tarefa ou limites de decisão para o modelo.",
      "C": "Prompts sem contexto levam a classificações ambíguas e inconsistentes.",
      "D": "Exemplos irrelevantes poluem o prompt e degradam a aderência à tarefa."
    }
  },
  {
    "id": "aif-c01-ai_services-025",
    "certification_id": "AIF-C01",
    "domain": "AI_SERVICES",
    "difficulty": "medium",
    "tier": "FREE",
    "required_selection_count": 1,
    "active": true,
    "question_text": "Uma empresa de segurança executa FMs no Amazon Bedrock e precisa garantir que apenas usuários autorizados invoquem os modelos. Eles precisam identificar tentativas de acesso não autorizado para refinar futuras políticas e funções do IAM. Qual serviço da AWS deve ser usado?",
    "option_a": "AWS Security Hub",
    "option_b": "AWS CloudTrail",
    "option_c": "Amazon GuardDuty",
    "option_d": "AWS Trusted Advisor",
    "correct_answers": [
      "B"
    ],
    "explanation_detailed": "O AWS CloudTrail registra a atividade da API em todas as contas da AWS, incluindo chamadas, falhas e eventos de acesso negado do Bedrock InvokeModel. A revisão dos logs do CloudTrail ajuda a identificar quais principais tentaram ações não autorizadas e por quê (permissões ausentes, funções erradas, políticas de recursos bloqueadas). Você pode rotear eventos para o CloudWatch Logs e criar alarmes ou regras do EventBridge para alertas. O GuardDuty detecta ameaças no nível da conta/rede, o Security Hub agrega descobertas e o Trusted Advisor oferece verificações de melhores práticas. Para auditar tentativas de acesso ao Bedrock especificamente, o CloudTrail é a fonte autoritativa de evidências.",
    "incorrect_explanations": {
      "A": "O Security Hub agrega descobertas; não captura tentativas de acesso à API brutas para análise forense.",
      "C": "O GuardDuty sinaliza comportamento suspeito, mas não é o registro canônico de chamadas e negações da API do Bedrock.",
      "D": "O Trusted Advisor fornece orientação e verificações, não logs de acesso detalhados por chamada."
    }
  },
  {
    "id": "aif-c01-ai_services-026",
    "certification_id": "AIF-C01",
    "domain": "AI_SERVICES",
    "difficulty": "medium",
    "tier": "FREE",
    "required_selection_count": 1,
    "active": true,
    "question_text": "Uma empresa construiu um modelo de classificação de imagem e quer previsões de produção para uma aplicação web sem gerenciar a infraestrutura. O que eles devem escolher?",
    "option_a": "Hospedar o modelo no AWS Lambda.",
    "option_b": "Usar a Inferência sem Servidor do Amazon SageMaker.",
    "option_c": "Usar o Amazon API Gateway para hospedar o modelo.",
    "option_d": "Usar o AWS Fargate para hospedar o modelo.",
    "correct_answers": [
      "B"
    ],
    "explanation_detailed": "A Inferência sem Servidor do Amazon SageMaker serve modelos sem provisionar instâncias, escalando automaticamente com base no tráfego enquanto cobra pela computação usada durante a inferência. Ela simplifica a implantação para cargas com picos ou imprevisíveis e se integra ao registro de modelos, CI/CD e observabilidade. O Lambda tem limites curtos de tempo de execução e memória e não é ideal para frameworks de ML típicos. O API Gateway serve de fachada para APIs; não executa modelos. O Fargate executa contêineres, mas exige que você gerencie o empacotamento do modelo, o auto scaling e as escolhas de GPU. A Inferência sem Servidor minimiza as operações, preservando os recursos de implantação específicos de ML.",
    "incorrect_explanations": {
      "A": "Os tempos limite curtos e as restrições de memória do Lambda limitam muitas cargas de trabalho e tamanhos de modelo de ML.",
      "C": "O API Gateway é uma camada de roteamento; não pode executar modelos de ML por si só.",
      "D": "O Fargate remove o gerenciamento de servidores, mas ainda requer operações de contêiner e lógica de escalonamento para o serviço de ML."
    }
  },
  {
    "id": "aif-c01-ai_services-027",
    "certification_id": "AIF-C01",
    "domain": "AI_SERVICES",
    "difficulty": "medium",
    "tier": "FREE",
    "required_selection_count": 1,
    "active": true,
    "question_text": "Uma empresa de IA usa fornecedores de software independentes (ISVs) para avaliações periódicas e precisa de notificações por e-mail quando novos relatórios de conformidade de ISVs estiverem disponíveis. Qual serviço da AWS eles devem usar?",
    "option_a": "AWS Control Tower",
    "option_b": "AWS Artifact",
    "option_c": "AWS Trusted Advisor",
    "option_d": "AWS Security Hub",
    "correct_answers": [
      "B"
    ],
    "explanation_detailed": "O AWS Artifact é o portal para acesso sob demanda a relatórios de conformidade e acordos da AWS e de ISVs selecionados. As equipes podem recuperar atestados SOC, ISO, PCI e outros de forma centralizada. Embora o Artifact em si seja um repositório, você pode integrar notificações por meio de automação (por exemplo, polling com o EventBridge Scheduler/Lambda) para alertar as partes interessadas quando novos documentos aparecerem. O Control Tower governa configurações de várias contas, o Trusted Advisor fornece otimização e verificações de melhores práticas, e o Security Hub agrega descobertas de segurança. Para acesso e distribuição de relatórios de conformidade, o Artifact é a principal fonte de verdade.",
    "incorrect_explanations": {
      "A": "O Control Tower gerencia a linha de base e as guardrails da conta, não a recuperação de documentos de conformidade.",
      "C": "O Trusted Advisor foca em verificações de custo, desempenho e segurança, não na entrega de relatórios de conformidade.",
      "D": "O Security Hub agrega descobertas de segurança; não hospeda relatórios de conformidade de ISVs."
    }
  },
  {
    "id": "aif-c01-responsible_ai-028",
    "certification_id": "AIF-C01",
    "domain": "RESPONSIBLE_AI",
    "difficulty": "medium",
    "tier": "FREE",
    "required_selection_count": 1,
    "active": true,
    "question_text": "Uma empresa está construindo um agente de conversação com um LLM e quer reduzir os ataques de engenharia de prompt que coagem ações prejudiciais ou sensíveis. O que ajuda a reduzir esse risco?",
    "option_a": "Criar um modelo de prompt que treine o LLM para detectar padrões de ataque.",
    "option_b": "Aumentar o parâmetro de temperatura.",
    "option_c": "Evitar LLMs não listados no Amazon SageMaker.",
    "option_d": "Reduzir o número de tokens de entrada.",
    "correct_answers": [
      "A"
    ],
    "explanation_detailed": "A defesa em profundidade inclui prompts de sistema robustos com políticas de recusa explícitas, separadores entre instruções e conteúdo do usuário, validação de entrada/saída e detecção de padrões de ataque (por exemplo, dicas de jailbreak). No Amazon Bedrock, combine modelos com Guardrails for Bedrock para filtrar conteúdo inseguro e restringir ações de ferramentas. Temperatura e contagem de tokens não abordam entradas adversárias. A listagem de fontes de modelo não garante segurança contra injeção de prompt. Incorpore listas de permissões de recuperação, saídas validadas por esquema e chamadas de ferramentas de lista de permissões por meio do Agents for Bedrock para limitar ainda mais os danos se as instruções forem subvertidas.",
    "incorrect_explanations": {
      "B": "Temperatura mais alta aumenta a aleatoriedade e pode piorar a aderência às políticas de segurança.",
      "C": "A listagem de modelos não impede a injeção de prompt; a segurança vem dos controles ao redor do modelo.",
      "D": "Prompts mais curtos por si só não neutralizam o conteúdo adversário embutido pelos invasores."
    }
  },
  {
    "id": "aif-c01-responsible_ai-029",
    "certification_id": "AIF-C01",
    "domain": "RESPONSIBLE_AI",
    "difficulty": "easy",
    "tier": "FREE",
    "required_selection_count": 1,
    "active": true,
    "question_text": "Usando uma Matriz de Escopo de Segurança de IA Generativa, qual escopo de solução dá à empresa a maior responsabilidade de segurança?",
    "option_a": "Usar um aplicativo de terceiros com recursos de IA generativa incorporados.",
    "option_b": "Construir um aplicativo usando um modelo de IA generativa de terceiros (FM).",
    "option_c": "Refinar um modelo de IA generativa de terceiros (FM) com dados de negócios.",
    "option_d": "Construir e treinar um modelo de IA generativa do zero com dados do cliente.",
    "correct_answers": [
      "D"
    ],
    "explanation_detailed": "Treinar um modelo do zero transfere a responsabilidade máxima para a empresa: coleta/consentimento de dados, qualidade da rotulagem, pipelines seguros, arquitetura do modelo, fortalecimento da infraestrutura de treinamento, avaliação, red-teaming e controles de implantação. Na AWS, isso abrange a governança de dados do S3, segurança de treinamento do SageMaker, criptografia do KMS, isolamento da VPC, varredura de imagens do ECR e menor privilégio do IAM. Usar ou ajustar FMs de terceiros transfere muitos controles para o provedor. A IA incorporada em SaaS deixa a menor responsabilidade. Quanto mais você personaliza, mais você assume a superfície de segurança e as obrigações de conformidade.",
    "incorrect_explanations": {
      "A": "A IA incorporada em SaaS descarrega a maior parte da responsabilidade para o fornecedor, não para a empresa.",
      "B": "Construir em um FM de terceiros reduz a responsabilidade em comparação com o treinamento de sua própria pilha de modelos.",
      "C": "O ajuste fino adiciona responsabilidade, mas ainda aproveita mais os controles do provedor do que o treinamento do zero."
    }
  },
  {
    "id": "aif-c01-ai_services-030",
    "certification_id": "AIF-C01",
    "domain": "AI_SERVICES",
    "difficulty": "easy",
    "tier": "FREE",
    "required_selection_count": 1,
    "active": true,
    "question_text": "Um praticante de IA tem um grande banco de dados de fotos de animais e quer identificar e categorizar automaticamente os animais nas imagens sem esforço manual. Qual estratégia se encaixa?",
    "option_a": "Detecção de objetos",
    "option_b": "Detecção de anomalias",
    "option_c": "Reconhecimento de entidade nomeada",
    "option_d": "Inpainting",
    "correct_answers": [
      "A"
    ],
    "explanation_detailed": "A detecção de objetos localiza e classifica objetos dentro de imagens usando caixas delimitadoras ou máscaras. Modelos pré-treinados adaptados por meio de aprendizado por transferência podem reconhecer espécies de animais e contar ocorrências. Na AWS, o Amazon Rekognition fornece rótulos pré-construídos, enquanto o SageMaker suporta detectores personalizados (por exemplo, YOLO, Detectron2). A detecção de anomalias visa outliers, o NER extrai entidades do texto e o inpainting preenche regiões de imagem ausentes. Para categorização automática com intervenção manual mínima, a detecção de objetos e a classificação de imagem são as abordagens de visão computacional apropriadas.",
    "incorrect_explanations": {
      "B": "A detecção de anomalias encontra padrões incomuns; não rotula classes de objetos conhecidos em imagens.",
      "C": "O NER é uma técnica de PNL aplicada ao texto, não a imagens.",
      "D": "O inpainting edita imagens preenchendo regiões; não realiza categorização."
    }
  },
  {
    "id": "aif-c01-ai_services-031",
    "certification_id": "AIF-C01",
    "domain": "AI_SERVICES",
    "difficulty": "easy",
    "tier": "FREE",
    "required_selection_count": 1,
    "active": true,
    "question_text": "Uma empresa quer construir com o Amazon Bedrock com um orçamento limitado e com flexibilidade, evitando compromissos de longo prazo. Qual modelo de precificação se encaixa?",
    "option_a": "Sob demanda",
    "option_b": "Personalização de modelo",
    "option_c": "Taxa de transferência provisionada",
    "option_d": "Instância spot",
    "correct_answers": [
      "A"
    ],
    "explanation_detailed": "A precificação sob demanda no Amazon Bedrock cobra por solicitação/token sem reservas de capacidade, ideal para experimentação e uso variável. A taxa de transferência provisionada reserva capacidade para cargas de trabalho consistentes e de alto volume a uma taxa comprometida. A personalização de modelo refere-se ao custo de ajuste fino, não a um plano de precificação para inferência. As instâncias spot são preços de computação do EC2, não aplicáveis a endpoints de modelo totalmente gerenciados do Bedrock. Comece sob demanda, meça o custo e a latência e, em seguida, considere a taxa de transferência provisionada quando o tráfego se estabilizar e os SLAs exigirem desempenho previsível.",
    "incorrect_explanations": {
      "B": "A personalização é uma atividade (ajuste fino), não um modelo de precificação de inferência flexível pague-conforme-o-uso.",
      "C": "A taxa de transferência provisionada requer compromisso e é otimizada para uso alto e estável, não para flexibilidade orçamentária.",
      "D": "A precificação spot se aplica à computação do EC2, não à invocação de modelo gerenciado do Bedrock."
    }
  },
  {
    "id": "aif-c01-ai_services-032",
    "certification_id": "AIF-C01",
    "domain": "AI_SERVICES",
    "difficulty": "medium",
    "tier": "FREE",
    "required_selection_count": 1,
    "active": true,
    "question_text": "Qual serviço da AWS ajuda uma equipe de desenvolvimento a implantar e consumir rapidamente um modelo de fundação em sua VPC?",
    "option_a": "Amazon Personalize",
    "option_b": "Amazon SageMaker JumpStart",
    "option_c": "PartyRock (playground do Amazon Bedrock)",
    "option_d": "Endpoints do Amazon SageMaker",
    "correct_answers": [
      "B"
    ],
    "explanation_detailed": "O Amazon SageMaker JumpStart fornece catálogos de modelos selecionados, modelos de solução e implantações com 1 clique em sua conta e VPC. As equipes podem configurar endpoints para modelos de fundação ou ajustar finamente rapidamente com padrões de MLOps integrados. O Personalize é um serviço de recomendação gerenciado, o PartyRock é um sandbox público não vinculado à sua VPC e os endpoints genéricos do SageMaker exigem que você traga e configure o modelo. O JumpStart acelera implantações seguras e orientadas para a produção com guardrails, IAM e observabilidade integrados.",
    "incorrect_explanations": {
      "A": "O Personalize visa casos de uso de recomendação; não implanta FMs gerais em sua VPC.",
      "C": "O PartyRock é um playground para experimentação, não um caminho de implantação de VPC de produção.",
      "D": "Endpoints simples precisam de um modelo e configuração; o JumpStart simplifica a seleção e a implantação."
    }
  },
  {
    "id": "aif-c01-responsible_ai-033",
    "certification_id": "AIF-C01",
    "domain": "RESPONSIBLE_AI",
    "difficulty": "easy",
    "tier": "FREE",
    "required_selection_count": 1,
    "active": true,
    "question_text": "Como as empresas podem usar os LLMs com segurança no Amazon Bedrock?",
    "option_a": "Elaborar prompts claros e configurar funções/políticas do IAM com o menor privilégio.",
    "option_b": "Habilitar o AWS Audit Manager para avaliações automáticas de modelos.",
    "option_c": "Habilitar a avaliação automática de modelos no Amazon Bedrock.",
    "option_d": "Usar os CloudWatch Logs para tornar os modelos explicáveis e monitorar o viés.",
    "correct_answers": [
      "A"
    ],
    "explanation_detailed": "O uso seguro de LLMs combina um design de instrução forte, IAM de menor privilégio, criptografia, conectividade privada (por exemplo, PrivateLink) e controles de conteúdo (Guardrails for Bedrock). Prompts claros reduzem a ambiguidade e o uso indevido, enquanto o IAM restringe o acesso a dados e ferramentas ao que é necessário. O Audit Manager ajuda a evidenciar a conformidade, mas não protege o modelo em si. O Bedrock não avalia automaticamente todos os riscos por padrão. O CloudWatch captura logs e métricas, mas não fornece explicabilidade ou controles de viés sozinho. A defesa em profundidade inclui políticas de prompt, listas de permissões de recuperação, saídas validadas por esquema e monitoramento contínuo.",
    "incorrect_explanations": {
      "B": "O Audit Manager auxilia com evidências de conformidade; não é um controle direto para a segurança ou o acesso do LLM.",
      "C": "Não há uma única opção que garanta o uso seguro do LLM; você deve implementar vários controles.",
      "D": "Os logs auxiliam na observabilidade, mas não impõem garantias de explicabilidade ou justiça."
    }
  },
  {
    "id": "aif-c01-ai_services-034",
    "certification_id": "AIF-C01",
    "domain": "AI_SERVICES",
    "difficulty": "medium",
    "tier": "FREE",
    "required_selection_count": 1,
    "active": true,
    "question_text": "Uma empresa tem terabytes de dados e quer um aplicativo de IA que transforme entradas em linguagem natural em SQL para funcionários não técnicos. O que se encaixa melhor?",
    "option_a": "Transformadores Generativos Pré-treinados (GPT)",
    "option_b": "Rede neural residual",
    "option_c": "Máquina de vetores de suporte",
    "option_d": "WaveNet",
    "correct_answers": [
      "A"
    ],
    "explanation_detailed": "Os LLMs do estilo GPT se destacam na síntese de código a partir da linguagem natural, incluindo a geração de SQL com dicas de esquema e exemplos. No Amazon Bedrock, você pode usar modelos de fundação para traduzir perguntas em SQL, validar em relação a um esquema e executar em um ambiente controlado com guardrails. As ResNets focam em tarefas de visão, as SVMs são classificadores de ML clássicos e o WaveNet visa o áudio. Adicione validação ciente do esquema, consultas parametrizadas e acesso baseado em função para evitar operações inseguras e registre prompts/resultados com o CloudWatch para auditoria.",
    "incorrect_explanations": {
      "B": "As redes residuais são usadas principalmente em visão computacional e não são projetadas para tarefas de texto para SQL.",
      "C": "As SVMs classificam vetores de características; elas não geram SQL executável a partir da linguagem natural.",
      "D": "O WaveNet é um gerador de formas de onda de áudio; não é um modelo de texto para código."
    }
  },
  {
    "id": "aif-c01-ai_fundamentals-035",
    "certification_id": "AIF-C01",
    "domain": "AI_FUNDAMENTALS",
    "difficulty": "easy",
    "tier": "FREE",
    "required_selection_count": 1,
    "active": true,
    "question_text": "Um detector de objetos de aprendizado profundo é implantado. Quando o modelo analisa uma nova imagem para identificar objetos, qual processo está ocorrendo?",
    "option_a": "Treinamento",
    "option_b": "Inferência",
    "option_c": "Implantação do modelo",
    "option_d": "Mitigação de viés",
    "correct_answers": [
      "B"
    ],
    "explanation_detailed": "A inferência é a fase de aplicação onde um modelo treinado processa novas entradas para produzir previsões. Em produção no Amazon SageMaker (endpoints em tempo real, sem servidor ou assíncronos), o modelo carrega pesos, executa passagens diretas e retorna detecções (classes, caixas, pontuações). O treinamento ajusta os pesos; a implantação provisiona a infraestrutura de serviço; a mitigação de viés aborda a justiça durante a preparação de dados/modelos. O monitoramento de latência, taxa de transferência e desvio de precisão com o CloudWatch e o Model Monitor garante que a inferência permaneça saudável ao longo do tempo.",
    "incorrect_explanations": {
      "A": "O treinamento atualiza os pesos usando dados rotulados; não descreve o uso do modelo em imagens não vistas.",
      "C": "A implantação configura o endpoint; não é o ato de prever.",
      "D": "A mitigação de viés é uma atividade de governança, não uma previsão em tempo de execução."
    }
  },
  {
    "id": "aif-c01-responsible_ai-036",
    "certification_id": "AIF-C01",
    "domain": "RESPONSIBLE_AI",
    "difficulty": "medium",
    "tier": "FREE",
    "required_selection_count": 1,
    "active": true,
    "question_text": "Um praticante de IA gera imagens de pessoas em diferentes profissões, mas encontra resultados enviesados devido a dados de entrada distorcidos. Qual técnica ajuda?",
    "option_a": "Aumento de dados para classes sub-representadas",
    "option_b": "Monitoramento do modelo para distribuição de classes",
    "option_c": "Geração aumentada por recuperação (RAG)",
    "option_d": "Detecção de marca d'água para imagens",
    "correct_answers": [
      "A"
    ],
    "explanation_detailed": "O viés muitas vezes decorre da sub-representação. Aumentar as classes minoritárias — por meio de amostras selecionadas adicionais ou transformações — ajuda a equilibrar a distribuição do treinamento. Combine isso com a coleta direcionada e a reponderação para corrigir as distorções. No SageMaker, use o Clarify para medir o viés pré/pós-treinamento e rastrear métricas de justiça, depois treine novamente com dados aumentados. O monitoramento da distribuição de classes é diagnóstico, mas não corretivo, o RAG afeta o embasamento do texto e a detecção de marca d'água não está relacionada. Dados balanceados são fundamentais para resultados gerativos mais justos.",
    "incorrect_explanations": {
      "B": "O monitoramento revela a distorção, mas não a corrige; você deve alterar os dados ou o objetivo do treinamento.",
      "C": "O RAG melhora o embasamento factual para modelos de linguagem, não o equilíbrio de classes visuais.",
      "D": "A marca d'água verifica a proveniência, não a representação demográfica ou ocupacional nos resultados."
    }
  },
  {
    "id": "aif-c01-ai_services-037",
    "certification_id": "AIF-C01",
    "domain": "AI_SERVICES",
    "difficulty": "medium",
    "tier": "FREE",
    "required_selection_count": 1,
    "active": true,
    "question_text": "Uma empresa está implementando o Amazon Titan (FM) no Amazon Bedrock e precisa complementar o modelo com dados privados relevantes. O que eles devem fazer?",
    "option_a": "Usar um FM diferente.",
    "option_b": "Diminuir a temperatura.",
    "option_c": "Criar uma base de conhecimento do Bedrock.",
    "option_d": "Habilitar o registro de invocação do modelo.",
    "correct_answers": [
      "C"
    ],
    "explanation_detailed": "Uma base de conhecimento do Bedrock indexa documentos privados, gera incorporações e recupera os trechos mais relevantes para embasar as respostas do modelo. Isso melhora a factualidade sem ajuste fino e mantém os dados sob seu controle. A temperatura afeta a aleatoriedade, não o conhecimento. O registro de invocação auxilia na observabilidade. A troca de modelos não injeta o conteúdo do seu domínio. Combine bases de conhecimento com guardrails, controle de acesso (IAM, endpoints da VPC) e avaliação para manter as respostas precisas, contextuais и em conformidade.",
    "incorrect_explanations": {
      "A": "Mudar de modelo não adiciona seu conhecimento privado; você precisa de recuperação ou ajuste fino.",
      "B": "Temperatura mais baixa reduz a aleatoriedade, mas não fornece fatos do domínio.",
      "D": "O registro é útil para auditoria, mas não melhora o embasamento do conteúdo."
    }
  },
  {
    "id": "aif-c01-responsible_ai-038",
    "certification_id": "AIF-C01",
    "domain": "RESPONSIBLE_AI",
    "difficulty": "medium",
    "tier": "FREE",
    "required_selection_count": 1,
    "active": true,
    "question_text": "Uma empresa médica personaliza um modelo de fundação para diagnóstico e precisa atender à transparência regulatória. O que eles devem usar?",
    "option_a": "Amazon Inspector para configuração de segurança e conformidade",
    "option_b": "Amazon SageMaker Clarify para métricas, relatórios e explicações baseadas em exemplos",
    "option_c": "Amazon Macie para criptografar e proteger os dados de treinamento",
    "option_d": "Coletar mais dados e usar os Rótulos Personalizados do Amazon Rekognition",
    "correct_answers": [
      "B"
    ],
    "explanation_detailed": "O Amazon SageMaker Clarify fornece explicabilidade (atribuições de características como SHAP), métricas de viés e relatórios no momento do treinamento e da inferência. Esses artefatos apoiam os cartões de modelo e a documentação regulatória, mostrando como as entradas influenciam as previsões e se existe impacto díspar. O Inspector verifica as vulnerabilidades nos recursos, o Macie descobre dados sensíveis, mas não explica os modelos, e o Rekognition está relacionado à rotulagem de visão. Em cuidados de saúde regulamentados, combine o Clarify com os Cartões de Modelo do SageMaker, linhagem de dados e revisão por intervenção humana para uma postura de transparência defensável.",
    "incorrect_explanations": {
      "A": "O Inspector visa avaliações de vulnerabilidade; não gera explicações de modelo ou métricas de justiça.",
      "C": "O Macie identifica a exposição de dados sensíveis; não é uma ferramenta de explicabilidade.",
      "D": "Mais dados ou serviços diferentes não substituem a explicabilidade formal e os relatórios de viés."
    }
  },
  {
    "id": "aif-c01-responsible_ai-039",
    "certification_id": "AIF-C01",
    "domain": "RESPONSIBLE_AI",
    "difficulty": "medium",
    "tier": "FREE",
    "required_selection_count": 2,
    "active": true,
    "question_text": "Uma empresa implantará um bot de conversação (ajustado via SageMaker JumpStart) e deve demonstrar conformidade com vários frameworks regulatórios. Quais capacidades ajudam? (Escolha duas.)",
    "option_a": "Endpoints de inferência com auto scaling",
    "option_b": "Detecção de ameaças",
    "option_c": "Proteção de dados",
    "option_d": "Otimização de custos",
    "option_e": "Microsserviços fracamente acoplados",
    "correct_answers": [
      "B",
      "C"
    ],
    "explanation_detailed": "As narrativas de conformidade dependem de controles de segurança: detecção de ameaças e proteção de dados. Serviços da AWS como GuardDuty, Security Hub e CloudTrail ajudam a detectar e agregar atividades suspeitas e fornecem trilhas de auditoria. A proteção de dados abrange criptografia em repouso/em trânsito (KMS, TLS), menor privilégio do IAM, rede privada (PrivateLink) e minimização de dados. O auto scaling e os microsserviços auxiliam na resiliência e na manutenibilidade; a otimização de custos afeta os gastos. Para chatbots regulamentados, evidencie controles de detecção fortes e políticas robustas de manuseio de dados.",
    "incorrect_explanations": {
      "A": "O auto scaling melhora a disponibilidade, mas não aborda diretamente os controles regulatórios.",
      "D": "A eficiência de custos é valiosa, mas não é um controle de conformidade.",
      "E": "O estilo de arquitetura ajuda na manutenibilidade; a conformidade foca nos controles de segurança e privacidade."
    }
  },
  {
    "id": "aif-c01-ai_fundamentals-040",
    "certification_id": "AIF-C01",
    "domain": "AI_FUNDAMENTALS",
    "difficulty": "easy",
    "tier": "FREE",
    "required_selection_count": 1,
    "active": true,
    "question_text": "Uma equipe quer aumentar a precisão de um modelo de fundação para um limiar aceitável. Que ação eles devem tomar?",
    "option_a": "Reduzir o tamanho do lote.",
    "option_b": "Aumentar o número de épocas.",
    "option_c": "Diminuir o número de épocas.",
    "option_d": "Aumentar a temperatura.",
    "correct_answers": [
      "B"
    ],
    "explanation_detailed": "Aumentar as épocas dá ao modelo mais passagens sobre o conjunto de treinamento, muitas vezes melhorando a precisão até que o overfitting comece. Monitore a perda/métricas de validação e aplique a parada antecipada. No SageMaker, acompanhe os experimentos e compare as execuções. O tamanho do lote afeta a dinâmica de otimização, não necessariamente a precisão final. A temperatura é um parâmetro de amostragem em tempo de inferência para modelos gerativos e não está relacionada ao treinamento. Se os ganhos estagnarem, considere mais dados, aprendizado por transferência, regularização ou ajuste de hiperparâmetros via Otimização Automática de Modelo do SageMaker.",
    "incorrect_explanations": {
      "A": "O tamanho do lote influencia a dinâmica do treinamento, mas não é uma alavanca garantida para a precisão.",
      "C": "Menos épocas reduzem a oportunidade de aprendizado e muitas vezes diminuem a precisão.",
      "D": "A temperatura altera a aleatoriedade da saída na inferência, não a qualidade do treinamento."
    }
  },
  {
    "id": "aif-c01-ai_services-041",
    "certification_id": "AIF-C01",
    "domain": "AI_SERVICES",
    "difficulty": "easy",
    "tier": "FREE",
    "required_selection_count": 1,
    "active": true,
    "question_text": "Um call center quer reduzir o número de ações que os agentes realizam para responder às perguntas dos clientes, implantando um chatbot LLM. Qual KPI de negócios mede melhor o impacto?",
    "option_a": "Taxa de engajamento do site",
    "option_b": "Tempo Médio de Atendimento (TMA)",
    "option_c": "Responsabilidade social corporativa",
    "option_d": "Conformidade regulatória",
    "correct_answers": [
      "B"
    ],
    "explanation_detailed": "O tempo médio de atendimento reflete a eficiência com que os problemas dos clientes são resolvidos. Um chatbot eficaz que recupera respostas ou automatiza etapas deve reduzir o TMA, pré-resolvendo ou acelerando os fluxos de trabalho dos agentes. O engajamento do site é uma métrica de marketing, a RSC se relaciona à ética/sustentabilidade e a conformidade é um resultado de governança. Combine o TMA com a taxa de contenção, resolução no primeiro contato, CSAT e métricas de desvio para uma imagem completa. Use o Amazon Bedrock para o LLM e integre com sistemas empresariais via Agents for Bedrock.",
    "incorrect_explanations": {
      "A": "O engajamento do site não está diretamente ligado à eficiência do call center ou à redução de tarefas do agente.",
      "C": "A RSC não mede a eficiência operacional nos fluxos de trabalho de suporte.",
      "D": "A conformidade é necessária, mas não quantifica os ganhos de produtividade de um chatbot."
    }
  },
  {
    "id": "aif-c01-ai_services-042",
    "certification_id": "AIF-C01",
    "domain": "AI_SERVICES",
    "difficulty": "easy",
    "tier": "FREE",
    "required_selection_count": 1,
    "active": true,
    "question_text": "Qual capacidade o Amazon SageMaker Clarify fornece?",
    "option_a": "Integra um fluxo de trabalho de Geração Aumentada por Recuperação (RAG)",
    "option_b": "Monitora a qualidade do ML de produção",
    "option_c": "Documenta detalhes críticos sobre modelos de ML",
    "option_d": "Identifica viés potencial durante a preparação dos dados",
    "correct_answers": [
      "D"
    ],
    "explanation_detailed": "O SageMaker Clarify mede o viés em conjuntos de dados e modelos e fornece explicabilidade por meio de atribuições de características (por exemplo, SHAP) no momento do treinamento e da inferência. Os relatórios ajudam as equipes a avaliar a justiça e a entender a influência das características, apoiando a IA responsável. O Model Monitor lida com o desvio/qualidade da produção, não o Clarify. Os Model Cards documentam os modelos. O RAG é um pipeline de recuperação separado, geralmente construído com o Bedrock e armazenamentos de vetores. Use o Clarify no pré-processamento, treinamento e auditorias pós-implantação para rastrear métricas de equidade e explicar as previsões.",
    "incorrect_explanations": {
      "A": "O RAG envolve recuperação e geração; o Clarify foca no viés e na explicabilidade, não na recuperação.",
      "B": "O SageMaker Model Monitor rastreia o desvio e a qualidade; o Clarify é para viés/explicabilidade.",
      "C": "Os Model Cards fornecem documentação; o Clarify gera artefatos de viés e explicação."
    }
  },
  {
    "id": "aif-c01-ai_fundamentals-043",
    "certification_id": "AIF-C01",
    "domain": "AI_FUNDAMENTALS",
    "difficulty": "medium",
    "tier": "FREE",
    "required_selection_count": 1,
    "active": true,
    "question_text": "Um modelo prevê bem os preços dos itens nos dados de treinamento, mas degrada significativamente na produção. O que a empresa deve fazer?",
    "option_a": "Reduzir a quantidade de dados de treinamento.",
    "option_b": "Adicionar mais hiperparâmetros ao modelo.",
    "option_c": "Aumentar a quantidade de dados de treinamento.",
    "option_d": "Aumentar o tempo de treinamento.",
    "correct_answers": [
      "C"
    ],
    "explanation_detailed": "A generalização melhora com dados mais diversos e representativos que correspondem às condições de produção. Colete amostras adicionais, reduza o ruído dos rótulos e atualize para sazonalidade ou mudança de covariável. No SageMaker, treine novamente com novos dados e avalie usando conjuntos de validação e testes retroativos. Adicionar hiperparâmetros ou treinar por mais tempo cegamente raramente corrige o overfitting ou o desvio de dados. Combine com engenharia de características, regularização e o Model Monitor para detectar o desvio precocemente e acionar os pipelines de retreinamento.",
    "incorrect_explanations": {
      "A": "Menos dados geralmente pioram a generalização e aumentam o risco de overfitting.",
      "B": "Mais hiperparâmetros aumentam a complexidade e podem exacerbar o overfitting sem melhorias nos dados.",
      "D": "Um treinamento mais longo pode superajustar ainda mais se os dados não forem representativos."
    }
  },
  {
    "id": "aif-c01-ai_services-044",
    "certification_id": "AIF-C01",
    "domain": "AI_SERVICES",
    "difficulty": "medium",
    "tier": "FREE",
    "required_selection_count": 2,
    "active": true,
    "question_text": "Uma empresa de e-commerce quer determinar o sentimento de avaliações de produtos escritas. Quais serviços da AWS satisfazem isso? (Escolha dois.)",
    "option_a": "Amazon Lex",
    "option_b": "Amazon Comprehend",
    "option_c": "Amazon Polly",
    "option_d": "Amazon Bedrock",
    "option_e": "Amazon Rekognition",
    "correct_answers": [
      "B",
      "D"
    ],
    "explanation_detailed": "O Amazon Comprehend fornece análise de sentimento pronta para uso para texto com APIs que retornam rótulos positivos/negativos/neutros/mistos e confiança. Para tom personalizado, jargão de domínio ou nuance multilíngue, um LLM no Amazon Bedrock pode ser solicitado ou ajustado para classificar o sentimento ou extrair justificativas. O Lex constrói chatbots, o Polly converte texto em fala e o Rekognition analisa imagens/vídeos. Combine o Comprehend para escala com prompts direcionados do Bedrock ou ajuste fino para regras de sentimento específicas da marca.",
    "incorrect_explanations": {
      "A": "O Lex orquestra conversas; não analisa o sentimento de corpora de texto.",
      "C": "O Polly sintetiza a fala a partir do texto, não relacionado à classificação de sentimento.",
      "E": "O Rekognition é para tarefas de visão, não para sentimento de texto."
    }
  },
  {
    "id": "aif-c01-ai_services-045",
    "certification_id": "AIF-C01",
    "domain": "AI_SERVICES",
    "difficulty": "medium",
    "tier": "FREE",
    "required_selection_count": 1,
    "active": true,
    "question_text": "Uma empresa construirá uma interface de bate-papo LLM para manuais de produtos armazenados como PDFs. Qual é a abordagem mais econômica?",
    "option_a": "Adicionar um único PDF a cada prompt de usuário via engenharia de prompt.",
    "option_b": "Adicionar todos os PDFs a cada prompt via engenharia de prompt.",
    "option_c": "Ajustar um modelo em todos os PDFs e usá-lo para prompts.",
    "option_d": "Carregar os PDFs em uma base de conhecimento do Bedrock e usá-la para recuperação no momento da consulta.",
    "correct_answers": [
      "D"
    ],
    "explanation_detailed": "Uma base de conhecimento do Bedrock indexa PDFs em incorporações e recupera apenas os trechos relevantes por consulta, minimizando o uso de tokens e evitando o ajuste fino caro. Inserir documentos inteiros nos prompts é caro e muitas vezes excede os limites de contexto. O ajuste fino para manuais estáticos é desnecessário; a recuperação mantém o conteúdo atualizado sem treinar novamente. Essa abordagem combina bem com guardrails, esquemas de saída e avaliação, fornecendo respostas precisas e controladas a um custo menor.",
    "incorrect_explanations": {
      "A": "Injetar um PDF inteiro em cada prompt desperdiça tokens e corre o risco de truncamento.",
      "B": "Inserir todos os PDFs é proibitivamente caro e excede as janelas de contexto.",
      "C": "O ajuste fino é caro e inflexível para documentos que mudam; a recuperação é mais barata e mais atualizada."
    }
  },
  {
    "id": "aif-c01-responsible_ai-046",
    "certification_id": "AIF-C01",
    "domain": "RESPONSIBLE_AI",
    "difficulty": "medium",
    "tier": "FREE",
    "required_selection_count": 1,
    "active": true,
    "question_text": "Uma empresa de mídia social usará um LLM para moderação de conteúdo e quer avaliar os resultados em busca de viés e discriminação com esforço administrativo mínimo. Qual fonte de dados deve ser usada?",
    "option_a": "Conteúdo gerado pelo usuário",
    "option_b": "Logs de moderação",
    "option_c": "Diretrizes de moderação de conteúdo",
    "option_d": "Conjuntos de dados de benchmark",
    "correct_answers": [
      "D"
    ],
    "explanation_detailed": "Os conjuntos de dados de benchmark são pré-selecionados, padronizados e rotulados para testes de justiça/viés, permitindo uma avaliação rápida e reproduzível sem rotulagem interna pesada. Eles ajudam a comparar modelos e prompts de forma consistente. O conteúdo do usuário e os logs são valiosos, mas exigem limpeza, rotulagem e manuseio de privacidade significativos. As diretrizes informam a política, mas não são conjuntos de teste rotulados. Use benchmarks primeiro, depois valide com dados internos mascarados para garantir o alinhamento com as normas da plataforma e as restrições legais.",
    "incorrect_explanations": {
      "A": "O conteúdo bruto do usuário exige um trabalho pesado de rotulagem e privacidade antes da avaliação de justiça.",
      "B": "Os logs refletem ações passadas, não conjuntos de teste padronizados para medição de viés.",
      "C": "As diretrizes são políticas, não conjuntos de dados com rótulos para avaliação."
    }
  },
  {
    "id": "aif-c01-ai_services-047",
    "certification_id": "AIF-C01",
    "domain": "AI_SERVICES",
    "difficulty": "easy",
    "tier": "FREE",
    "required_selection_count": 1,
    "active": true,
    "question_text": "Uma equipe de marketing usará um modelo gerativo pré-treinado para criar conteúdo de campanha e deve garantir que os resultados estejam alinhados com a voz da marca e os requisitos de comunicação. O que eles devem fazer?",
    "option_a": "Otimizar a arquitetura e os hiperparâmetros do modelo base.",
    "option_b": "Adicionar mais camadas para aumentar a complexidade.",
    "option_c": "Elaborar prompts eficazes com instruções claras e restrições contextuais.",
    "option_d": "Pré-treinar um novo modelo gerativo em um corpus amplo e diversificado.",
    "correct_answers": [
      "C"
    ],
    "explanation_detailed": "O prompting define persona, tom, listas de fazer/não fazer, guias de estilo e exemplos para restringir os resultados aos padrões da marca. No Bedrock, use prompts de sistema, exemplos de poucas tentativas (few-shot) e guardrails para filtrar temas não permitidos. As alterações de arquitetura ou o pré-treinamento são caros e desnecessários para o controle de estilo. Comece com prompts, depois considere um ajuste fino leve em ativos de marca aprovados para consistência em escala. Acompanhe os resultados com loops de revisão humana para campanhas de alto impacto.",
    "incorrect_explanations": {
      "A": "Ajustar a arquitetura base é caro e não é necessário para impor diretrizes de estilo.",
      "B": "Adicionar camadas aumenta a complexidade e o custo sem garantir a aderência à marca.",
      "D": "O pré-treinamento de um novo modelo é excessivo para a voz da marca; o prompting e o ajuste fino são suficientes."
    }
  },
  {
    "id": "aif-c01-responsible_ai-048",
    "certification_id": "AIF-C01",
    "domain": "RESPONSIBLE_AI",
    "difficulty": "medium",
    "tier": "FREE",
    "required_selection_count": 2,
    "active": true,
    "question_text": "Uma empresa de empréstimos está construindo uma solução de IA generativa para oferecer descontos a novos candidatos e quer minimizar o viés prejudicial. Que ações eles devem tomar? (Escolha duas.)",
    "option_a": "Detectar desequilíbrios ou disparidades nos dados.",
    "option_b": "Garantir que o modelo seja executado com frequência.",
    "option_c": "Avaliar o comportamento do modelo para fornecer transparência às partes interessadas.",
    "option_d": "Usar o ROUGE para garantir 100% de precisão.",
    "option_e": "Garantir que o tempo de inferência esteja dentro dos limites aceitos.",
    "correct_answers": [
      "A",
      "C"
    ],
    "explanation_detailed": "O controle do viés começa com auditorias de dados: identifique lacunas de representação e impacto díspar em grupos protegidos. Aplique rebalanceamento, reponderação ou coleta direcionada. Avalie o comportamento com métricas de justiça e artefatos de transparência (SageMaker Clarify, Model Cards). A frequência de execuções, o ROUGE (uma métrica de sumarização) e a latência não abordam a justiça. Documente a governança, implemente explicações de ações adversas quando aplicável e mantenha o monitoramento para detectar desvios tanto nos dados quanto nas métricas de justiça.",
    "incorrect_explanations": {
      "B": "Executar com frequência não mitiga o viés; a qualidade e a avaliação dos dados o fazem.",
      "D": "O ROUGE mede a sobreposição de resumos; não avalia a justiça nem garante a precisão.",
      "E": "A latência é uma métrica de desempenho, não relacionada ao controle do viés."
    }
  },
  {
    "id": "aif-c01-ai_services-049",
    "certification_id": "AIF-C01",
    "domain": "AI_SERVICES",
    "difficulty": "medium",
    "tier": "FREE",
    "required_selection_count": 1,
    "active": true,
    "question_text": "Uma empresa usa um modelo base do Bedrock para resumir chats de suporte ao cliente, mas treinou um modelo personalizado para melhorar a qualidade. O que eles precisam fazer para usar o modelo personalizado via Bedrock?",
    "option_a": "Comprar taxa de transferência provisionada para o modelo personalizado.",
    "option_b": "Implantar o modelo personalizado em um endpoint em tempo real do Amazon SageMaker.",
    "option_c": "Registrar o modelo apenas no SageMaker Model Registry.",
    "option_d": "Conceder acesso ao modelo personalizado diretamente no Bedrock.",
    "correct_answers": [
      "B"
    ],
    "explanation_detailed": "Modelos personalizados que você treina são normalmente hospedados em endpoints do SageMaker. Sua aplicação pode orquestrar chamadas ao Bedrock para modelos base e ao SageMaker para seu sumarizador personalizado por trás de uma única camada de API. A taxa de transferência provisionada se aplica a modelos gerenciados pelo Bedrock. O Model Registry auxilia no versionamento/governança, mas não serve o tráfego sozinho. O Bedrock não pode hospedar automaticamente seu modelo treinado independentemente, a menos que integrado por meio de sua arquitetura. Use o IAM e os endpoints da VPC para proteger ambos os caminhos e registre a inferência com o CloudWatch.",
    "incorrect_explanations": {
      "A": "A taxa de transferência provisionada é para modelos do Bedrock; não hospeda seu modelo personalizado.",
      "C": "O Model Registry registra versões; não cria um endpoint de inferência.",
      "D": "O Bedrock não hospeda diretamente modelos personalizados externos arbitrários sem sua implantação."
    }
  },
  {
    "id": "aif-c01-ai_services-050",
    "certification_id": "AIF-C01",
    "domain": "AI_SERVICES",
    "difficulty": "medium",
    "tier": "FREE",
    "required_selection_count": 1,
    "active": true,
    "question_text": "Uma empresa precisa selecionar um modelo do Bedrock para uso interno e identificar um que gere respostas no estilo preferido dos funcionários. O que eles devem fazer?",
    "option_a": "Avaliar os modelos usando apenas conjuntos de dados de prompt integrados.",
    "option_b": "Avaliar os modelos com uma força de trabalho humana e conjuntos de dados de prompt personalizados.",
    "option_c": "Usar classificações de modelos públicos para escolher o modelo.",
    "option_d": "Usar métricas de latência do CloudWatch durante o teste dos modelos.",
    "correct_answers": [
      "B"
    ],
    "explanation_detailed": "O alinhamento de estilo é subjetivo e específico do domínio. Crie um conjunto de prompts representativo a partir de casos de uso internos reais e peça a revisores humanos que pontuem os resultados quanto ao tom, clareza e conformidade com a política. Automatize a avaliação sempre que possível, mas mantenha o julgamento humano central. As classificações públicas e as métricas de latência não capturam as preferências de estilo. No Bedrock, teste vários FMs com os mesmos prompts, aplique guardrails e considere um ajuste fino leve nos guias de estilo ou exemplos internos após selecionar um candidato.",
    "incorrect_explanations": {
      "A": "Os prompts integrados são genéricos e podem não refletir as necessidades de estilo de sua organização.",
      "C": "As classificações públicas raramente medem o tom e as restrições específicas da empresa.",
      "D": "A latência mede a velocidade, não o ajuste estilístico ou a qualidade."
    }
  },
  {
    "id": "aif-c01-ai_services-051",
    "certification_id": "AIF-C01",
    "domain": "AI_SERVICES",
    "difficulty": "medium",
    "tier": "FREE",
    "required_selection_count": 1,
    "active": true,
    "question_text": "Uma empresa precisa escolher um modelo do Amazon Bedrock para uso interno e identificar um modelo que produza respostas no estilo preferido pelos funcionários. O que a empresa deve fazer?",
    "option_a": "Avaliar os modelos usando conjuntos de dados de prompt integrados.",
    "option_b": "Avaliar os modelos com uma força de trabalho humana e conjuntos de dados de prompt personalizados.",
    "option_c": "Usar classificações públicas de modelos para escolher o modelo.",
    "option_d": "Usar métricas de latência de invocação de modelo do Amazon CloudWatch durante os testes.",
    "correct_answers": [
      "B"
    ],
    "explanation_detailed": "Estilo, tom e adequação à tarefa são subjetivos e específicos do domínio. A maneira mais confiável de selecionar um modelo de fundação do Amazon Bedrock para o estilo preferido de uma organização é realizar uma avaliação humana estruturada em prompts e referências que reflitam casos de uso internos reais. Crie um conjunto de prompts personalizado a partir de dados de produção (higienizados) e peça a um grupo representativo de funcionários para avaliar os resultados quanto a tom, clareza, fidelidade e aderência à política. Combine comparações lado a lado e classificações Likert, depois analise a concordância entre os avaliadores. Este método revela o alinhamento de preferências e os modos de falha que as métricas automatizadas e as classificações públicas não capturam. Na AWS, você pode orquestrar avaliações com endpoints de modelo do Bedrock, armazenar resultados no Amazon S3 e analisar no Amazon QuickSight ou em notebooks do SageMaker.",
    "incorrect_explanations": {
      "A": "Conjuntos de prompts integrados ou genéricos raramente espelham a voz, a terminologia de domínio ou as restrições de conformidade de sua empresa. Eles podem filtrar a qualidade geral, mas não medirão o alinhamento com suas preferências de estilo específicas.",
      "C": "As classificações otimizam para benchmarks públicos, não para a voz, os dados ou as políticas de segurança de sua empresa. Um modelo de ponta em tarefas públicas ainda pode estar desalinhado com o estilo de sua marca ou as necessidades de conformidade.",
      "D": "A latência é uma métrica operacional. Ajuda a dimensionar e a custear a solução, mas não diz nada sobre o alinhamento estilístico ou a qualidade da resposta para seus funcionários."
    }
  },
  {
    "id": "aif-c01-responsible_ai-052",
    "certification_id": "AIF-C01",
    "domain": "RESPONSIBLE_AI",
    "difficulty": "easy",
    "tier": "FREE",
    "required_selection_count": 1,
    "active": true,
    "question_text": "Um estudante universitário copia conteúdo de IA generativa para escrever redações. Qual desafio de IA responsável isso representa?",
    "option_a": "Toxicidade",
    "option_b": "Alucinações",
    "option_c": "Plágio",
    "option_d": "Privacidade",
    "correct_answers": [
      "C"
    ],
    "explanation_detailed": "Usar texto gerado por modelo sem atribuição constitui plágio. As práticas de IA responsável exigem proveniência clara, citação e políticas que impeçam a apresentação de conteúdo gerado como trabalho original. Ambientes educacionais e empresariais devem estabelecer diretrizes que exijam a divulgação quando ferramentas de IA auxiliaram e, quando aplicável, citar as fontes recuperadas por meio de geração aumentada. Na AWS, as Guardrails para Amazon Bedrock podem ser configuradas para solicitar divulgações e desencorajar o uso indevido, e o registro do Amazon Bedrock pode registrar prompts e saídas para auditoria. Soluções de autenticidade de conteúdo e detecção de marca d'água (quando disponíveis) podem complementar a política. A questão central aqui não é toxicidade ou privacidade, mas a atribuição incorreta de autoria e a falha em manter a integridade acadêmica.",
    "incorrect_explanations": {
      "A": "A toxicidade diz respeito a resultados prejudiciais ou ofensivos. O cenário foca na atribuição incorreta de autoria, não em linguagem prejudicial.",
      "B": "As alucinações são resultados confiantes, mas incorretos. Mesmo que o texto fosse preciso, copiá-lo sem atribuição ainda é plágio.",
      "D": "As questões de privacidade envolvem a exposição de dados pessoais ou sensíveis. O problema central aqui é a atribuição e a autoria inadequadas."
    }
  },
  {
    "id": "aif-c01-ai_services-053",
    "certification_id": "AIF-C01",
    "domain": "AI_SERVICES",
    "difficulty": "medium",
    "tier": "FREE",
    "required_selection_count": 1,
    "active": true,
    "question_text": "Uma empresa precisa treinar seu próprio LLM usando apenas dados privados e está preocupada com o impacto ambiental. Qual tipo de instância do Amazon EC2 tem o menor impacto ambiental para o treinamento de LLMs?",
    "option_a": "Série C do EC2",
    "option_b": "Série G do EC2",
    "option_c": "Série P do EC2",
    "option_d": "Série Trn do EC2",
    "correct_answers": [
      "D"
    ],
    "explanation_detailed": "As instâncias Trn do EC2 (Trn1/Trn1n), alimentadas pelo AWS Trainium, são construídas especificamente para o treinamento eficiente em energia e custo de modelos de aprendizado profundo em escala. Em relação às instâncias de GPU gerais, o Trainium pode oferecer maior desempenho por watt e menor energia total para grandes cargas de trabalho de transformadores, fornecendo maior taxa de transferência, aceleração de precisão mista e comunicações coletivas otimizadas. O Trainium se integra com o SDK AWS Neuron, PyTorch/XLA e pilhas de LLM populares para reduzir as alterações de código enquanto aumenta a utilização de hardware. O uso do Trn pode, portanto, reduzir as pegadas de carbono e de custo para o treinamento de LLMs de longa duração em comparação com as GPUs da série P, enquanto as instâncias C/G não são projetadas para treinamento em grande escala. Combine com armazenamento gerenciado (Amazon S3) e spot quando apropriado para otimizar ainda mais a sustentabilidade.",
    "incorrect_explanations": {
      "A": "A série C são CPUs otimizadas para computação, adequadas para computação geral, mas não eficientes para treinamento de aprendizado profundo em grande escala.",
      "B": "A série G visa inferência/gráficos com GPUs menores e não é otimizada para o treinamento de LLMs com eficiência de custo ou energia.",
      "C": "As GPUs da série P são fortes para treinamento, mas normalmente consomem mais energia por taxa de transferência equivalente do que o Trainium para cargas de trabalho de transformadores."
    }
  },
  {
    "id": "aif-c01-ai_services-054",
    "certification_id": "AIF-C01",
    "domain": "AI_SERVICES",
    "difficulty": "medium",
    "tier": "FREE",
    "required_selection_count": 1,
    "active": true,
    "question_text": "Uma empresa construirá um gerador de histórias para crianças no Amazon Bedrock e precisa garantir que os resultados e os tópicos permaneçam apropriados para crianças. Qual capacidade da AWS satisfaz isso?",
    "option_a": "Amazon Rekognition",
    "option_b": "Playgrounds do Amazon Bedrock",
    "option_c": "Guardrails para Amazon Bedrock",
    "option_d": "Agents para Amazon Bedrock",
    "correct_answers": [
      "C"
    ],
    "explanation_detailed": "As Guardrails para Amazon Bedrock fornecem controles de política para moldar e restringir o comportamento do modelo no momento da inferência. Você pode definir categorias de segurança, tópicos não permitidos, manuseio de PII e listas personalizadas de palavras/frases bloqueadas e, em seguida, aplicar a guardrail a vários modelos do Bedrock de forma consistente. Isso é ideal para um aplicativo de contação de histórias para crianças, onde você precisa filtrar conteúdo violento, sexual ou inadequado para a idade e impedir que o modelo responda a prompts fora do escopo aceitável. As Guardrails também suportam a transformação da resposta (por exemplo, recusa ou redação) e registro para auditorias. O Rekognition resolve tarefas de visão, os playgrounds são para experimentação e os Agents orquestram ferramentas/conhecimento, mas não impõem políticas de segurança por si só.",
    "incorrect_explanations": {
      "A": "O Rekognition detecta objetos, texto e rostos em imagens e vídeos; não filtra o texto do LLM para adequação à idade.",
      "B": "Os playgrounds são para testes ad-hoc; eles não impõem políticas de segurança de produção em todos os endpoints.",
      "D": "Os Agents orquestram ações e ferramentas. Sem guardrails, eles não bloquearão inerentemente conteúdo inseguro ou inadequado para a idade."
    }
  },
  {
    "id": "aif-c01-ai_fundamentals-055",
    "certification_id": "AIF-C01",
    "domain": "AI_FUNDAMENTALS",
    "difficulty": "easy",
    "tier": "FREE",
    "required_selection_count": 1,
    "active": true,
    "question_text": "Uma empresa precisa gerar dados sintéticos com base em dados existentes. Qual tipo de modelo se encaixa?",
    "option_a": "Rede Adversária Gerativa (GAN)",
    "option_b": "XGBoost",
    "option_c": "Rede neural residual",
    "option_d": "WaveNet",
    "correct_answers": [
      "A"
    ],
    "explanation_detailed": "As GANs emparelham um gerador e um discriminador em um jogo minimax: o gerador produz candidatos e o discriminador distingue o real do sintético. Durante o treinamento, o gerador aprende a produzir amostras realistas que correspondem à distribuição de dados reais. As GANs são amplamente usadas para criar imagens, dados tabulares e até séries temporais sob restrições de privacidade e desequilíbrio. Na AWS, você pode prototipar no Amazon SageMaker usando frameworks como PyTorch ou TensorFlow, acompanhar experimentos com o SageMaker Experiments e armazenar conjuntos de dados no Amazon S3. O XGBoost é um algoritmo de árvore de decisão com gradient boosting para tarefas supervisionadas, as redes residuais são redes profundas discriminativas e o WaveNet visa a geração de áudio, não a geração geral de dados tabulares/de imagem sintéticos.",
    "incorrect_explanations": {
      "B": "O XGBoost se destaca em classificação/regressão, mas não gera novas distribuições de dados.",
      "C": "As ResNets são principalmente arquiteturas discriminativas para visão; não são modelos gerativos.",
      "D": "O WaveNet é especializado na geração de formas de onda de áudio; é inadequado para a geração de dados sintéticos genéricos em todas as modalidades."
    }
  },
  {
    "id": "aif-c01-ai_services-056",
    "certification_id": "AIF-C01",
    "domain": "AI_SERVICES",
    "difficulty": "medium",
    "tier": "FREE",
    "required_selection_count": 1,
    "active": true,
    "question_text": "Uma empresa de dispositivos quer previsão de demanda para hardware de memória, não tem habilidades de codificação e precisa analisar conjuntos de dados internos e externos. Qual solução da AWS se encaixa?",
    "option_a": "Armazenar no Amazon S3 e treinar com algoritmos integrados do SageMaker a partir do S3.",
    "option_b": "Usar o SageMaker Data Wrangler e algoritmos integrados diretamente.",
    "option_c": "Usar o SageMaker Data Wrangler com o Amazon Personalize Trending-Now.",
    "option_d": "Usar o Amazon SageMaker Canvas para construir modelos e previsões sem código.",
    "correct_answers": [
      "D"
    ],
    "explanation_detailed": "O Amazon SageMaker Canvas fornece uma interface sem código para que os usuários de negócios construam, avaliem e implantem modelos de ML usando engenharia de características e seleção de algoritmos automatizadas. Ele se conecta a fontes de dados como Amazon S3, Redshift e feeds de terceiros, permitindo que os usuários criem previsões de demanda e análises de cenários hipotéticos sem escrever código. O Canvas aproveita o SageMaker Autopilot nos bastidores e pode entregar modelos ao SageMaker Studio para que os especialistas revisem. O Data Wrangler é poderoso para a preparação de dados, mas assume habilidades de praticante; o Personalize é um serviço de recomendação, não um mecanismo de previsão geral; e o treinamento de algoritmos integrados diretamente requer conhecimento de ML e script. O Canvas se encaixa melhor na necessidade de previsão de múltiplas fontes sem desenvolvedores descrita.",
    "incorrect_explanations": {
      "A": "O treinamento de algoritmos integrados a partir do S3 precisa de codificação e conhecimento de ML, que a empresa não possui.",
      "B": "O Data Wrangler foca na preparação de dados, não na construção de modelos de ponta a ponta sem código para não especialistas.",
      "C": "O Amazon Personalize é para sistemas de recomendação, não para previsão de demanda geral em dados heterogêneos."
    }
  },
  {
    "id": "aif-c01-responsible_ai-057",
    "certification_id": "AIF-C01",
    "domain": "RESPONSIBLE_AI",
    "difficulty": "medium",
    "tier": "FREE",
    "required_selection_count": 1,
    "active": true,
    "question_text": "Um modelo de câmera de segurança sinaliza furto em loja desproporcionalmente para um grupo étnico. Qual viés é mais provável?",
    "option_a": "Viés de medição",
    "option_b": "Viés de amostragem",
    "option_c": "Viés de observador",
    "option_d": "Viés de confirmação",
    "correct_answers": [
      "B"
    ],
    "explanation_detailed": "O viés de amostragem ocorre quando os dados de treinamento sub ou super-representam grupos, contextos ou condições, levando o modelo a aprender limites de decisão distorcidos. Em sistemas de visão, se certos dados demográficos forem sub-representados ou capturados em iluminação, ângulos ou qualidade de câmera diferentes, o modelo pode generalizar mal e produzir falsos positivos díspares. A mitigação de IA responsável inclui a auditoria da composição do conjunto de dados, o rebalanceamento com coleta ou aumento direcionados e a avaliação usando métricas por grupo (por exemplo, probabilidades equalizadas, paridade demográfica). Na AWS, você pode usar o Amazon SageMaker Clarify para calcular métricas de viés pré e pós-treinamento e para gerar artefatos de explicabilidade. O viés de medição se concentra em rótulos/sensores defeituosos; os vieses de observador e de confirmação se referem ao julgamento humano e à seleção orientada por hipóteses, respectivamente.",
    "incorrect_explanations": {
      "A": "O viés de medição diz respeito a sensores descalibrados ou rótulos inconsistentes, não à sub-representação de grupos no conjunto de dados.",
      "C": "O viés do observador surge dos julgamentos subjetivos dos anotadores humanos, não necessariamente da representatividade do conjunto de dados.",
      "D": "O viés de confirmação é a seleção de evidências para confirmar uma hipótese. O cenário aponta para um desequilíbrio de representação."
    }
  },
  {
    "id": "aif-c01-ai_fundamentals-058",
    "certification_id": "AIF-C01",
    "domain": "AI_FUNDAMENTALS",
    "difficulty": "medium",
    "tier": "FREE",
    "required_selection_count": 1,
    "active": true,
    "question_text": "Um chatbot de atendimento ao cliente deve melhorar aprendendo com interações passadas e recursos online. Qual estratégia de aprendizado permite a auto-melhoria contínua?",
    "option_a": "Aprendizado supervisionado com um conjunto selecionado de respostas boas e ruins",
    "option_b": "Aprendizado por reforço com recompensas por feedback positivo do cliente",
    "option_c": "Aprendizado não supervisionado para agrupar consultas de clientes semelhantes",
    "option_d": "Aprendizado supervisionado com um banco de dados de perguntas frequentes continuamente atualizado",
    "correct_answers": [
      "B"
    ],
    "explanation_detailed": "O aprendizado por reforço (RL) otimiza o comportamento maximizando a recompensa cumulativa. Para chatbots, você pode projetar um sinal de recompensa a partir de classificações explícitas, resultados de resolução ou sinais proxy (por exemplo, tempo de atendimento reduzido). Um agente de RL explora estratégias de resposta, recebe feedback e atualiza as políticas para melhorar as respostas futuras. Na AWS, você pode combinar o Amazon Bedrock para LLMs base, coletar telemetria de interação no Amazon Kinesis/Firehose e calcular recompensas com pilhas de AWS Lambda ou SageMaker RL. O ajuste fino supervisionado em pares rotulados ajuda na qualidade inicial, mas não otimiza inerentemente para resultados a longo prazo. O agrupamento organiza os dados, mas não otimiza as ações. A atualização das perguntas frequentes melhora o acesso ao conhecimento, mas sem um framework de recompensa o modelo não aprenderá sistematicamente as políticas.",
    "incorrect_explanations": {
      "A": "Conjuntos de dados supervisionados ajudam na precisão inicial, mas não otimizam estratégias de conversação a longo prazo por meio de recompensas.",
      "C": "O agrupamento ajuda a descobrir padrões, mas não adapta o comportamento com base em feedback ou resultados.",
      "D": "Alimentar as perguntas frequentes adiciona conhecimento; não define um mecanismo de recompensa para impulsionar a otimização do comportamento."
    }
  },
  {
    "id": "aif-c01-ai_fundamentals-059",
    "certification_id": "AIF-C01",
    "domain": "AI_FUNDAMENTALS",
    "difficulty": "easy",
    "tier": "FREE",
    "required_selection_count": 1,
    "active": true,
    "question_text": "Um praticante de ML construiu um classificador de aprendizado profundo para tipos de materiais em imagens e quer medir o desempenho. Qual métrica ajuda?",
    "option_a": "Matriz de confusão",
    "option_b": "Matriz de correlação",
    "option_c": "Pontuação R²",
    "option_d": "Erro quadrático médio (MSE)",
    "correct_answers": [
      "A"
    ],
    "explanation_detailed": "Uma matriz de confusão tabula as contagens de verdadeiros positivos, falsos positivos, verdadeiros negativos e falsos negativos por classe. A partir dela, você deriva acurácia, precisão, recall, F1 e análise de erro por classe. Isso é essencial para tarefas de visão de múltiplas classes, onde a acurácia agregada pode ocultar falhas de classes minoritárias. Uma matriz de correlação mede as correlações de características, não o desempenho da classificação. R² e MSE são métricas de regressão e não se aplicam a rótulos discretos. Na AWS, você pode calcular matrizes de confusão em notebooks do Amazon SageMaker usando o scikit-learn, armazenar artefatos no Amazon S3 e visualizar erros por classe para orientar a coleta de dados e o aumento, o ajuste de limiares ou o rebalanceamento de classes.",
    "incorrect_explanations": {
      "B": "As matrizes de correlação revelam relações entre as variáveis, não os resultados de classificação preditiva.",
      "C": "O R² avalia o ajuste de regressão; não é definido para previsões categóricas.",
      "D": "O MSE é uma perda de regressão e não descreve diretamente o erro de classificação por classe."
    }
  },
  {
    "id": "aif-c01-ai_services-060",
    "certification_id": "AIF-C01",
    "domain": "AI_SERVICES",
    "difficulty": "medium",
    "tier": "FREE",
    "required_selection_count": 1,
    "active": true,
    "question_text": "Um chatbot responde a perguntas em linguagem natural com imagens. A empresa precisa garantir que ele nunca retorne imagens inadequadas. Qual solução se encaixa?",
    "option_a": "Implementar APIs de moderação",
    "option_b": "Treinar novamente o modelo em um conjunto de dados público amplo",
    "option_c": "Realizar validação do modelo",
    "option_d": "Automatizar a integração de feedback do usuário",
    "correct_answers": [
      "A"
    ],
    "explanation_detailed": "A moderação de conteúdo deve acontecer no momento da inferência para impor a política de forma confiável. Implemente APIs de moderação tanto para os prompts de texto (para bloquear solicitações inseguras) quanto para as saídas de imagem (para analisar e filtrar a mídia gerada ou recuperada). Na AWS, combine as Guardrails para Amazon Bedrock para políticas de prompt/saída com as APIs de moderação do Amazon Rekognition para classes de segurança de imagem. Você também pode manter listas de negação e adicionar revisão por intervenção humana para casos extremos usando o Amazon A2I. O retreinamento pode reduzir o risco, mas não pode garantir a conformidade. A validação e os loops de feedback são valiosos, mas sem a moderação em tempo de execução você corre o risco de violações da política passarem.",
    "incorrect_explanations": {
      "B": "O retreinamento melhora as médias, mas não pode garantir que nenhuma imagem insegura será produzida ou retornada.",
      "C": "A validação é um teste pré-implantação; não impõe segurança em respostas em tempo real.",
      "D": "O feedback melhora o comportamento futuro, mas não pode bloquear de forma confiável conteúdo inseguro no momento."
    }
  },
  {
    "id": "aif-c01-ai_services-061",
    "certification_id": "AIF-C01",
    "domain": "AI_SERVICES",
    "difficulty": "medium",
    "tier": "FREE",
    "required_selection_count": 1,
    "active": true,
    "question_text": "Um praticante de IA usa um modelo base do Amazon Bedrock para resumir chats de suporte ao cliente e quer armazenar os logs de invocação para monitorar entradas e saídas. O que eles devem fazer?",
    "option_a": "Configurar o AWS CloudTrail como o destino de log do modelo.",
    "option_b": "Habilitar o registro de invocação de modelo no Amazon Bedrock.",
    "option_c": "Usar o AWS Audit Manager como o destino de log para o modelo.",
    "option_d": "Configurar o registro de invocação de modelo no Amazon EventBridge.",
    "correct_answers": [
      "B"
    ],
    "explanation_detailed": "O Amazon Bedrock fornece registro nativo de invocação de modelo para capturar prompts, respostas e metadados (sujeito às suas políticas de retenção e privacidade). Você pode direcionar os logs para o Amazon S3 e integrar com o AWS CloudTrail para auditoria do plano de controle, mas o CloudTrail não captura as cargas úteis. Com o registro do Bedrock, as equipes podem auditar o uso, depurar casos de falha e construir conjuntos de dados de avaliação. Você também deve aplicar políticas de redação de dados, criptografia com o AWS KMS e controles de acesso via IAM. O EventBridge é para roteamento de eventos, não para registro de carga útil, e o Audit Manager é para fluxos de trabalho de auditoria e coleta de evidências, em vez de capturar o conteúdo de entrada/saída do modelo.",
    "incorrect_explanations": {
      "A": "O CloudTrail registra chamadas de API e identidades, não as cargas úteis completas de entrada/saída do modelo necessárias para o monitoramento de qualidade.",
      "C": "O Audit Manager gerencia programas e evidências de auditoria; não é usado para capturar as cargas úteis de inferência.",
      "D": "O EventBridge roteia eventos; não é um mecanismo nativo para registrar prompts e respostas do Bedrock."
    }
  },
  {
    "id": "aif-c01-ai_services-062",
    "certification_id": "AIF-C01",
    "domain": "AI_SERVICES",
    "difficulty": "medium",
    "tier": "FREE",
    "required_selection_count": 1,
    "active": true,
    "question_text": "Uma empresa precisa executar inferência sobre conjuntos de dados arquivados de vários GB e não precisa de previsões imediatas. Qual opção de inferência do SageMaker se encaixa?",
    "option_a": "Transformação em Lote",
    "option_b": "Inferência em tempo real",
    "option_c": "Inferência sem servidor",
    "option_d": "Inferência assíncrona",
    "correct_answers": [
      "A"
    ],
    "explanation_detailed": "A Transformação em Lote do SageMaker processa grandes conjuntos de dados de forma assíncrona, lendo do Amazon S3, executando a inferência em escala e gravando os resultados de volta no S3. É ideal quando a latência não é crítica, as instâncias podem ser dimensionadas corretamente para a taxa de transferência e você evita gerenciar endpoints de solicitação/resposta. Os endpoints em tempo real e sem servidor visam latências de milissegundos a segundos e cargas de trabalho de solicitação/resposta. A inferência assíncrona é melhor para grandes cargas úteis individuais que ainda retornam resultados por solicitação via retorno de chamada, não trabalhos de conjunto de dados completos. Para arquivos de vários GB e SLAs não interativos, a Transformação em Lote minimiza a sobrecarga operacional e o custo.",
    "incorrect_explanations": {
      "B": "Os endpoints em tempo real são otimizados para inferência por solicitação de baixa latência, não para processamento em massa de arquivos.",
      "C": "A inferência sem servidor é adequada para cargas de trabalho de solicitação/resposta esporádicas e de pequena carga útil, não para pontuação em lote em massa.",
      "D": "A inferência assíncrona lida com grandes solicitações únicas, mas não é otimizada para trabalhos em lote de conjunto de dados completos."
    }
  },
  {
    "id": "aif-c01-ai_fundamentals-063",
    "certification_id": "AIF-C01",
    "domain": "AI_FUNDAMENTALS",
    "difficulty": "easy",
    "tier": "FREE",
    "required_selection_count": 1,
    "active": true,
    "question_text": "Qual termo descreve representações numéricas de objetos e conceitos do mundo real que ajudam os modelos de PNL/IA a entender o texto?",
    "option_a": "Incorporações (Embeddings)",
    "option_b": "Tokens",
    "option_c": "Modelos",
    "option_d": "Binários",
    "correct_answers": [
      "A"
    ],
    "explanation_detailed": "As incorporações mapeiam símbolos discretos (palavras, frases, imagens) para vetores densos, de modo que a similaridade semântica corresponda à proximidade geométrica. Elas impulsionam a recuperação, o agrupamento, a classificação e a busca semântica. Em aplicações de LLM, as incorporações de texto permitem pipelines de RAG usando bancos de dados vetoriais (por exemplo, OpenSearch k-NN) para encontrar passagens contextualmente relevantes. Os tokens são as unidades atômicas de entrada/saída; os modelos são as arquiteturas/parâmetros; os binários são artefatos compilados. Na AWS, você pode calcular incorporações com modelos de incorporação de texto do Amazon Bedrock e armazená-las no Amazon OpenSearch Service ou no Aurora PostgreSQL com o pgvector para suportar a busca por similaridade em escala.",
    "incorrect_explanations": {
      "B": "Os tokens são unidades de entrada/saída, não representações vetoriais semânticas para cálculos de similaridade.",
      "C": "Um modelo é a função aprendida; as incorporações são representações de dados que o modelo usa.",
      "D": "Os binários são executáveis compilados, não relacionados a representações semânticas."
    }
  },
  {
    "id": "aif-c01-ml_development-064",
    "certification_id": "AIF-C01",
    "domain": "ML_DEVELOPMENT",
    "difficulty": "medium",
    "tier": "FREE",
    "required_selection_count": 1,
    "active": true,
    "question_text": "Uma empresa de pesquisa usou um FM do Amazon Bedrock para um chatbot de perguntas e respostas sobre artigos científicos. A engenharia de prompt falhou devido à terminologia complexa. Como eles podem melhorar o desempenho?",
    "option_a": "Usar o prompting de poucas tentativas (few-shot) para definir o estilo da resposta.",
    "option_b": "Aplicar o ajuste fino de adaptação de domínio para ensinar a terminologia científica.",
    "option_c": "Alterar os parâmetros de inferência.",
    "option_d": "Limpar os artigos para remover termos complexos.",
    "correct_answers": [
      "B"
    ],
    "explanation_detailed": "A adaptação de domínio ajusta um FM base em corpora específicos do domínio para que o modelo internalize o vocabulário, a sintaxe e os padrões de discurso especializados. Para perguntas e respostas científicas, o ajuste fino em artigos selecionados, glossários e pares de perguntas e respostas elaborados por especialistas pode melhorar materialmente a compreensão e o embasamento factual. No Amazon Bedrock, use a personalização de modelo (onde suportado) com pares de prompt-conclusão rotulados e avalie por meio de benchmarks de domínio retidos. O prompting de poucas tentativas e os ajustes de parâmetros podem ajudar no estilo e no determinismo, mas raramente fecham as lacunas de conhecimento para uma linguagem altamente técnica. A remoção de termos complexos destrói o sinal necessário. A combinação de ajuste fino com recuperação (Bases de Conhecimento do Bedrock ou OpenSearch) gera ganhos adicionais.",
    "incorrect_explanations": {
      "A": "O prompting de poucas tentativas melhora a formatação e a indução da tarefa, mas não pode ensinar o vocabulário profundo do domínio de forma abrangente.",
      "C": "Temperatura, top-p, etc., afetam a diversidade e o determinismo, não a aquisição de conhecimento de domínio.",
      "D": "A exclusão de termos complexos remove informações essenciais e prejudica a qualidade da resposta."
    }
  },
  {
    "id": "aif-c01-ai_fundamentals-065",
    "certification_id": "AIF-C01",
    "domain": "AI_FUNDAMENTALS",
    "difficulty": "medium",
    "tier": "FREE",
    "required_selection_count": 1,
    "active": true,
    "question_text": "Para fazer um LLM no Amazon Bedrock produzir respostas mais consistentes para o mesmo prompt, qual mudança de parâmetro de inferência ajuda mais?",
    "option_a": "Diminuir a temperatura",
    "option_b": "Aumentar a temperatura",
    "option_c": "Reduzir o máximo de tokens de saída",
    "option_d": "Aumentar o comprimento máximo de geração",
    "correct_answers": [
      "A"
    ],
    "explanation_detailed": "A temperatura controla a aleatoriedade da amostragem. Valores mais baixos inclinam a distribuição para tokens de maior probabilidade, aumentando o determinismo e a repetibilidade das saídas para a mesma entrada — útil para tarefas de classificação, extração e vinculadas a políticas. No Bedrock, combine baixa temperatura com sementes fixas (onde disponível) e decodificação restrita quando possível. A alteração do máximo de tokens afeta o comprimento, não a variabilidade; o aumento do comprimento pode até introduzir mais variação no final da geração. Os ajustes de top-p/top-k também influenciam a aleatoriedade, mas a temperatura é o principal botão para a nitidez global da amostragem. Combine com prompts robustos e, se necessário, guardrails para imposição de formato.",
    "incorrect_explanations": {
      "B": "Temperatura mais alta aumenta a aleatoriedade e a diversidade — o oposto da consistência.",
      "C": "A limitação de tokens trunca as saídas, mas não reduz fundamentalmente a variância da amostragem.",
      "D": "Saídas mais longas podem introduzir variabilidade adicional e desvio sem melhorar a consistência."
    }
  },
  {
    "id": "aif-c01-ai_services-066",
    "certification_id": "AIF-C01",
    "domain": "AI_SERVICES",
    "difficulty": "medium",
    "tier": "FREE",
    "required_selection_count": 1,
    "active": true,
    "question_text": "Uma empresa constrói um aplicativo LLM no Amazon Bedrock com dados do cliente no S3. Política de segurança: cada equipe pode acessar apenas os dados de seus próprios clientes. O que eles devem fazer?",
    "option_a": "Criar uma função de serviço dedicada do Amazon Bedrock por equipe com escopo apenas para os dados do S3 dessa equipe.",
    "option_b": "Criar uma única função de serviço com acesso ao S3 e confiar nas equipes para passar um nome de cliente em cada solicitação.",
    "option_c": "Redigir PII no S3 e abrir o acesso ao S3 amplamente para as equipes.",
    "option_d": "Dar ao Bedrock acesso total ao S3 e restringir via funções do IAM da equipe a pastas.",
    "correct_answers": [
      "A"
    ],
    "explanation_detailed": "Use o IAM de menor privilégio com uma função de serviço distinta do Amazon Bedrock por equipe. Limite as permissões do S3 de cada função (ARNs de recursos e chaves de condição como aws:PrincipalTag ou s3:prefix) aos buckets/prefixos dessa equipe. Isso garante que as invocações do Bedrock em nome de uma equipe só possam acessar os dados do cliente dessa equipe. Passar um nome de cliente não é controle de acesso. O acesso amplo ao S3 viola a política, mesmo que os dados sejam redigidos. Conceder ao Bedrock acesso total ao S3 prejudica o isolamento e complica a auditoria. Combine com criptografia do KMS, políticas de bucket do S3 e CloudTrail para auditoria de acesso.",
    "incorrect_explanations": {
      "B": "Confiar na entrada do usuário não é um limite de autorização e é propenso a erros.",
      "C": "A redação de PII não impõe o isolamento do inquilino; o acesso amplo ainda viola a política.",
      "D": "O acesso total ao S3 para o Bedrock quebra o menor privilégio e aumenta o raio de impacto."
    }
  },
  {
    "id": "aif-c01-ai_services-067",
    "certification_id": "AIF-C01",
    "domain": "AI_SERVICES",
    "difficulty": "medium",
    "tier": "FREE",
    "required_selection_count": 1,
    "active": true,
    "question_text": "Uma empresa médica implantou um modelo de detecção de doenças no Amazon Bedrock. Ela precisa impedir que as PII do paciente apareçam nas respostas e receber notificações sobre violações de política. O que se encaixa?",
    "option_a": "Usar o Amazon Macie para escanear as saídas e alertar sobre dados sensíveis.",
    "option_b": "Usar o AWS CloudTrail para monitorar as respostas e alertar sobre PII.",
    "option_c": "Usar as Guardrails para Amazon Bedrock para filtrar o conteúdo e os alarmes do CloudWatch para notificações de violação.",
    "option_d": "Usar o SageMaker Model Monitor para desvio de dados e alertas de qualidade.",
    "correct_answers": [
      "C"
    ],
    "explanation_detailed": "As Guardrails para Amazon Bedrock podem detectar e redigir PII ou bloquear respostas que violam as políticas de conteúdo, impedindo que dados sensíveis sejam retornados. Configure as guardrails com detecção de PII, listas de negação personalizadas e transformações de resposta segura. Transmita os logs para o CloudWatch e defina filtros/alarmes de métricas para eventos de violação para notificar as equipes. O Macie descobre dados sensíveis em repouso, não dinamicamente em saídas geradas. O CloudTrail rastreia chamadas de API, não cargas úteis de resposta. O Model Monitor foca em desvio e qualidade de dados para endpoints do SageMaker, não na aplicação de políticas de conteúdo do Bedrock. Guardrails + CloudWatch é a combinação correta de controle em tempo de execução e alerta.",
    "incorrect_explanations": {
      "A": "O Macie classifica os dados em repouso no S3; não filtra ou redige as saídas do modelo em tempo real.",
      "B": "O CloudTrail registra eventos do plano de controle e não possui inspeção de carga útil de saída para PII.",
      "D": "O Model Monitor detecta desvio/qualidade em endpoints do SageMaker, não na aplicação da política de conteúdo do Bedrock."
    }
  },
  {
    "id": "aif-c01-ai_services-068",
    "certification_id": "AIF-C01",
    "domain": "AI_SERVICES",
    "difficulty": "easy",
    "tier": "FREE",
    "required_selection_count": 1,
    "active": true,
    "question_text": "A empresa precisa converter muitos currículos em PDF para texto simples para processamento automatizado. Qual serviço da AWS se encaixa?",
    "option_a": "Amazon Textract",
    "option_b": "Amazon Personalize",
    "option_c": "Amazon Lex",
    "option_d": "Amazon Transcribe",
    "correct_answers": [
      "A"
    ],
    "explanation_detailed": "O Amazon Textract extrai texto, formulários, tabelas e pares de chave-valor de PDFs e imagens digitalizados. Ele lida com layouts variáveis e retorna JSON estruturado para análise downstream. Para currículos, você pode enviar documentos do S3 para o Textract por meio de trabalhos assíncronos, armazenar os resultados de volta no S3 e indexá-los no OpenSearch ou em um banco de dados. O Transcribe converte fala em texto, o Lex constrói chatbots e o Personalize é para recomendações. O Textract é construído especificamente para OCR de documentos e extração de estrutura necessários em pipelines de processamento de currículos.",
    "incorrect_explanations": {
      "B": "O Personalize constrói sistemas de recomendação, não relacionados a OCR e extração de texto de PDF.",
      "C": "O Lex é um serviço de interface de conversação; não analisa documentos.",
      "D": "O Transcribe lida com áudio para texto, não com OCR de imagem/PDF."
    }
  },
  {
    "id": "aif-c01-ai_services-069",
    "certification_id": "AIF-C01",
    "domain": "AI_SERVICES",
    "difficulty": "medium",
    "tier": "FREE",
    "required_selection_count": 1,
    "active": true,
    "question_text": "Um provedor de educação constrói um aplicativo de perguntas e respostas com um modelo gerativo e quer que o estilo da resposta se adapte automaticamente à faixa etária do usuário (fornecida ao modelo). Qual é a solução de menor esforço?",
    "option_a": "Ajustar o modelo em conjuntos de dados que abrangem todas as idades suportadas.",
    "option_b": "Adicionar uma descrição de função/sistema no prompt instruindo o modelo sobre a faixa etária alvo.",
    "option_c": "Usar o raciocínio de cadeia de pensamento para inferir o estilo correto.",
    "option_d": "Pós-resumir as respostas para serem mais curtas para usuários mais jovens.",
    "correct_answers": [
      "B"
    ],
    "explanation_detailed": "O condicionamento do prompt é o método mais leve para adaptar o tom e a complexidade. Inclua instruções de função como: “Você é um tutor. Público-alvo: {faixa etária}. Use vocabulário e exemplos apropriados para esta idade.” Forneça exemplos de estilo (few-shot) se necessário e imponha restrições de comprimento. Esta abordagem evita custos de treinamento e é fácil de iterar. O ajuste fino pode ajudar, mas é excessivo para o controle de estilo simples. A cadeia de pensamento afeta as etapas de raciocínio, não a adaptação estilística garantida, e pode vazar texto interno. O pós-resumo pode encurtar o texto, mas não ajustará de forma confiável o vocabulário, a estrutura ou a carga conceitual para a idade do usuário.",
    "incorrect_explanations": {
      "A": "O ajuste fino é caro e mais lento para iterar; excessivo quando o controle de prompt simples é suficiente.",
      "C": "Os rastros de raciocínio não adaptam inerentemente o tom ou o vocabulário para faixas etárias.",
      "D": "Encurtar por si só não garante um vocabulário ou pedagogia apropriados para a idade."
    }
  },
  {
    "id": "aif-c01-ml_development-070",
    "certification_id": "AIF-C01",
    "domain": "ML_DEVELOPMENT",
    "difficulty": "medium",
    "tier": "FREE",
    "required_selection_count": 1,
    "active": true,
    "question_text": "Como você deve avaliar a precisão de um modelo de fundação usado para classificação de imagens?",
    "option_a": "Calcular o custo total de recursos usado pelo modelo.",
    "option_b": "Medir a precisão em relação a um conjunto de dados de benchmark predefinido.",
    "option_c": "Contar o número de camadas na rede neural.",
    "option_d": "Avaliar a fidelidade das cores das imagens processadas.",
    "correct_answers": [
      "B"
    ],
    "explanation_detailed": "Para avaliar um classificador, use um conjunto de dados de benchmark retido com rótulos de verdade fundamental e calcule a precisão e as métricas por classe (precisão, recall, F1). Os benchmarks permitem comparações diretas entre modelos e versões. Para uma avaliação robusta, estratifique por subpopulações e condições (iluminação, oclusão) e analise as matrizes de confusão para encontrar erros sistemáticos. Na AWS, armazene conjuntos de dados no S3, execute avaliações em notebooks do SageMaker e visualize as métricas no Amazon QuickSight. O custo dos recursos e o tamanho da arquitetura não garantem a qualidade preditiva; a fidelidade das cores é irrelevante para a precisão do rótulo, a menos que a tarefa seja explicitamente sobre a reprodução de cores.",
    "incorrect_explanations": {
      "A": "O custo indica eficiência, não a correção preditiva do modelo em imagens rotuladas.",
      "C": "A profundidade por si só não prevê a precisão; treinamento, dados e regularização importam.",
      "D": "A fidelidade das cores não está relacionada, a menos que a cor seja o rótulo alvo; a precisão da classificação precisa de benchmarks rotulados."
    }
  },
  {
    "id": "aif-c01-responsible_ai-071",
    "certification_id": "AIF-C01",
    "domain": "RESPONSIBLE_AI",
    "difficulty": "medium",
    "tier": "FREE",
    "required_selection_count": 2,
    "active": true,
    "question_text": "Uma empresa de contabilidade implantará um LLM para automatizar o processamento de documentos e deve proceder com responsabilidade para evitar danos. O que eles devem fazer? (Escolha duas.)",
    "option_a": "Incluir métricas de justiça na avaliação do modelo.",
    "option_b": "Ajustar o parâmetro de temperatura do modelo.",
    "option_c": "Modificar os dados de treinamento para mitigar vieses.",
    "option_d": "Evitar o overfitting nos dados de treinamento.",
    "option_e": "Aplicar técnicas de engenharia de prompt.",
    "correct_answers": [
      "A",
      "C"
    ],
    "explanation_detailed": "A IA responsável exige a medição e a mitigação de danos. Incorpore métricas de justiça (por exemplo, paridade demográfica, probabilidades equalizadas) e avaliações por grupo para detectar desempenho díspar entre tipos de documentos, segmentos de clientes ou idiomas. Se aparecerem disparidades, ajuste a amostragem de dados, repondere os registros ou colete dados direcionados para reduzir o viés. O ajuste de temperatura e a engenharia de prompt afetam o estilo e a aleatoriedade, mas não abordam o viés estrutural. O controle geral do overfitting é uma boa prática, mas não é específico para o risco de IA responsável. Na AWS, use o SageMaker Clarify para análise de viés e explicabilidade, as Guardrails do Bedrock para aplicação de políticas e controles de PII, e o CloudWatch mais o registro do Bedrock para auditabilidade.",
    "incorrect_explanations": {
      "B": "A temperatura controla a aleatoriedade; não mede ou corrige o viés sistêmico.",
      "D": "Evitar o overfitting é a higiene padrão do ML, não uma etapa específica de mitigação de justiça/viés.",
      "E": "O prompting pode guiar o estilo e o formato, mas não corrigirá o viés estrutural dos dados por si só."
    }
  },
  {
    "id": "aif-c01-ml_development-072",
    "certification_id": "AIF-C01",
    "domain": "ML_DEVELOPMENT",
    "difficulty": "easy",
    "tier": "FREE",
    "required_selection_count": 1,
    "active": true,
    "question_text": "Uma empresa coletou novos dados, calculou uma matriz de correlação, estatísticas básicas e visualizações. Qual estágio do pipeline de ML é este?",
    "option_a": "Pré-processamento de dados",
    "option_b": "Engenharia de características",
    "option_c": "Análise Exploratória de Dados (EDA)",
    "option_d": "Ajuste de hiperparâmetros",
    "correct_answers": [
      "C"
    ],
    "explanation_detailed": "A Análise Exploratória de Dados revela estrutura, distribuições, correlações, outliers e valores ausentes antes das decisões de modelagem. Os insights da EDA informam o pré-processamento (imputação, escalonamento), a engenharia de características (transformações informadas pelo domínio) e a seleção de modelos. Na AWS, os analistas normalmente executam a EDA em notebooks do SageMaker Studio com Pandas/Seaborn/Matplotlib, lendo dados do S3, e podem publicar resumos no QuickSight para as partes interessadas. O pré-processamento e a engenharia de características são etapas subsequentes guiadas pelas descobertas da EDA; o ajuste de hiperparâmetros vem depois que você define uma abordagem de modelagem e características.",
    "incorrect_explanations": {
      "A": "O pré-processamento aplica transformações; a EDA precede e informa essas escolhas.",
      "B": "A engenharia de características cria/transforma variáveis após entender os dados via EDA.",
      "D": "O ajuste ocorre pós-seleção do modelo; não na fase de exploração de dados."
    }
  },
  {
    "id": "aif-c01-ai_fundamentals-073",
    "certification_id": "AIF-C01",
    "domain": "AI_FUNDAMENTALS",
    "difficulty": "medium",
    "tier": "FREE",
    "required_selection_count": 1,
    "active": true,
    "question_text": "Alguns documentos perderam palavras devido a um erro de banco de dados. A empresa quer um modelo para sugerir palavras prováveis para preencher as lacunas. Qual tipo de modelo se encaixa?",
    "option_a": "Modelagem de tópicos",
    "option_b": "Modelos de agrupamento",
    "option_c": "Modelos de ML prescritivos",
    "option_d": "Modelos baseados em BERT",
    "correct_answers": [
      "D"
    ],
    "explanation_detailed": "O BERT e modelos de linguagem mascarada semelhantes são treinados para reconstruir tokens ausentes, dado o contexto tanto à esquerda quanto à direita. Este condicionamento bidirecional produz um forte desempenho em tarefas de preenchimento de lacunas. Para produção, você pode implantar uma variante do BERT no SageMaker ou usar um FM do Bedrock com prompts de mascaramento. A modelagem de tópicos e o agrupamento são não supervisionados e não preveem palavras ausentes específicas. O ML prescritivo refere-se à otimização de decisões, não à reconstrução de tokens. Garanta a privacidade removendo as PII antes da inferência e registre as previsões no S3 com versionamento para auditar as edições.",
    "incorrect_explanations": {
      "A": "A modelagem de tópicos descobre temas latentes, não a previsão precisa de tokens para lacunas.",
      "B": "O agrupamento agrupa itens semelhantes; não infere palavras ausentes exatas.",
      "C": "A análise prescritiva otimiza as ações, não a conclusão de texto."
    }
  },
  {
    "id": "aif-c01-ai_services-074",
    "certification_id": "AIF-C01",
    "domain": "AI_SERVICES",
    "difficulty": "medium",
    "tier": "FREE",
    "required_selection_count": 1,
    "active": true,
    "question_text": "Uma empresa quer gráficos automatizados do total de vendas dos principais produtos em locais de varejo nos últimos 12 meses. Qual solução da AWS eles devem usar?",
    "option_a": "Amazon Q no Amazon EC2",
    "option_b": "Amazon Q Developer",
    "option_c": "Amazon Q no Amazon QuickSight",
    "option_d": "Amazon Q no AWS Chatbot",
    "correct_answers": [
      "C"
    ],
    "explanation_detailed": "O Amazon QuickSight com o Amazon Q fornece BI orientado por linguagem natural. Os analistas podem perguntar: “Mostre as vendas dos últimos 12 meses para os principais produtos por local”, e o Q constrói visuais a partir de modelos semânticos governados. Agende painéis, defina segurança em nível de linha e compartilhe insights. O Q Developer foca em tarefas de código e software; o EC2 é desnecessário para BI gerenciado; o AWS Chatbot integra o bate-papo com as operações da AWS, não com visualizações de BI. Para análise de vendas em lojas, o QuickSight + Q é a solução de análise gerenciada correta na AWS.",
    "incorrect_explanations": {
      "A": "Executar o Q no EC2 é desnecessário; o QuickSight é o serviço de BI gerenciado com o Q integrado.",
      "B": "O Q Developer visa fluxos de trabalho de desenvolvedores, não a geração de gráficos de BI para usuários de negócios.",
      "D": "O AWS Chatbot faz a ponte entre as ferramentas de bate-papo e os eventos da AWS; não constrói painéis de BI."
    }
  },
  {
    "id": "aif-c01-ml_development-075",
    "certification_id": "AIF-C01",
    "domain": "ML_DEVELOPMENT",
    "difficulty": "medium",
    "tier": "FREE",
    "required_selection_count": 1,
    "active": true,
    "question_text": "Uma empresa usa um LLM do Amazon Bedrock para detecção de intenção em um chatbot e quer aplicar o aprendizado de poucas tentativas (few-shot). De quais dados adicionais eles precisam?",
    "option_a": "Respostas do chatbot pareadas com as intenções corretas dos usuários",
    "option_b": "Mensagens do usuário pareadas com as respostas corretas do chatbot",
    "option_c": "Mensagens do usuário pareadas com as intenções corretas do usuário",
    "option_d": "Intenções do usuário pareadas com as respostas corretas do chatbot",
    "correct_answers": [
      "C"
    ],
    "explanation_detailed": "O prompting de poucas tentativas ensina o modelo por exemplo. Para a classificação de intenção, forneça enunciados do usuário rotulados com os nomes de intenção corretos. O LLM então generaliza a partir dos exemplos para novos enunciados. As respostas são irrelevantes para treinar o rotulador de intenção e podem confundir a tarefa. Na AWS, armazene exemplos rotulados no S3, carregue-os em seus modelos de prompt do Bedrock e avalie o desempenho em um conjunto rotulado retido. Se o volume crescer, considere o ajuste fino ou um pequeno classificador supervisionado servido no SageMaker.",
    "incorrect_explanations": {
      "A": "As respostas não definem a intenção verdadeira; você precisa de pares enunciado→intenção.",
      "B": "O pareamento com respostas enquadra uma tarefa gerativa, não rótulos de classificação de intenção.",
      "D": "Os pares intenção→resposta são úteis para a seleção de respostas, não para a detecção de intenção."
    }
  },
  {
    "id": "aif-c01-ai_services-076",
    "certification_id": "AIF-C01",
    "domain": "AI_SERVICES",
    "difficulty": "medium",
    "tier": "FREE",
    "required_selection_count": 1,
    "active": true,
    "question_text": "Um modelo base do Bedrock usa 10 exemplos de poucas tentativas (few-shot) no prompt, é invocado uma vez por dia, tem bom desempenho, e a empresa quer reduzir o custo mensal. O que eles devem fazer?",
    "option_a": "Personalizar o modelo por meio de ajuste fino.",
    "option_b": "Reduzir o número de tokens no prompt.",
    "option_c": "Aumentar o número de tokens no prompt.",
    "option_d": "Usar a Taxa de Transferência Provisionada.",
    "correct_answers": [
      "B"
    ],
    "explanation_detailed": "A precificação do Bedrock é baseada em tokens. Reduzir os tokens do prompt — comprimindo instruções, abreviando exemplos ou substituindo alguns por um esquema compacto — diminui o custo sem alterar a infraestrutura. Uma invocação diária não justifica a Taxa de Transferência Provisionada. O ajuste fino pode reduzir o comprimento do prompt, mas adiciona custo de treinamento e complexidade operacional. Aumentar os tokens aumenta o custo. Mantenha a qualidade selecionando os exemplos de poucas tentativas mais informativos e impondo uma formatação concisa.",
    "incorrect_explanations": {
      "A": "O ajuste fino incorre em custo e sobrecarga de treinamento; desnecessário para uma única chamada diária.",
      "C": "Mais tokens aumentam diretamente o custo sem ganhos de qualidade comprovados aqui.",
      "D": "A Taxa de Transferência Provisionada é para cargas de trabalho sustentadas e de alto volume — não para uma chamada/dia."
    }
  },
  {
    "id": "aif-c01-ai_fundamentals-077",
    "certification_id": "AIF-C01",
    "domain": "AI_FUNDAMENTALS",
    "difficulty": "easy",
    "tier": "FREE",
    "required_selection_count": 1,
    "active": true,
    "question_text": "Um LLM gera uma cópia de marketing plausível, mas factualmente incorreta. Qual problema é este?",
    "option_a": "Vazamento de dados",
    "option_b": "Alucinação",
    "option_c": "Overfitting",
    "option_d": "Underfitting",
    "correct_answers": [
      "B"
    ],
    "explanation_detailed": "A alucinação ocorre quando um modelo produz declarações confiantes, mas falsas. Ela surge de lacunas nos dados de treinamento, mudança de distribuição ou geração não restrita. Mitigação: adicione geração aumentada por recuperação (Bases de Conhecimento do Amazon Bedrock ou OpenSearch), diminua a temperatura para tarefas determinísticas, restrinja as saídas com esquemas e adicione verificação pós-hoc. Overfitting/underfitting se referem à dinâmica de treinamento, não à factualidade da inferência; o vazamento de dados é a contaminação não intencional de treinamento/teste. O registro de prompts/saídas para auditorias e a adição de guardrails melhoram a confiabilidade na produção.",
    "incorrect_explanations": {
      "A": "O vazamento de dados diz respeito ao treinamento com dados de teste ou sensíveis, não a resultados plausíveis, mas errados.",
      "C": "O overfitting é um problema de treinamento; não explica diretamente fatos fabricados na inferência.",
      "D": "O underfitting é o sub-aprendizado de padrões, não a fabricação de detalhes críveis."
    }
  },
  {
    "id": "aif-c01-responsible_ai-078",
    "certification_id": "AIF-C01",
    "domain": "RESPONSIBLE_AI",
    "difficulty": "medium",
    "tier": "FREE",
    "required_selection_count": 1,
    "active": true,
    "question_text": "Um praticante ajustou um modelo do Bedrock com dados sensíveis e quer garantir que as inferências não exponham esses elementos sensíveis. Como eles devem evitar a exposição?",
    "option_a": "Excluir o modelo, remover os dados sensíveis do treinamento e treinar novamente.",
    "option_b": "Mascarar dinamicamente os dados sensíveis nas respostas de inferência.",
    "option_c": "Criptografar os dados sensíveis nas respostas de inferência com o SageMaker.",
    "option_d": "Criptografar os dados sensíveis dentro do modelo com o AWS KMS.",
    "correct_answers": [
      "B"
    ],
    "explanation_detailed": "Aplique a detecção e redação de PII em tempo de execução às saídas. Com as Guardrails para Amazon Bedrock, configure políticas de PII para redigir nomes, endereços, IDs e padrões personalizados. Você também pode integrar a detecção de PII do Amazon Comprehend ou regex personalizado para mascarar as saídas antes de retorná-las aos clientes. Isso evita a divulgação acidental sem retreinamento. A criptografia protege os dados em repouso/em trânsito, mas não impede que o modelo gere conteúdo sensível. O retreinamento completo pode ser o ideal, mas é caro; o mascaramento em tempo de execução fornece proteção e auditabilidade imediatas.",
    "incorrect_explanations": {
      "A": "O retreinamento pode ajudar, mas é caro e lento; os controles em tempo de execução protegem imediatamente.",
      "C": "Criptografar as respostas ainda retorna conteúdo sensível ao chamador uma vez descriptografado; não impede a divulgação.",
      "D": "O KMS protege os artefatos armazenados, não o conteúdo semântico que o modelo escolhe emitir."
    }
  },
  {
    "id": "aif-c01-ai_fundamentals-079",
    "certification_id": "AIF-C01",
    "domain": "AI_FUNDAMENTALS",
    "difficulty": "medium",
    "tier": "FREE",
    "required_selection_count": 1,
    "active": true,
    "question_text": "Uma empresa traduz manuais de treinamento do inglês para outros idiomas com LLMs e quer avaliar a precisão da tradução. Qual métrica se encaixa?",
    "option_a": "BLEU (Bilingual Evaluation Understudy)",
    "option_b": "RMSE",
    "option_c": "ROUGE",
    "option_d": "Pontuação F1",
    "correct_answers": [
      "A"
    ],
    "explanation_detailed": "O BLEU mede a sobreposição de n-gramas entre uma tradução candidata e uma ou mais referências humanas, capturando a precisão dos segmentos traduzidos. É uma métrica automática padrão para avaliação de tradução automática. O ROUGE visa o recall de sumarização, o RMSE é uma métrica de erro de regressão e o F1 é para classificação. Para uma avaliação robusta, combine o BLEU com a avaliação humana para fluência e adequação e acompanhe os termos de domínio com glossários personalizados. Na AWS, armazene as referências no S3, calcule o BLEU em notebooks do SageMaker e visualize as tendências no QuickSight.",
    "incorrect_explanations": {
      "B": "O RMSE avalia previsões numéricas, não a qualidade da tradução de idiomas.",
      "C": "O ROUGE foca na sobreposição de recall para sumarização, não na fidelidade da tradução.",
      "D": "O F1 se aplica a tarefas de classificação; a tradução precisa de métricas em nível de sequência como o BLEU."
    }
  },
  {
    "id": "aif-c01-ai_services-080",
    "certification_id": "AIF-C01",
    "domain": "AI_SERVICES",
    "difficulty": "medium",
    "tier": "FREE",
    "required_selection_count": 1,
    "active": true,
    "question_text": "Um grande varejista recebe milhares de consultas de suporte a produtos diariamente e quer usar o Agents for Amazon Bedrock. Qual benefício chave ajuda aqui?",
    "option_a": "Gerar automaticamente FMs personalizados para prever as necessidades dos clientes",
    "option_b": "Automatizar tarefas repetitivas e orquestrar fluxos de trabalho complexos",
    "option_c": "Chamar automaticamente vários FMs e consolidar os resultados",
    "option_d": "Selecionar o melhor FM por métricas predefinidas",
    "correct_answers": [
      "B"
    ],
    "explanation_detailed": "Os Agents for Amazon Bedrock dividem as tarefas em etapas, chamam ferramentas/APIs, mantêm a memória e orquestram fluxos de trabalho de várias etapas — ideal para casos de uso de suporte de alto volume. Um agente pode autenticar usuários, consultar o status do pedido via APIs, gerar resumos e escalar quando necessário, reduzindo o tempo de atendimento. Eles não criam FMs personalizados nem selecionam FMs automaticamente, embora possam rotear solicitações de acordo com sua lógica. Para moderação e segurança, combine os agentes com Guardrails; registre os rastros no CloudWatch e no S3 para observabilidade.",
    "incorrect_explanations": {
      "A": "Os Agents orquestram fluxos de trabalho; eles não treinam ou geram automaticamente modelos de fundação personalizados.",
      "C": "Os Agents podem chamar ferramentas/modelos, mas o valor principal é a orquestração do fluxo de trabalho, não apenas a consolidação de vários FMs.",
      "D": "A seleção do modelo depende da sua lógica de roteamento; os agentes não escolhem autonomamente o melhor FM por métricas."
    }
  },
  {
    "id": "aif-c01-ml_development-081",
    "certification_id": "AIF-C01",
    "domain": "ML_DEVELOPMENT",
    "difficulty": "medium",
    "tier": "FREE",
    "required_selection_count": 1,
    "active": true,
    "question_text": "Qual é o benefício do pré-treinamento contínuo ao adaptar um modelo de fundação?",
    "option_a": "Diminui a complexidade arquitetônica do modelo",
    "option_b": "Melhora o desempenho do modelo ao longo do tempo",
    "option_c": "Reduz o requisito de tempo de treinamento",
    "option_d": "Otimiza a latência de inferência do modelo",
    "correct_answers": [
      "B"
    ],
    "explanation_detailed": "O pré-treinamento contínuo expõe o modelo a novos corpora para que ele internalize terminologia, estilos e fatos novos, reduzindo a mudança de distribuição ao longo do tempo. Isso pode melhorar o desempenho da tarefa downstream, especialmente quando combinado com avaliação periódica e auditorias de segurança. Não simplifica inerentemente a arquitetura, não encurta o tempo de treinamento nem acelera a inferência. Na AWS, você pode executar o treinamento agendado em instâncias Trn ou da série P, armazenar conjuntos de dados no S3 com versionamento, acompanhar os experimentos no SageMaker e validar a segurança com o Clarify e as Guardrails antes da promoção.",
    "incorrect_explanations": {
      "A": "O pré-treinamento contínuo altera os parâmetros, não a complexidade da arquitetura.",
      "C": "Geralmente adiciona tempo de treinamento, pois você treina em mais dados ao longo do tempo.",
      "D": "A latência da inferência depende do hardware/arquitetura de implantação, não do pré-treinamento contínuo."
    }
  },
  {
    "id": "aif-c01-ai_fundamentals-082",
    "certification_id": "AIF-C01",
    "domain": "AI_FUNDAMENTALS",
    "difficulty": "easy",
    "tier": "FREE",
    "required_selection_count": 1,
    "active": true,
    "question_text": "Na IA generativa, o que são tokens?",
    "option_a": "Unidades básicas de entrada/saída nas quais o modelo opera — palavras, subpalavras ou outras unidades linguísticas",
    "option_b": "Representações matemáticas de palavras usadas pelos modelos",
    "option_c": "Pesos pré-treinados ajustados para tarefas específicas",
    "option_d": "Prompts ou instruções fornecidas ao modelo",
    "correct_answers": [
      "A"
    ],
    "explanation_detailed": "Os tokens são as unidades atômicas processadas pelos modelos de linguagem. A tokenização divide o texto em palavras, subpalavras ou caracteres, permitindo que os modelos lidem com vocabulários diversos de forma eficiente. Custos, janelas de contexto e limites de taxa são medidos em tokens. As incorporações (vetores) podem representar tokens semanticamente, mas os tokens em si são os símbolos discretos. Os pesos são parâmetros do modelo. Os prompts são sequências de tokens fornecidas como entrada. Na AWS com o Amazon Bedrock, os preços e os limites são baseados em tokens; entender a tokenização ajuda a controlar os custos e a garantir que os prompts se encaixem nas janelas de contexto.",
    "incorrect_explanations": {
      "B": "Isso descreve as incorporações; os tokens são as unidades discretas antes da incorporação.",
      "C": "Os pesos são parâmetros, não as unidades de entrada/saída do processamento de texto.",
      "D": "Os prompts são compostos de tokens, mas não são tokens em si."
    }
  },
  {
    "id": "aif-c01-ai_services-083",
    "certification_id": "AIF-C01",
    "domain": "AI_SERVICES",
    "difficulty": "easy",
    "tier": "FREE",
    "required_selection_count": 1,
    "active": true,
    "question_text": "Uma empresa quer estimar os custos de inferência ao usar o Amazon Bedrock para construir aplicações generativas. Qual fator mais influencia o custo?",
    "option_a": "Número de tokens consumidos",
    "option_b": "Valor da temperatura",
    "option_c": "Quantidade de dados usados para treinar o LLM",
    "option_d": "Tempo total de treinamento",
    "correct_answers": [
      "A"
    ],
    "explanation_detailed": "O preço da inferência do Bedrock é baseado principalmente nos tokens processados — tanto o prompt quanto a conclusão. Reduzir a verbosidade do prompt, podar exemplos de poucas tentativas (few-shot) e limitar o máximo de tokens pode reduzir substancialmente o custo. A temperatura afeta a aleatoriedade, não o preço. O tamanho dos dados de treinamento e o tempo se aplicam ao treinamento do modelo, não à inferência pague-conforme-o-uso com FMs gerenciados. Monitore o uso de tokens com o registro do Bedrock e as métricas do CloudWatch e agrupe as solicitações em lote ou armazene os resultados em cache quando possível para otimizar os gastos.",
    "incorrect_explanations": {
      "B": "A temperatura molda a diversidade da saída, não o faturamento.",
      "C": "O volume de dados de treinamento é irrelevante para a precificação da inferência sob demanda.",
      "D": "O tempo de treinamento afeta o custo do treinamento personalizado, não as taxas de uso do modelo do Bedrock."
    }
  },
  {
    "id": "aif-c01-ai_services-084",
    "certification_id": "AIF-C01",
    "domain": "AI_SERVICES",
    "difficulty": "medium",
    "tier": "FREE",
    "required_selection_count": 1,
    "active": true,
    "question_text": "Uma equipe usa notebooks do Amazon SageMaker Studio com dados no S3 e precisa gerenciar o fluxo de dados com segurança do S3 para o Studio. O que eles devem configurar?",
    "option_a": "Amazon Inspector para monitorar o SageMaker Studio",
    "option_b": "Amazon Macie para monitorar o SageMaker Studio",
    "option_c": "SageMaker em uma VPC com um endpoint de VPC do S3",
    "option_d": "SageMaker com S3 Glacier Deep Archive",
    "correct_answers": [
      "C"
    ],
    "explanation_detailed": "Coloque o SageMaker Studio em uma VPC e configure um Endpoint de VPC do S3 de Gateway (ou Interface) para rotear o tráfego de forma privada para o S3 sem atravessar a internet pública. Adicione grupos de segurança, funções do IAM com o menor privilégio e criptografia do KMS para dados em repouso. O Inspector e o Macie abordam a descoberta de vulnerabilidades e dados sensíveis, respectivamente, não o controle do caminho da rede. O Glacier Deep Archive é armazenamento a frio e não está relacionado ao acesso a dados do Studio. Os endpoints da VPC impõem o movimento de dados seguro, controlado e auditável para notebooks.",
    "incorrect_explanations": {
      "A": "O Inspector verifica as vulnerabilidades; não controla os caminhos de rede Studio↔S3.",
      "B": "O Macie ajuda a descobrir dados sensíveis; não fornece conectividade privada.",
      "D": "O Glacier Deep Archive é armazenamento de arquivamento, não uma configuração de conectividade/segurança."
    }
  },
  {
    "id": "aif-c01-ai_fundamentals-086",
    "certification_id": "AIF-C01",
    "domain": "AI_FUNDAMENTALS",
    "difficulty": "easy",
    "tier": "FREE",
    "required_selection_count": 1,
    "active": true,
    "question_text": "Qual é a principal diferença entre IA e ML?",
    "option_a": "IA é um subconjunto de ML",
    "option_b": "ML é um subconjunto de IA",
    "option_c": "São campos completamente não relacionados",
    "option_d": "IA e ML são a mesma coisa",
    "correct_answers": [
      "B"
    ],
    "explanation_detailed": "A Inteligência Artificial é o campo amplo de construção de sistemas que executam tarefas que exigem inteligência semelhante à humana. O Aprendizado de Máquina é um subcampo da IA que aprende padrões a partir de dados para melhorar o desempenho da tarefa sem programação explícita de regras. O DL (aprendizado profundo) é um subconjunto adicional que usa redes neurais com muitas camadas. Na AWS, o Amazon SageMaker fornece ferramentas para desenvolver, treinar e implantar ML; o Bedrock expõe modelos de fundação (uma aplicação de ML) para IA generativa; serviços de nível superior como Comprehend, Rekognition e Transcribe são serviços de IA alimentados por ML nos bastidores.",
    "incorrect_explanations": {
      "A": "É o inverso: ML é um subconjunto de IA, não IA um subconjunto de ML.",
      "C": "Eles estão intimamente relacionados; o ML é uma abordagem central dentro da IA.",
      "D": "A IA é mais ampla; o ML é uma abordagem, então eles não são idênticos."
    }
  },
  {
    "id": "aif-c01-ai_fundamentals-087",
    "certification_id": "AIF-C01",
    "domain": "AI_FUNDAMENTALS",
    "difficulty": "easy",
    "tier": "FREE",
    "required_selection_count": 1,
    "active": true,
    "question_text": "Qual NÃO é um tipo padrão de aprendizado de máquina?",
    "option_a": "Aprendizado supervisionado",
    "option_b": "Aprendizado não supervisionado",
    "option_c": "Aprendizado por reforço",
    "option_d": "Aprendizado diagnóstico",
    "correct_answers": [
      "D"
    ],
    "explanation_detailed": "As categorias canônicas são aprendizado supervisionado, não supervisionado e por reforço. O supervisionado usa exemplos rotulados; o não supervisionado encontra estrutura em dados não rotulados; o reforço aprende por meio de recompensas em um ambiente. “Aprendizado diagnóstico” não é uma categoria padrão de ML, embora os modelos possam auxiliar no diagnóstico. Na AWS, o SageMaker suporta os três por meio de algoritmos integrados, frameworks e kits de ferramentas de RL.",
    "incorrect_explanations": {
      "A": "O aprendizado supervisionado é um paradigma central do ML que usa dados rotulados.",
      "B": "O aprendizado não supervisionado é padrão para agrupamento, redução de dimensionalidade e descoberta.",
      "C": "O aprendizado por reforço é um paradigma importante para a tomada de decisões sequenciais."
    }
  },
  {
    "id": "aif-c01-ai_fundamentals-088",
    "certification_id": "AIF-C01",
    "domain": "AI_FUNDAMENTALS",
    "difficulty": "easy",
    "tier": "FREE",
    "required_selection_count": 1,
    "active": true,
    "question_text": "Qual tipo de dado é mais adequado para treinar um modelo de visão computacional?",
    "option_a": "Dados tabulares",
    "option_b": "Dados de séries temporais",
    "option_c": "Dados de imagem",
    "option_d": "Dados de texto",
    "correct_answers": [
      "C"
    ],
    "explanation_detailed": "Os modelos de visão computacional aprendem com imagens ou quadros de vídeo. Eles exigem matrizes de pixels com rótulos para tarefas como classificação, detecção ou segmentação. Os dados de séries temporais e tabulares alimentam tarefas de previsão ou de ML estruturado, e o texto é adequado para PNL. Na AWS, use os algoritmos integrados do SageMaker (por exemplo, classificação de imagem) ou frameworks (PyTorch, TensorFlow), armazene imagens no S3 e otimize os pipelines com os trabalhos de Processamento e Treinamento do SageMaker.",
    "incorrect_explanations": {
      "A": "Os dados tabulares são para ML estruturado, não para tarefas de percepção visual.",
      "B": "As séries temporais são adequadas para previsão e análise de sinais, não diretamente para a visão de imagem.",
      "D": "O texto é para tarefas de PNL; a visão computacional consome imagens/vídeos."
    }
  },
  {
    "id": "aif-c01-ai_services-089",
    "certification_id": "AIF-C01",
    "domain": "AI_SERVICES",
    "difficulty": "easy",
    "tier": "FREE",
    "required_selection_count": 1,
    "active": true,
    "question_text": "Qual serviço da AWS é melhor para tarefas de PNL como sentimento, extração de entidades e frases-chave?",
    "option_a": "Amazon SageMaker",
    "option_b": "Amazon Comprehend",
    "option_c": "Amazon Polly",
    "option_d": "Amazon Transcribe",
    "correct_answers": [
      "B"
    ],
    "explanation_detailed": "O Amazon Comprehend é um serviço de PNL gerenciado para análise de sentimento, reconhecimento de entidades, frases-chave e modelagem de tópicos prontos para uso. Ele suporta modelos de classificação e entidade personalizados. O SageMaker é uma plataforma geral de ML; o Polly converte texto em fala; o Transcribe converte fala em texto. O Comprehend acelera o tempo de obtenção de valor para tarefas comuns de PNL sem a necessidade de construir modelos personalizados.",
    "incorrect_explanations": {
      "A": "O SageMaker pode construir modelos de PNL, mas requer engenharia de ML; o Comprehend é pronto para uso.",
      "C": "O Polly é TTS, não compreensão de texto.",
      "D": "O Transcribe é de fala para texto, não análise de PNL."
    }
  },
  {
    "id": "aif-c01-ml_development-090",
    "certification_id": "AIF-C01",
    "domain": "ML_DEVELOPMENT",
    "difficulty": "easy",
    "tier": "FREE",
    "required_selection_count": 1,
    "active": true,
    "question_text": "Qual é o objetivo principal da Análise Exploratória de Dados (EDA) no desenvolvimento de ML?",
    "option_a": "Treinar o modelo",
    "option_b": "Implantar o modelo",
    "option_c": "Entender as características dos dados",
    "option_d": "Monitorar o modelo em produção",
    "correct_answers": [
      "C"
    ],
    "explanation_detailed": "A EDA ajuda a entender distribuições, correlações, valores ausentes, outliers e possíveis vazamentos de dados antes da modelagem. Os insights da EDA informam o pré-processamento, a engenharia de características e a escolha do modelo. Na AWS, execute a EDA no SageMaker Studio com Pandas/Matplotlib, armazenando conjuntos de dados no S3, e comunique as descobertas por meio de painéis do QuickSight.",
    "incorrect_explanations": {
      "A": "O treinamento do modelo acontece depois que você entende e prepara os dados via EDA.",
      "B": "A implantação ocorre após o treinamento e a avaliação, não durante a EDA.",
      "D": "O monitoramento é uma preocupação da produção, não uma tarefa da fase de análise."
    }
  },
  {
    "id": "aif-c01-ml_development-091",
    "certification_id": "AIF-C01",
    "domain": "ML_DEVELOPMENT",
    "difficulty": "easy",
    "tier": "FREE",
    "required_selection_count": 1,
    "active": true,
    "question_text": "Qual NÃO é uma etapa típica em um pipeline de ML?",
    "option_a": "Coleta de dados",
    "option_b": "Engenharia de características",
    "option_c": "Treinamento do modelo",
    "option_d": "Aquisição de clientes",
    "correct_answers": [
      "D"
    ],
    "explanation_detailed": "Os pipelines de ML focam em operações de dados, modelagem, avaliação e implantação. A aquisição de clientes é um processo de negócios, não uma etapa técnica do pipeline. Na AWS, os pipelines podem usar o S3 para dados, Processamento/Treinamento do SageMaker, Registro de Modelos para versões e CI/CD via SageMaker Pipelines ou CodePipeline.",
    "incorrect_explanations": {
      "A": "A coleta de dados é fundamental para os pipelines de ML.",
      "B": "A engenharia de características molda as entradas e é uma etapa central.",
      "C": "O treinamento é a fase central da modelagem."
    }
  },
  {
    "id": "aif-c01-ai_fundamentals-092",
    "certification_id": "AIF-C01",
    "domain": "AI_FUNDAMENTALS",
    "difficulty": "easy",
    "tier": "FREE",
    "required_selection_count": 1,
    "active": true,
    "question_text": "Nas métricas de desempenho do modelo, o que significa AUC?",
    "option_a": "Custo Médio do Usuário",
    "option_b": "Área Sob a Curva",
    "option_c": "Cálculo Universal Automatizado",
    "option_d": "Caso de Uso Aumentado",
    "correct_answers": [
      "B"
    ],
    "explanation_detailed": "A AUC — Área Sob a Curva ROC — mede a capacidade de um classificador de classificar os positivos acima dos negativos em todos os limiares. Ela resume as trocas entre as taxas de verdadeiros positivos e falsos positivos. A AUC é independente do limiar e útil para conjuntos de dados desbalanceados. Na AWS, calcule a AUC em notebooks do SageMaker (scikit-learn) e acompanhe-a no Registro de Modelos juntamente com gráficos de precisão/recall e calibração.",
    "incorrect_explanations": {
      "A": "Não é um acrônimo de métrica de ML padrão.",
      "C": "Não é um termo de desempenho de ML.",
      "D": "Não está relacionado ao desempenho da classificação."
    }
  },
  {
    "id": "aif-c01-ai_fundamentals-093",
    "certification_id": "AIF-C01",
    "domain": "AI_FUNDAMENTALS",
    "difficulty": "easy",
    "tier": "FREE",
    "required_selection_count": 1,
    "active": true,
    "question_text": "Qual tipo de aprendizado é mais apropriado quando você tem um grande conjunto de dados rotulado?",
    "option_a": "Aprendizado não supervisionado",
    "option_b": "Aprendizado por reforço",
    "option_c": "Aprendizado supervisionado",
    "option_d": "Aprendizado semi-supervisionado",
    "correct_answers": [
      "C"
    ],
    "explanation_detailed": "O aprendizado supervisionado mapeia entradas para saídas usando exemplos rotulados e tem o melhor desempenho quando existem dados rotulados abundantes. Ele sustenta as tarefas de classificação e regressão. O aprendizado não supervisionado descobre estrutura sem rótulos; o aprendizado por reforço otimiza as ações por meio de recompensas; o semi-supervisionado usa rótulos limitados mais dados não rotulados. Na AWS, treine modelos supervisionados com os algoritmos integrados do SageMaker (XGBoost) ou frameworks.",
    "incorrect_explanations": {
      "A": "O não supervisionado não tem rótulos e não explorará seu conjunto de dados rotulado.",
      "B": "O RL é para decisões e recompensas sequenciais, não para pares fixos de entrada-rótulo.",
      "D": "O semi-supervisionado é para rótulos escassos; você já tem muitos."
    }
  },
  {
    "id": "aif-c01-ai_fundamentals-094",
    "certification_id": "AIF-C01",
    "domain": "AI_FUNDAMENTALS",
    "difficulty": "easy",
    "tier": "FREE",
    "required_selection_count": 1,
    "active": true,
    "question_text": "Qual é a principal vantagem de usar modelos pré-treinados?",
    "option_a": "Eles sempre superam os modelos personalizados",
    "option_b": "Eles exigem menos recursos de computação para treinar",
    "option_c": "Eles são sempre mais precisos",
    "option_d": "Eles podem ser usados imediatamente sem treinamento adicional",
    "correct_answers": [
      "D"
    ],
    "explanation_detailed": "Os modelos pré-treinados já codificam o conhecimento linguístico ou visual aprendido de grandes corpora, permitindo o uso de zero-shot e few-shot e a prototipagem rápida. Muitas vezes, você pode implantá-los diretamente ou adaptá-los levemente por meio de engenharia de prompt ou ajuste fino. Eles não garantem maior precisão do que os modelos personalizados, mas reduzem o tempo de obtenção de valor. Na AWS, use o Amazon Bedrock para acesso gerenciado a FMs pré-treinados e o SageMaker JumpStart para modelos e notebooks pré-construídos.",
    "incorrect_explanations": {
      "A": "Eles nem sempre superam os modelos personalizados específicos do domínio.",
      "B": "Eles ainda exigem computação se você fizer o ajuste fino; a vantagem é começar de uma linha de base treinada.",
      "C": "A precisão depende do domínio e dos dados; não é garantida."
    }
  },
  {
    "id": "aif-c01-ai_services-095",
    "certification_id": "AIF-C01",
    "domain": "AI_SERVICES",
    "difficulty": "easy",
    "tier": "FREE",
    "required_selection_count": 1,
    "active": true,
    "question_text": "Qual serviço da AWS melhor automatiza a busca por hiperparâmetros fortes para um modelo?",
    "option_a": "Amazon SageMaker Autopilot",
    "option_b": "Amazon Comprehend",
    "option_c": "Amazon Polly",
    "option_d": "Amazon Transcribe",
    "correct_answers": [
      "A"
    ],
    "explanation_detailed": "O SageMaker Autopilot automatiza o pré-processamento de características, a seleção de algoritmos e o ajuste de hiperparâmetros para produzir linhas de base fortes com código mínimo. Ele aproveita a busca Bayesiana ou de grade/aleatória nos bastidores (e você também pode usar os Trabalhos de Otimização de Hiperparâmetros do SageMaker diretamente). O Comprehend é PNL, o Polly é TTS, o Transcribe é STT. O Autopilot reduz a iteração manual e registra os candidatos para inspeção e implantação via Registro de Modelos.",
    "incorrect_explanations": {
      "B": "O Comprehend realiza análises de PNL; não ajusta modelos arbitrários.",
      "C": "O Polly sintetiza a fala, não relacionado ao ajuste de modelos de ML.",
      "D": "O Transcribe converte áudio em texto, não otimização de modelo."
    }
  },
  {
    "id": "aif-c01-ml_development-096",
    "certification_id": "AIF-C01",
    "domain": "ML_DEVELOPMENT",
    "difficulty": "easy",
    "tier": "FREE",
    "required_selection_count": 1,
    "active": true,
    "question_text": "O que significa MLOps?",
    "option_a": "Operações de Aprendizado de Máquina",
    "option_b": "Otimizações de Aprendizado Múltiplo",
    "option_c": "Objetivos de Aprendizado de Modelo",
    "option_d": "Saídas de Aprendizado Gerenciadas",
    "correct_answers": [
      "A"
    ],
    "explanation_detailed": "MLOps é a prática de construir, implantar, monitorar e governar sistemas de ML em produção de forma confiável. Abrange CI/CD para modelos, gerenciamento de dados/versões, testes automatizados, verificações de viés/segurança e observabilidade. Na AWS, use o SageMaker Pipelines, o Registro de Modelos, o Clarify, o Model Monitor e o CI/CD via CodePipeline/CodeBuild para implementar o MLOps em escala.",
    "incorrect_explanations": {
      "B": "Não é uma expansão padrão; o MLOps se refere a práticas operacionais, não apenas a otimizações.",
      "C": "Os objetivos fazem parte do design do ML, não da definição de MLOps.",
      "D": "O gerenciamento de saídas é um subconjunto; o MLOps são operações de ciclo de vida mais amplas."
    }
  },
  {
    "id": "aif-c01-ml_development-097",
    "certification_id": "AIF-C01",
    "domain": "ML_DEVELOPMENT",
    "difficulty": "easy",
    "tier": "FREE",
    "required_selection_count": 1,
    "active": true,
    "question_text": "Qual NÃO é uma métrica de negócios típica para avaliar sistemas de ML?",
    "option_a": "Custo por usuário",
    "option_b": "Custos de desenvolvimento",
    "option_c": "Feedback do cliente",
    "option_d": "Pontuação F1",
    "correct_answers": [
      "D"
    ],
    "explanation_detailed": "A F1 é uma métrica técnica que reflete o equilíbrio entre precisão e recall. As métricas de negócios medem o impacto comercial, os custos e o sentimento do usuário. Alinhar as métricas técnicas com os KPIs de negócios é crucial para a entrega de valor. Na AWS, vincule as métricas de modelo do CloudWatch a painéis do QuickSight que incluem resultados de custo e de clientes.",
    "incorrect_explanations": {
      "A": "A economia unitária, como o custo por usuário, são KPIs de negócios centrais.",
      "B": "Os custos de desenvolvimento afetam o ROI e são rastreados pelas equipes de produto.",
      "C": "O feedback do usuário mede a satisfação e o valor, chave para o sucesso."
    }
  },
  {
    "id": "aif-c01-ai_fundamentals-098",
    "certification_id": "AIF-C01",
    "domain": "AI_FUNDAMENTALS",
    "difficulty": "easy",
    "tier": "FREE",
    "required_selection_count": 1,
    "active": true,
    "question_text": "Qual paradigma de aprendizado é melhor quando um agente precisa aprender com as interações com um ambiente?",
    "option_a": "Aprendizado supervisionado",
    "option_b": "Aprendizado não supervisionado",
    "option_c": "Aprendizado por reforço",
    "option_d": "Aprendizado por transferência",
    "correct_answers": [
      "C"
    ],
    "explanation_detailed": "O aprendizado por reforço aprende políticas que maximizam a recompensa cumulativa por meio de tentativa e erro. Ele se adequa ao controle, à recomendação com valor a longo prazo e ao gerenciamento de diálogo. O aprendizado supervisionado requer pares de entrada/saída rotulados; o não supervisionado encontra estrutura; a transferência adapta o conhecimento entre tarefas. Na AWS, você pode implementar o RL no SageMaker ou aproveitar a simulação no AWS RoboMaker para cenários de robótica.",
    "incorrect_explanations": {
      "A": "O aprendizado supervisionado usa rótulos estáticos, não loops de feedback interativos.",
      "B": "O aprendizado não supervisionado descobre padrões sem recompensas ou ações.",
      "D": "O aprendizado por transferência reutiliza o conhecimento, mas é ortogonal ao aprendizado orientado por interação."
    }
  },
  {
    "id": "aif-c01-ai_services-099",
    "certification_id": "AIF-C01",
    "domain": "AI_SERVICES",
    "difficulty": "easy",
    "tier": "FREE",
    "required_selection_count": 1,
    "active": true,
    "question_text": "Qual serviço da AWS converte texto em fala realista?",
    "option_a": "Amazon Comprehend",
    "option_b": "Amazon Translate",
    "option_c": "Amazon Transcribe",
    "option_d": "Amazon Polly",
    "correct_answers": [
      "D"
    ],
    "explanation_detailed": "O Amazon Polly é um serviço neural de conversão de texto em fala que fornece vozes naturais e controle SSML sobre prosódia, pronúncia e ênfase. Ele suporta vários idiomas e pode transmitir áudio para aplicações de baixa latência. O Comprehend realiza análises de PNL, o Translate lida com a tradução de idiomas e o Transcribe converte fala em texto. O Polly é a solução TTS correta na AWS.",
    "incorrect_explanations": {
      "A": "O Comprehend analisa o texto, mas não sintetiza a fala.",
      "B": "O Translate converte texto entre idiomas, não para áudio.",
      "C": "O Transcribe é de fala para texto, o inverso do TTS."
    }
  },
  {
    "id": "aif-c01-ml_development-100",
    "certification_id": "AIF-C01",
    "domain": "ML_DEVELOPMENT",
    "difficulty": "easy",
    "tier": "FREE",
    "required_selection_count": 1,
    "active": true,
    "question_text": "Qual é o objetivo principal da engenharia de características no desenvolvimento de ML?",
    "option_a": "Coletar mais dados",
    "option_b": "Criar ou transformar características para melhorar o desempenho do modelo",
    "option_c": "Avaliar o desempenho do modelo",
    "option_d": "Implantar o modelo em produção",
    "correct_answers": [
      "B"
    ],
    "explanation_detailed": "A engenharia de características extrai o sinal do domínio dos dados brutos — criando, transformando e selecionando variáveis que melhoram a capacidade do modelo de generalizar. Os exemplos incluem escalonamento, codificação de categorias, agregações, proporções específicas do domínio e características de janela de tempo. Na AWS, use o Processamento do SageMaker ou o Feature Store para construir, documentar e compartilhar características de forma consistente no treinamento e na inferência. Boas características reduzem os requisitos de complexidade e a fragilidade do modelo.",
    "incorrect_explanations": {
      "A": "Mais dados podem ajudar, mas não substituem a engenharia de características informativas.",
      "C": "A avaliação mede o desempenho; a engenharia de características altera as entradas para afetá-lo.",
      "D": "A implantação é uma etapa operacional, não a criação de características."
    }
  },
  {
    "id": "aif-c01-ai_fundamentals-101",
    "certification_id": "AIF-C01",
    "domain": "AI_FUNDAMENTALS",
    "difficulty": "easy",
    "tier": "FREE",
    "required_selection_count": 1,
    "active": true,
    "question_text": "Qual dos seguintes é um exemplo de aprendizado não supervisionado?",
    "option_a": "Detecção de spam",
    "option_b": "Classificação de imagens",
    "option_c": "Agrupamento de segmentos de clientes",
    "option_d": "Previsão do preço de casas",
    "correct_answers": [
      "C"
    ],
    "explanation_detailed": "O aprendizado não supervisionado busca padrões sem rótulos humanos. O exemplo clássico é o agrupamento, que organiza as amostras em grupos (clusters) com alta similaridade interna e baixa similaridade externa. Isso é útil para segmentação de clientes quando você não tem classes predefinidas, descoberta de tópicos e detecção de estruturas latentes. Em um contexto da AWS, você pode experimentar o agrupamento usando algoritmos disponíveis no Amazon SageMaker (por exemplo, K-Means), executando treinamento gerenciado, ajuste automático e inferência em lote. Essa abordagem difere do aprendizado supervisionado, que usa dados rotulados para treinar um classificador ou regressor. O agrupamento ajuda a revelar segmentos ocultos para campanhas, precificação e personalização.",
    "incorrect_explanations": {
      "A": "A detecção de spam é tipicamente um problema supervisionado: você treina um classificador com exemplos rotulados de mensagens 'spam' e 'não spam'. O modelo aprende os limites a partir desses rótulos.",
      "B": "A classificação de imagens usa rótulos de classe (por exemplo, 'gato', 'cachorro'). Sem rótulos, o modelo não pode aprender os limites discriminativos típicos da classificação supervisionada.",
      "D": "A previsão do preço de casas é uma regressão supervisionada: o modelo aprende com pares de características-alvo rotulados (características da casa → preço) para estimar valores contínuos."
    }
  },
  {
    "id": "aif-c01-ml_development-102",
    "certification_id": "AIF-C01",
    "domain": "ML_DEVELOPMENT",
    "difficulty": "medium",
    "tier": "FREE",
    "required_selection_count": 1,
    "active": true,
    "question_text": "Qual é a principal diferença entre a inferência em lote e a inferência em tempo real?",
    "option_a": "A inferência em lote é sempre mais precisa",
    "option_b": "A inferência em tempo real só pode ser feita em pequenos conjuntos de dados",
    "option_c": "A inferência em lote processa várias entradas de uma vez, enquanto a inferência em tempo real processa entradas individuais à medida que chegam",
    "option_d": "A inferência em tempo real é sempre mais rápida do que a inferência em lote",
    "correct_answers": [
      "C"
    ],
    "explanation_detailed": "A inferência em lote agrega muitas entradas e as processa periodicamente, otimizando o custo e a taxa de transferência quando a latência não é crítica (por exemplo, pontuação noturna de milhões de registros). Em tempo real, cada solicitação é processada com baixa latência para alimentar experiências online, como recomendações ou chatbots. Na AWS, o Amazon SageMaker oferece o Batch Transform para lotes e Endpoints em tempo real (incluindo a Inferência sem Servidor) para baixa latência. Os serviços de IA gerenciados, como o Amazon Comprehend ou o Rekognition, também expõem APIs síncronas (em tempo real) e, em alguns cenários, mecanismos assíncronos para processar grandes coleções. A escolha depende do SLA de latência, volume, custo e necessidades operacionais.",
    "incorrect_explanations": {
      "A": "A precisão não é uma função direta do modo de inferência. O mesmo modelo pode ser aplicado em lote ou em tempo real; a diferença central está na latência e no processamento agregado.",
      "B": "O tempo real não se limita a dados 'pequenos'; ele processa solicitações unitárias com baixa latência. O limite prático vem do SLA, da escalabilidade do endpoint e do custo.",
      "D": "O tempo real visa a baixa latência por solicitação, mas 'sempre mais rápido' é falso. Em cenários massivos, os lotes podem concluir o conjunto total mais rapidamente e a um custo menor."
    }
  },
  {
    "id": "aif-c01-ai_services-103",
    "certification_id": "AIF-C01",
    "domain": "AI_SERVICES",
    "difficulty": "easy",
    "tier": "FREE",
    "required_selection_count": 1,
    "active": true,
    "question_text": "Qual serviço da AWS é mais adequado para gerenciar todo o ciclo de vida do aprendizado de máquina?",
    "option_a": "Amazon Comprehend",
    "option_b": "Amazon SageMaker",
    "option_c": "Amazon Polly",
    "option_d": "Amazon Translate",
    "correct_answers": [
      "B"
    ],
    "explanation_detailed": "O Amazon SageMaker é a plataforma completa de ML da AWS para preparar dados, treinar, ajustar, implantar e monitorar modelos em escala. Ele fornece notebooks gerenciados, pipelines de MLOps (SageMaker Pipelines), experimentos, ajuste automático (HPO), bem como endpoints para inferência em tempo real, assíncrona e em lote. Recursos como o SageMaker Model Monitor detectam o desvio de dados/modelos, e o SageMaker JumpStart fornece modelos e soluções pré-treinados. Comparativamente, o Amazon Comprehend, Polly e Translate são serviços de alto nível para PNL, síntese de fala e tradução, respectivamente, consumidos via API, sem gerenciar todo o ciclo de vida. O SageMaker centraliza o ML de ponta a ponta, promovendo a governança e a repetibilidade.",
    "incorrect_explanations": {
      "A": "O Amazon Comprehend resolve tarefas de PNL (tópicos, entidades, sentimento) via API. Não é uma plataforma de ciclo de vida completo para criar e operar modelos arbitrários.",
      "C": "O Amazon Polly realiza a conversão de texto em fala. Não abrange a preparação de dados, experimentos, treinamento personalizado, implantação e monitoramento de modelos gerais.",
      "D": "O Amazon Translate foca na tradução automática. Embora poderoso, não é uma solução abrangente de gerenciamento de ML para qualquer domínio e pipeline de MLOps."
    }
  },
  {
    "id": "aif-c01-ml_development-104",
    "certification_id": "AIF-C01",
    "domain": "ML_DEVELOPMENT",
    "difficulty": "medium",
    "tier": "FREE",
    "required_selection_count": 1,
    "active": true,
    "question_text": "Qual é o objetivo principal do monitoramento de modelos em produção?",
    "option_a": "Treinar novos modelos",
    "option_b": "Coletar mais dados",
    "option_c": "Detectar problemas como desvio de modelo ou desvio de dados",
    "option_d": "Realizar engenharia de características",
    "correct_answers": [
      "C"
    ],
    "explanation_detailed": "Após a implantação, o ambiente de produção muda: a distribuição dos dados de entrada e a relação entre as características e os rótulos podem evoluir. O monitoramento detecta o desvio de dados, o desvio de conceito/modelo, a degradação de métricas e os problemas operacionais. Na AWS, o SageMaker Model Monitor coleta amostras de tráfego, compara estatísticas com linhas de base e aciona alertas (por exemplo, via CloudWatch) quando há violações. Essa telemetria orienta ações como retreinamento programado, reengenharia de características e revisão de prompts para LLMs. Sem monitoramento, os erros silenciosos crescem, corroendo os KPIs de negócios e a confiança do usuário, especialmente em sistemas de recomendação e decisão.",
    "incorrect_explanations": {
      "A": "Treinar novos modelos pode ser uma consequência, não o objetivo principal. O alvo é detectar anomalias e degradação para decidir se o retreinamento faz sentido.",
      "B": "A coleta de dados ajuda, mas o foco é analisar a qualidade/distribuição e as métricas em produção. A coleta sem análise não garante a detecção de desvios ou regressões.",
      "D": "A engenharia de características melhora as representações, mas ocorre antes ou durante o retreinamento. O monitoramento avalia o comportamento pós-implantação e aciona correções quando necessário."
    }
  },
  {
    "id": "aif-c01-ai_fundamentals-105",
    "certification_id": "AIF-C01",
    "domain": "AI_FUNDAMENTALS",
    "difficulty": "easy",
    "tier": "FREE",
    "required_selection_count": 1,
    "active": true,
    "question_text": "Qual dos seguintes NÃO é um caso de uso típico para IA/ML?",
    "option_a": "Detecção de fraude",
    "option_b": "Sistemas de recomendação",
    "option_c": "Entrada manual de dados",
    "option_d": "Reconhecimento de fala",
    "correct_answers": [
      "C"
    ],
    "explanation_detailed": "A IA/ML automatiza tarefas repetitivas, toma decisões probabilísticas e extrai padrões de dados em escala. A detecção de fraude usa modelos supervisionados e detecção de anomalias; as recomendações empregam filtragem colaborativa e modelos como o Amazon Personalize; o reconhecimento de fala utiliza redes acústicas, disponíveis via Amazon Transcribe. A 'entrada manual de dados' é precisamente o oposto: processos manuais propensos a erros e de baixa escala que tendem a ser reduzidos pela automação. Na AWS, os serviços gerenciados aceleram a adoção sem exigir equipes de pesquisa e, quando necessário, o SageMaker permite a criação de modelos e pipelines personalizados para incorporar MLOps e governança.",
    "incorrect_explanations": {
      "A": "A detecção de fraude é uma aplicação consolidada de IA/ML com classificação e anomalias. Bancos e e-commerce a usam há anos para reduzir perdas e revisar alertas.",
      "B": "Os sistemas de recomendação estão no centro de muitos produtos digitais. A AWS oferece o Amazon Personalize para acelerar essa capacidade sem uma infraestrutura pesada.",
      "D": "O reconhecimento de fala é amplamente servido pelo Amazon Transcribe, permitindo a transcrição de áudio/vídeo e alimentando a análise de sentimento, buscas e relatórios."
    }
  },
  {
    "id": "aif-c01-ai_fundamentals-106",
    "certification_id": "AIF-C01",
    "domain": "AI_FUNDAMENTALS",
    "difficulty": "easy",
    "tier": "FREE",
    "required_selection_count": 1,
    "active": true,
    "question_text": "O que é um token no contexto de IA generativa?",
    "option_a": "Um recurso de segurança",
    "option_b": "Uma unidade de texto processada pelo modelo",
    "option_c": "Um tipo de rede neural",
    "option_d": "Uma métrica de avaliação de modelo",
    "correct_answers": [
      "B"
    ],
    "explanation_detailed": "Tokens são unidades atômicas que os modelos de linguagem consomem e produzem. Dependendo da tokenização, um token pode ser uma palavra, subpalavra ou caractere. O custo e a latência nos serviços de LLM são frequentemente proporcionais ao número de tokens de entrada e saída. No Amazon Bedrock, a maioria dos provedores de modelo precifica por 1.000 tokens processados, impactando diretamente a fatura. Entender os tokens ajuda a projetar prompts concisos, aplicar a fragmentação para documentos longos e prever os limites de contexto. Ferramentas como contadores de tokens e incorporações guiam o dimensionamento das janelas de contexto e os custos de inferência em aplicações de geração e RAG.",
    "incorrect_explanations": {
      "A": "'Token' aqui não se refere a credenciais de segurança ou JWTs. É uma unidade textual usada internamente pelo modelo para representar entradas/saídas.",
      "C": "Um token não é uma arquitetura. Redes como Transformers processam sequências de tokens, mas um token é a entrada, não o tipo de rede em si.",
      "D": "Não é uma métrica de avaliação. Métricas como BLEU, ROUGE ou BERTScore avaliam a qualidade; os tokens medem a quantidade de texto e os custos."
    }
  },
  {
    "id": "aif-c01-ai_fundamentals-107",
    "certification_id": "AIF-C01",
    "domain": "AI_FUNDAMENTALS",
    "difficulty": "easy",
    "tier": "FREE",
    "required_selection_count": 1,
    "active": true,
    "question_text": "Qual dos seguintes NÃO é um caso de uso típico para modelos de IA generativa?",
    "option_a": "Geração de imagens",
    "option_b": "Sumarização",
    "option_c": "Criptografia de dados",
    "option_d": "Geração de código",
    "correct_answers": [
      "C"
    ],
    "explanation_detailed": "Os modelos generativos criam novo conteúdo a partir de padrões aprendidos: texto, imagens, áudio e código. A sumarização condensa textos enquanto mantém o significado; a geração de imagens produz visuais originais; a geração de código auxilia no desenvolvimento. A criptografia, por outro lado, é um domínio de segurança com algoritmos e protocolos matemáticos (AES, RSA). Embora a IA possa auxiliar na detecção de vulnerabilidades ou na geração de exemplos, 'criptografar' dados não é uma função típica de um modelo gerativo. Na AWS, as capacidades generativas aparecem no Amazon Bedrock (LLMs e difusão), enquanto os controles de segurança e criptografia são fornecidos por serviços como o KMS, não por LLMs.",
    "incorrect_explanations": {
      "A": "A geração de imagens é um uso central dos modelos de difusão, disponíveis nos provedores do Amazon Bedrock e em frameworks de visão.",
      "B": "A sumarização é um caso de uso clássico para LLMs, inclusive por meio de APIs gerenciadas e exemplos no Bedrock ou no SageMaker JumpStart.",
      "D": "A geração de código é amplamente suportada por LLMs centrados em código. A saída não substitui a revisão humana, mas é um caso de uso genuíno."
    }
  },
  {
    "id": "aif-c01-ai_fundamentals-108",
    "certification_id": "AIF-C01",
    "domain": "AI_FUNDAMENTALS",
    "difficulty": "easy",
    "tier": "FREE",
    "required_selection_count": 1,
    "active": true,
    "question_text": "Qual é a principal vantagem da adaptabilidade da IA generativa?",
    "option_a": "Ela só pode trabalhar com dados estruturados",
    "option_b": "Ela pode lidar com uma ampla gama de tarefas e domínios",
    "option_c": "Ela sempre produz resultados perfeitos",
    "option_d": "Ela elimina a necessidade de supervisão humana",
    "correct_answers": [
      "B"
    ],
    "explanation_detailed": "Os LLMs e modelos multimodais são pré-treinados em dados diversos e aprendem representações reutilizáveis. Essa base permite a adaptação por meio de engenharia de prompt, poucas tentativas (few-shot) e ajuste fino para múltiplas tarefas: sumarização, extração, geração de código, classificação, geração de imagens e muito mais. No Amazon Bedrock, você escolhe FMs adequados e pode realizar ajuste fino gerenciado ou embasamento via RAG. Essa versatilidade reduz o tempo de obtenção de valor, sem reescrever os pipelines do zero. No entanto, a adaptabilidade não implica perfeição nem substitui a governança: a supervisão humana, as avaliações e o monitoramento continuam essenciais para a precisão, a segurança e a conformidade. A força reside em generalizar e compor capacidades sob diferentes contextos de negócios.",
    "incorrect_explanations": {
      "A": "Os modelos generativos funcionam bem com dados não estruturados (texto, imagem, áudio). Eles não se limitam a estruturas tabulares.",
      "C": "Resultados 'perfeitos' não são garantidos. Há variabilidade estocástica, dependência do prompt e risco de alucinações.",
      "D": "A supervisão humana continua relevante para validação, segurança e ética, incluindo a revisão de saídas e decisões críticas."
    }
  },
  {
    "id": "aif-c01-responsible_ai-109",
    "certification_id": "AIF-C01",
    "domain": "RESPONSIBLE_AI",
    "difficulty": "medium",
    "tier": "FREE",
    "required_selection_count": 1,
    "active": true,
    "question_text": "O que é uma alucinação no contexto de IA generativa?",
    "option_a": "Uma saída visual produzida pelo modelo",
    "option_b": "Um tipo de arquitetura de modelo",
    "option_c": "Uma saída incorreta ou fabricada apresentada como fato",
    "option_d": "Um método de treinamento de modelo",
    "correct_answers": [
      "C"
    ],
    "explanation_detailed": "Uma alucinação ocorre quando um modelo gera conteúdo que é plausível, mas falso, incompleto ou não baseado em uma fonte. Em contextos críticos, isso mina a confiança e pode causar danos. As estratégias de mitigação incluem o embasamento com RAG (recuperando evidências de bases como Amazon OpenSearch Service, Amazon Kendra ou S3), controles de temperatura/top-p, validações pós-processamento e avaliação humana. No Amazon Bedrock, você pode combinar FMs com Guardrails para políticas de segurança e com Agents para orquestrar chamadas a ferramentas e fontes. O monitoramento da precisão e da rastreabilidade, o registro de citações e a limitação de tarefas de 'fato concreto' para fluxos com verificação reduzem o risco operacional de alucinações.",
    "incorrect_explanations": {
      "A": "Uma 'saída visual' não caracteriza uma alucinação por si só. Uma alucinação é sobre incorreção factual, independentemente da modalidade (texto, imagem).",
      "B": "Não é uma arquitetura de modelo. Arquiteturas como Transformers ou difusão podem alucinar se usadas sem embasamento/validação.",
      "D": "Não é um método de treinamento. Uma técnica como o RLHF pode reduzir as alucinações, mas a alucinação é um comportamento indesejado na inferência."
    }
  },
  {
    "id": "aif-c01-ai_services-110",
    "certification_id": "AIF-C01",
    "domain": "AI_SERVICES",
    "difficulty": "easy",
    "tier": "FREE",
    "required_selection_count": 1,
    "active": true,
    "question_text": "Qual serviço da AWS é projetado especificamente para desenvolver aplicações de IA generativa?",
    "option_a": "Amazon EC2",
    "option_b": "Amazon S3",
    "option_c": "Amazon Bedrock",
    "option_d": "Amazon RDS",
    "correct_answers": [
      "C"
    ],
    "explanation_detailed": "O Amazon Bedrock oferece acesso via API a vários modelos de fundação (FMs) para texto, imagem e multimodal, bem como capacidades como ajuste fino, guardrails e orquestração com Agents. Ele abstrai a infraestrutura, simplificando a integração de LLMs em produtos. Você pode prototipar no console, usar SDKs e combinar com RAG (Kendra/OpenSearch) para respostas baseadas em dados proprietários. Enquanto o EC2, S3 e RDS são serviços de computação, armazenamento e banco de dados, respectivamente, o Bedrock foca diretamente no ciclo de construção de aplicativos generativos, com faturamento por token e ferramentas para segurança, auditoria e integração empresarial.",
    "incorrect_explanations": {
      "A": "O Amazon EC2 fornece instâncias de computação. Útil para hospedar modelos, mas não é o serviço gerenciado para FMs e orquestrações de IA generativa.",
      "B": "O Amazon S3 armazena objetos. Pode conter dados de treinamento, prompts e resultados, mas não fornece modelos generativos via API.",
      "D": "O Amazon RDS gerencia bancos de dados relacionais. Não entrega diretamente FMs ou recursos de geração/orquestração de conteúdo."
    }
  },
  {
    "id": "aif-c01-ai_fundamentals-111",
    "certification_id": "AIF-C01",
    "domain": "AI_FUNDAMENTALS",
    "difficulty": "easy",
    "tier": "FREE",
    "required_selection_count": 1,
    "active": true,
    "question_text": "O que é um modelo de fundação em IA generativa?",
    "option_a": "Um modelo que só pode gerar texto",
    "option_b": "Um modelo grande e pré-treinado que pode ser adaptado para várias tarefas",
    "option_c": "Um modelo projetado especificamente para geração de imagens",
    "option_d": "Um modelo que não requer dados de treinamento",
    "correct_answers": [
      "B"
    ],
    "explanation_detailed": "Os modelos de fundação (FMs) são redes de grande escala pré-treinadas em dados extensos e diversos para aprender representações gerais. Eles servem como base para muitas tarefas: perguntas e respostas, sumarização, extração, geração de imagem/código e muito mais. A adaptação ocorre por meio de engenharia de prompt, poucas tentativas (few-shot), ajuste fino ou instrução. No Amazon Bedrock, você escolhe FMs de diferentes provedores e os ajusta ao seu caso de uso, economizando o tempo e o custo de treinar do zero. Esses modelos não se limitam a texto ou imagens; o termo abrange variantes unimodais e multimodais. Embora extensíveis, eles ainda exigem dados de treinamento na fase de pré-treinamento e curadoria ao personalizar.",
    "incorrect_explanations": {
      "A": "Os FMs não se limitam a texto; existem modelos para imagem, multimodalidade e código. A chave é o pré-treinamento amplo e reutilizável.",
      "C": "Os modelos de imagem (por exemplo, difusão) são um possível subconjunto. 'Fundação' não implica necessariamente um foco em imagens.",
      "D": "Todo modelo requer dados em algum estágio. O pré-treinamento massivo é central para o conceito de um FM."
    }
  },
  {
    "id": "aif-c01-ai_fundamentals-112",
    "certification_id": "AIF-C01",
    "domain": "AI_FUNDAMENTALS",
    "difficulty": "easy",
    "tier": "FREE",
    "required_selection_count": 1,
    "active": true,
    "question_text": "Qual das seguintes NÃO é uma etapa no ciclo de vida do modelo de fundação?",
    "option_a": "Seleção de dados",
    "option_b": "Pré-treinamento",
    "option_c": "Implantação",
    "option_d": "Marketing",
    "correct_answers": [
      "D"
    ],
    "explanation_detailed": "O ciclo de vida técnico dos FMs envolve coletar/curar dados, pré-treinar modelos em grande escala, ajustar para tarefas específicas (instrução/ajuste fino), avaliar com métricas apropriadas e implantar/monitorar em produção. Governança, segurança e custo são considerações transversais. O marketing é uma atividade de go-to-market e não faz parte do ciclo de vida técnico do modelo. Na AWS, o SageMaker e o Bedrock oferecem ferramentas para várias fases: JumpStart e conjuntos de dados, pipelines de MLOps, endpoints gerenciados, guardrails e monitoramento. A disciplina garante reprodutibilidade, qualidade e conformidade, reduzindo os riscos de desvio e degradando menos em cenários dinâmicos.",
    "incorrect_explanations": {
      "A": "A seleção/curadoria de dados é essencial para a qualidade e a cobertura. Dados ruidosos degradam o pré-treinamento e as adaptações subsequentes.",
      "B": "O pré-treinamento aprende representações gerais em corpora massivos e é central para o conceito de FM.",
      "C": "A implantação garante que o modelo sirva o tráfego com SLA, segurança e observabilidade, uma parte crítica do ciclo operacional."
    }
  },
  {
    "id": "aif-c01-ai_services-113",
    "certification_id": "AIF-C01",
    "domain": "AI_SERVICES",
    "difficulty": "easy",
    "tier": "FREE",
    "required_selection_count": 1,
    "active": true,
    "question_text": "Qual é a principal vantagem de usar os serviços de IA generativa da AWS para construir aplicações?",
    "option_a": "Eles são sempre gratuitos",
    "option_b": "Eles fornecem uma barreira de entrada mais baixa",
    "option_c": "Eles garantem 100% de precisão",
    "option_d": "Eles eliminam a necessidade de qualquer codificação",
    "correct_answers": [
      "B"
    ],
    "explanation_detailed": "Serviços gerenciados como Amazon Bedrock, Amazon Q e integrações com Kendra/OpenSearch removem grande parte da complexidade da infraestrutura e orquestração de modelos. Você acessa FMs via API, pode prototipar rapidamente, ajustar por instrução, aplicar guardrails e combinar com RAG. Essa abordagem reduz o tempo de obtenção de valor, o esforço de MLOps e os custos iniciais, permitindo o foco no caso de negócios. Ainda assim, a precisão não é garantida e a codificação pode ser necessária para integrações, avaliação e monitoramento. O principal benefício é acelerar a entrega com menos carga operacional, mantendo boas práticas de segurança e governança suportadas pelo ecossistema da AWS.",
    "incorrect_explanations": {
      "A": "Eles não são 'sempre gratuitos'. A maioria adota o faturamento por token, solicitação ou recursos de computação usados.",
      "C": "Nenhum serviço garante 100% de precisão. A avaliação, os limites e as validações continuam obrigatórios.",
      "D": "Embora existam opções de baixo código/sem código, as integrações e a lógica de negócios exigem algum nível de codificação."
    }
  },
  {
    "id": "aif-c01-ai_fundamentals-114",
    "certification_id": "AIF-C01",
    "domain": "AI_FUNDAMENTALS",
    "difficulty": "medium",
    "tier": "FREE",
    "required_selection_count": 1,
    "active": true,
    "question_text": "O que é engenharia de prompt no contexto de IA generativa?",
    "option_a": "Um método de otimização de hardware",
    "option_b": "Uma técnica para projetar a estrutura física dos modelos de IA",
    "option_c": "O processo de criar prompts de entrada eficazes para guiar as saídas do modelo",
    "option_d": "Uma maneira de reduzir o consumo de energia em sistemas de IA",
    "correct_answers": [
      "C"
    ],
    "explanation_detailed": "A engenharia de prompt consiste em estruturar instruções, contexto e exemplos para guiar os LLMs a produzir respostas alinhadas com a tarefa. Inclui técnicas como delimitação clara, funções, restrições, poucas tentativas (few-shot), prompts negativos e validações automáticas. Em aplicações da AWS, você pode iterar em prompts no Amazon Bedrock, usar Agents para decompor tarefas e combinar RAG (Kendra/OpenSearch) para embasamento, reduzindo as alucinações. A prática envolve medir a qualidade, o custo (tokens) e a latência, bem como versionar modelos e usar parâmetros como temperatura e top-p. Bons prompts tornam a aplicação mais previsível e ajudam a atender aos requisitos de negócios e de conformidade.",
    "incorrect_explanations": {
      "A": "Não otimiza o hardware. Lida com o design textual/estrutural da entrada para guiar o comportamento do modelo.",
      "B": "Não projeta a 'física' do modelo. É uma camada de aplicação sobre modelos já treinados.",
      "D": "Pode influenciar indiretamente o custo/latência, mas não é uma técnica de eficiência energética."
    }
  },
  {
    "id": "aif-c01-responsible_ai-115",
    "certification_id": "AIF-C01",
    "domain": "RESPONSIBLE_AI",
    "difficulty": "medium",
    "tier": "FREE",
    "required_selection_count": 1,
    "active": true,
    "question_text": "Qual das seguintes é uma desvantagem potencial das soluções de IA generativa?",
    "option_a": "Adaptabilidade",
    "option_b": "Responsividade",
    "option_c": "Inexatidão",
    "option_d": "Simplicidade",
    "correct_answers": [
      "C"
    ],
    "explanation_detailed": "A IA generativa pode produzir resultados incorretos, enviesados ou sem fundamento, afetando a segurança, a reputação e os KPIs. A mitigação requer políticas, avaliação humana, embasamento (RAG), verificação de fatos e guardrails. Na AWS, a combinação do Bedrock com o Amazon Kendra/OpenSearch e regras de conteúdo, além do monitoramento contínuo (CloudWatch, logs, métricas), ajuda a reduzir os riscos. A avaliação da qualidade com métricas apropriadas e testes A/B garante que as mudanças nos prompts/modelos não degradem os resultados de negócios. A adaptabilidade e a simplicidade são benefícios, mas devem vir com governança, controles de custo por token e estratégias de revisão humana para domínios críticos como saúde, finanças e direito.",
    "incorrect_explanations": {
      "A": "A adaptabilidade é uma vantagem: os modelos se ajustam a vários contextos por meio de prompt, poucas tentativas (few-shot) e ajuste fino.",
      "B": "A responsividade se refere à interação rápida e natural, geralmente benéfica em assistentes e bate-papo.",
      "D": "A simplicidade na integração reduz as barreiras. O problema não é ser simples, mas gerenciar os riscos de qualidade e segurança."
    }
  },
  {
    "id": "aif-c01-ai_fundamentals-116",
    "certification_id": "AIF-C01",
    "domain": "AI_FUNDAMENTALS",
    "difficulty": "easy",
    "tier": "FREE",
    "required_selection_count": 1,
    "active": true,
    "question_text": "O que é um modelo multimodal em IA generativa?",
    "option_a": "Um modelo que só pode processar dados de texto",
    "option_b": "Um modelo que pode trabalhar com vários tipos de dados (por exemplo, texto, imagens, áudio)",
    "option_c": "Um modelo que requer várias GPUs para ser executado",
    "option_d": "Um modelo que só pode gerar imagens",
    "correct_answers": [
      "B"
    ],
    "explanation_detailed": "Os modelos multimodais consomem e geram diferentes modalidades, como texto, imagem, áudio e vídeo. Eles criam representações compartilhadas entre as modalidades, permitindo tarefas como descrever imagens, responder a perguntas sobre figuras ou gerar ilustrações a partir de texto. No Amazon Bedrock, existem provedores com capacidades multimodais expostas via API, que você pode combinar com armazenamento no S3 e busca vetorial (OpenSearch) para embasamento. Essa integração expande a gama de aplicações, desde suporte técnico com capturas de tela até análise de mídia. Embora flexíveis, eles exigem a mesma governança que os LLMs somente de texto: avaliação, monitoramento e controle de custos por token e por conteúdo.",
    "incorrect_explanations": {
      "A": "Somente texto descreve modelos unimodais. A multimodalidade vai além de apenas uma forma de dados.",
      "C": "O requisito de GPU depende do tamanho e do SLA, não é uma definição de multimodalidade.",
      "D": "Gerar apenas imagens é unimodal. O multimodal combina várias modalidades de entrada/saída."
    }
  },
  {
    "id": "aif-c01-ai_services-117",
    "certification_id": "AIF-C01",
    "domain": "AI_SERVICES",
    "difficulty": "easy",
    "tier": "FREE",
    "required_selection_count": 1,
    "active": true,
    "question_text": "Qual serviço da AWS fornece um playground para experimentar modelos de IA generativa?",
    "option_a": "Amazon SageMaker",
    "option_b": "Amazon Comprehend",
    "option_c": "PartyRock",
    "option_d": "Amazon Polly",
    "correct_answers": [
      "C"
    ],
    "explanation_detailed": "O PartyRock é um playground baseado no Amazon Bedrock que permite construir rapidamente aplicativos generativos, experimentando com prompts, componentes de UI e fontes de dados sem gerenciar a infraestrutura. Ele facilita o aprendizado e a prototipagem para casos como bate-papo, sumarização, extração e geração de imagens. Em paralelo, o console e os SDKs do Bedrock permitem o teste direto de FMs e a integração com serviços como o Kendra para RAG. Diferente do Comprehend ou do Polly, que resolvem tarefas específicas de PNL e TTS via API, o PartyRock acelera a ideação e a validação de casos de uso, reduzindo a barreira inicial para que equipes técnicas e não técnicas explorem soluções generativas.",
    "incorrect_explanations": {
      "A": "O SageMaker é a plataforma completa de ML. Útil para ciclos de vida e ajuste fino, mas não é um 'playground' focado em aplicativos generativos prontos.",
      "B": "O Comprehend fornece PNL pronta para uso (entidades, sentimento). Não é um ambiente de prototipagem para aplicativos de IA generativa.",
      "D": "O Polly faz a conversão de texto em fala. Não oferece experimentação ampla com LLMs/fluxos gerativos."
    }
  },
  {
    "id": "aif-c01-ml_development-118",
    "certification_id": "AIF-C01",
    "domain": "ML_DEVELOPMENT",
    "difficulty": "medium",
    "tier": "FREE",
    "required_selection_count": 1,
    "active": true,
    "question_text": "Qual é uma consideração chave ao selecionar um modelo de IA generativa apropriado para um problema de negócios?",
    "option_a": "A popularidade do modelo nas redes sociais",
    "option_b": "Os requisitos de desempenho do modelo",
    "option_c": "A data de desenvolvimento do modelo",
    "option_d": "O país de origem do modelo",
    "correct_answers": [
      "B"
    ],
    "explanation_detailed": "A seleção do modelo deve alinhar os requisitos da tarefa com métricas e restrições: qualidade esperada, latência, custo por token, contexto máximo, segurança e suporte a idiomas/modalidades. Na AWS, comparar os provedores do Bedrock por tamanho de contexto, capacidades de ferramentas (agentes, chamada de função), opções de ajuste fino e guardrails é essencial. Para tarefas longas, janelas maiores e custos previsíveis são importantes; para bate-papo seguro, controles e embasamento com Kendra/OpenSearch são decisivos. A popularidade não garante a adequação. Uma avaliação sistemática com benchmarks e testes A/B reduz o risco de escolher modelos inadequados ao SLA e ao ROI desejados.",
    "incorrect_explanations": {
      "A": "A popularidade não garante qualidade/adequação ao seu caso de uso. As métricas e os requisitos de negócios devem guiar a escolha.",
      "C": "A data de lançamento não implica melhor desempenho para sua tarefa. A avaliação empírica importa mais.",
      "D": "A origem não determina a adequação técnica. Foco em métricas, políticas de uso e conformidade aplicável."
    }
  },
  {
    "id": "aif-c01-ml_development-119",
    "certification_id": "AIF-C01",
    "domain": "ML_DEVELOPMENT",
    "difficulty": "medium",
    "tier": "FREE",
    "required_selection_count": 1,
    "active": true,
    "question_text": "Qual das seguintes NÃO é uma métrica de negócios típica para avaliar aplicações de IA generativa?",
    "option_a": "Taxa de conversão",
    "option_b": "Receita média por usuário",
    "option_c": "Valor vitalício do cliente",
    "option_d": "Contagem de parâmetros do modelo",
    "correct_answers": [
      "D"
    ],
    "explanation_detailed": "As métricas de negócios medem o impacto nos resultados: conversão, retenção, ticket médio, LTV, NPS e tempo economizado. Em aplicações generativas, além das métricas técnicas (latência, custo por 1.000 tokens), é vital capturar KPIs que comprovem o ROI. A contagem de parâmetros é uma característica técnica do modelo e não se traduz em valor de negócio. Na AWS, combine a telemetria da aplicação (painéis do CloudWatch, OpenSearch) e a experimentação (A/B via CloudWatch Evidently ou frameworks) para acompanhar as metas. Mapear métricas para hipóteses e executar ciclos de melhoria contínua evita otimizações 'modelo por modelo' sem evidência de ganho real.",
    "incorrect_explanations": {
      "A": "A taxa de conversão reflete o impacto direto nos objetivos de aquisição/vendas. É uma métrica de negócios central.",
      "B": "A receita média por usuário indica a monetização por base ativa, útil para avaliar crescimento e upsell.",
      "C": "O LTV mede o valor total projetado do cliente, importante para priorizar investimentos e CAC."
    }
  },
  {
    "id": "aif-c01-responsible_ai-120",
    "certification_id": "AIF-C01",
    "domain": "RESPONSIBLE_AI",
    "difficulty": "medium",
    "tier": "FREE",
    "required_selection_count": 1,
    "active": true,
    "question_text": "Qual é um benefício chave da infraestrutura da AWS para aplicações de IA generativa?",
    "option_a": "Ela elimina a necessidade de quaisquer medidas de segurança",
    "option_b": "Ela fornece recursos computacionais gratuitos e ilimitados",
    "option_c": "Ela garante a conformidade com as regulamentações relevantes",
    "option_d": "Ela garante que os modelos de IA nunca cometerão erros",
    "correct_answers": [
      "C"
    ],
    "explanation_detailed": "A AWS fornece controles rigorosos de segurança e conformidade (por exemplo, certificações e recursos de criptografia) que facilitam o cumprimento dos requisitos regulatórios setoriais ao hospedar dados e executar inferências. Para IA generativa, isso permite integrar FMs com governança, auditoria e isolamento de dados (endpoints de VPC, KMS). Serviços como Bedrock, SageMaker e Kendra operam dentro deste ecossistema, permitindo políticas de acesso, registro e monitoramento. Isso não implica infalibilidade do modelo, mas cria uma base confiável para construir soluções em domínios regulamentados. Além disso, os recursos elásticos reduzem o esforço operacional sem prometer recursos gratuitos 'ilimitados' ou a ausência de práticas de segurança.",
    "incorrect_explanations": {
      "A": "Medidas de segurança ainda são necessárias: identidade, criptografia, revisão humana e controles de conteúdo.",
      "B": "A nuvem é elástica, mas cobrada de acordo com o uso. Não há recursos 'gratuitos e ilimitados'.",
      "D": "Os modelos podem cometer erros. A conformidade não elimina a necessidade de avaliação e monitoramento contínuos."
    }
  },
  {
    "id": "aif-c01-ml_development-121",
    "certification_id": "AIF-C01",
    "domain": "ML_DEVELOPMENT",
    "difficulty": "medium",
    "tier": "FREE",
    "required_selection_count": 1,
    "active": true,
    "question_text": "O que é 'chunking' (fragmentação) no contexto de IA generativa?",
    "option_a": "Um método de compressão de dados",
    "option_b": "Uma técnica para dividir grandes entradas em pedaços menores e gerenciáveis",
    "option_c": "Um tipo de arquitetura de modelo",
    "option_d": "Uma maneira de aumentar a precisão do modelo",
    "correct_answers": [
      "B"
    ],
    "explanation_detailed": "A fragmentação divide documentos longos em segmentos menores (fragmentos) para caber no limite de contexto dos LLMs e melhorar a recuperação no RAG. Cada fragmento recebe metadados (título, fonte, data) e, opcionalmente, uma sobreposição para manter a coerência entre as seções. Na AWS, você pode criar fragmentos em pipelines no Amazon SageMaker, armazenar incorporações por fragmento no Amazon OpenSearch Service ou consultar com o Amazon Kendra. Durante a geração (Amazon Bedrock), o sistema recupera os fragmentos mais relevantes (similaridade semântica) e os injeta no prompt. Isso reduz as alucinações, melhora a precisão factual e controla o custo por token, pois apenas partes relevantes do conteúdo são enviadas ao modelo em vez do documento inteiro.",
    "incorrect_explanations": {
      "A": "A compressão reduz o tamanho binário; a fragmentação organiza o texto em blocos semânticos para contornar os limites de contexto e permitir a recuperação eficiente.",
      "C": "A arquitetura do modelo descreve estruturas como Transformers ou difusão. A fragmentação é uma estratégia de pré-processamento e recuperação, não uma arquitetura.",
      "D": "Pode contribuir indiretamente para uma melhor precisão no RAG, mas o objetivo central é permitir o contexto e a recuperação, não 'aumentar a precisão' por si só."
    }
  },
  {
    "id": "aif-c01-ai_fundamentals-122",
    "certification_id": "AIF-C01",
    "domain": "AI_FUNDAMENTALS",
    "difficulty": "easy",
    "tier": "FREE",
    "required_selection_count": 1,
    "active": true,
    "question_text": "Qual das seguintes é uma vantagem chave da simplicidade da IA generativa?",
    "option_a": "Ela sempre produz resultados perfeitos",
    "option_b": "Ela não requer nenhuma entrada humana",
    "option_c": "Pode ser mais fácil de implementar e usar em comparação com métodos tradicionais",
    "option_d": "Ela elimina a necessidade de pré-processamento de dados",
    "correct_answers": [
      "C"
    ],
    "explanation_detailed": "A simplicidade vem do acesso a modelos de fundação pré-treinados e interfaces de alto nível. Em vez de construir pipelines do zero, você chama APIs (por exemplo, Amazon Bedrock) e obtém capacidades como sumarização e extração. Isso reduz o tempo de obtenção de valor e o esforço de MLOps. Serviços como Amazon Q, Amazon Kendra e integrações com o Amazon SageMaker JumpStart aceleram a adoção sem exigir conhecimento profundo de treinamento. Apesar de ser simples para começar, as melhores práticas permanecem: engenharia de prompt, avaliação, monitoramento e controle de custo por token. A simplicidade reduz o atrito inicial, mas não garante a perfeição nem elimina a necessidade de curadoria e validação humanas.",
    "incorrect_explanations": {
      "A": "Os modelos generativos podem cometer erros ou alucinar. A simplicidade de uso não implica perfeição dos resultados.",
      "B": "A interação humana continua essencial para definir requisitos, validar saídas e aplicar a governança.",
      "D": "O pré-processamento (limpeza, fragmentação, metadados) ainda é útil, especialmente em fluxos com RAG e avaliações automáticas."
    }
  },
  {
    "id": "aif-c01-ai_fundamentals-123",
    "certification_id": "AIF-C01",
    "domain": "AI_FUNDAMENTALS",
    "difficulty": "easy",
    "tier": "FREE",
    "required_selection_count": 1,
    "active": true,
    "question_text": "O que é um modelo de difusão em IA generativa?",
    "option_a": "Um modelo que só funciona com dados textuais",
    "option_b": "Um tipo de modelo gerativo frequentemente usado para geração de imagens",
    "option_c": "Um modelo que não requer dados de treinamento",
    "option_d": "Um modelo projetado especificamente para processamento de linguagem natural",
    "correct_answers": [
      "B"
    ],
    "explanation_detailed": "Os modelos de difusão aprendem a reverter um processo de ruído progressivo até reconstruir uma imagem coerente. Durante o treinamento, o modelo observa dados com ruído crescente; durante a geração, ele remove o ruído passo a passo do ruído puro até criar a imagem. Essa abordagem produz resultados de alta qualidade e é a base de muitos sistemas modernos de síntese visual. Na AWS, você pode consumir geradores de imagem via Amazon Bedrock (modelos de fundação de provedores parceiros) ou treinar/inferir modelos personalizados no Amazon SageMaker. As integrações com o Amazon S3 para dados e o monitoramento com o CloudWatch completam um pipeline de produção.",
    "incorrect_explanations": {
      "A": "A difusão é aplicada principalmente a imagens, embora haja pesquisas multimodais. Não é 'somente texto'.",
      "C": "Todo modelo aprende com dados. A difusão requer conjuntos de dados de imagem para treinar o processo reverso.",
      "D": "Embora a difusão exista para áudio e vídeo, não é um modelo projetado 'apenas' para PNL."
    }
  },
  {
    "id": "aif-c01-ai_services-124",
    "certification_id": "AIF-C01",
    "domain": "AI_SERVICES",
    "difficulty": "easy",
    "tier": "FREE",
    "required_selection_count": 1,
    "active": true,
    "question_text": "Qual serviço da AWS é projetado para fornecer capacidades de IA conversacional?",
    "option_a": "Amazon Bedrock",
    "option_b": "Amazon SageMaker",
    "option_c": "Amazon Q",
    "option_d": "Amazon S3",
    "correct_answers": [
      "C"
    ],
    "explanation_detailed": "O Amazon Q é um assistente de IA generativa para o trabalho, com bate-papo seguro, geração de conteúdo e ações em aplicações de negócios. Ele pode ser conectado a fontes internas (por exemplo, através de conectores) para responder com base em seus dados, aplicar políticas e registrar auditorias. Ele pode usar modelos de fundação do Amazon Bedrock e se integra ao ecossistema da AWS para segurança, identidade e monitoramento. Enquanto o Bedrock fornece os FMs e o SageMaker permite a construção de modelos e pipelines personalizados, o Q entrega a experiência conversacional corporativa pronta para uso, com governança, controle de acesso e recursos de produtividade.",
    "incorrect_explanations": {
      "A": "O Bedrock expõe FMs e orquestração, mas não é por si só a experiência conversacional corporativa pronta para uso.",
      "B": "O SageMaker é uma plataforma de ML de ponta a ponta, não um produto de bate-papo corporativo pronto para uso.",
      "D": "O S3 é armazenamento de objetos; não fornece uma interface ou lógica de conversação."
    }
  },
  {
    "id": "aif-c01-ai_services-125",
    "certification_id": "AIF-C01",
    "domain": "AI_SERVICES",
    "difficulty": "medium",
    "tier": "FREE",
    "required_selection_count": 1,
    "active": true,
    "question_text": "Qual é uma consideração chave nas trocas de custo dos serviços de IA generativa da AWS?",
    "option_a": "O esquema de cores da interface do usuário",
    "option_b": "O número de funcionários na empresa",
    "option_c": "Precificação baseada em tokens",
    "option_d": "A localização física do data center",
    "correct_answers": [
      "C"
    ],
    "explanation_detailed": "Muitos provedores no Amazon Bedrock cobram por 1.000 tokens de entrada e saída. Os custos crescem com prompts longos, contexto extenso e respostas prolixas. As estratégias de redução incluem fragmentação eficiente, prompts concisos, RAG focado, cache de resultados e parametrização (temperatura/top-p) para saídas mais curtas, quando possível. Em fluxos de alto volume, considere endpoints otimizados e monitoramento de uso via CloudWatch e Cost Explorer. Para seus próprios modelos no Amazon SageMaker, os custos incluem instâncias, armazenamento e tráfego. Em todos os casos, instrumente a telemetria por recurso, rastreie o custo por chamada e meça o impacto nos negócios, evitando otimizar apenas as métricas técnicas.",
    "incorrect_explanations": {
      "A": "As cores da UI não influenciam o faturamento por token, latência ou taxa de transferência.",
      "B": "O número de funcionários não determina diretamente o custo; o volume de chamadas e tokens o faz.",
      "D": "A região pode afetar marginalmente o preço, mas a principal variável é o volume de tokens e o tipo de modelo."
    }
  },
  {
    "id": "aif-c01-ai_fundamentals-126",
    "certification_id": "AIF-C01",
    "domain": "AI_FUNDAMENTALS",
    "difficulty": "medium",
    "tier": "FREE",
    "required_selection_count": 1,
    "active": true,
    "question_text": "Qual é o objetivo principal das incorporações (embeddings) na IA generativa?",
    "option_a": "Comprimir dados para armazenamento",
    "option_b": "Representar dados em um espaço de alta dimensão",
    "option_c": "Criptografar informações sensíveis",
    "option_d": "Gerar números aleatórios",
    "correct_answers": [
      "B"
    ],
    "explanation_detailed": "As incorporações transformam itens (palavras, sentenças, imagens) em vetores de alta dimensão onde a proximidade reflete a similaridade semântica. Elas são a base para a busca semântica, a desduplicação, a classificação e o RAG. Na AWS, você pode gerar incorporações com modelos disponíveis no Amazon Bedrock ou no Amazon SageMaker e armazená-las em motores com busca vetorial, como o Amazon OpenSearch Service, ou indexá-las com o Amazon Kendra. Durante a inferência, o sistema consulta os vetores mais próximos do conteúdo da pergunta e injeta o contexto recuperado no prompt, reduzindo as alucinações e melhorando a precisão factual. As incorporações não são criptografia ou compressão; são representações para calcular a similaridade.",
    "incorrect_explanations": {
      "A": "Embora possam ser compactas, o objetivo é semântico, não a compressão de dados como o ZIP.",
      "C": "Não fornecem confidencialidade como a criptografia. Os vetores podem até vazar informações se mal gerenciados.",
      "D": "A geração de números aleatórios não está relacionada a representações semânticas."
    }
  },
  {
    "id": "aif-c01-ai_fundamentals-127",
    "certification_id": "AIF-C01",
    "domain": "AI_FUNDAMENTALS",
    "difficulty": "easy",
    "tier": "FREE",
    "required_selection_count": 1,
    "active": true,
    "question_text": "Qual dos seguintes NÃO é um caso de uso típico para IA generativa no atendimento ao cliente?",
    "option_a": "Chatbots",
    "option_b": "Respostas de e-mail automatizadas",
    "option_c": "Assistentes robóticos físicos",
    "option_d": "Geração de FAQ",
    "correct_answers": [
      "C"
    ],
    "explanation_detailed": "No atendimento ao cliente, os LLMs fornecem respostas contextualizadas, resumem históricos, geram e-mails e atualizam FAQs com base no conhecimento corporativo. Os chatbots (por exemplo, o Amazon Lex se integrando com FMs via Amazon Bedrock) lidam com solicitações comuns e escalam para humanos quando necessário. O Amazon Kendra e as incorporações permitem o RAG em manuais e políticas. 'Assistentes robóticos físicos' envolvem robótica e hardware, fora do escopo usual da IA generativa aplicada a canais digitais. A prioridade é reduzir o tempo de resolução, manter um tom apropriado e rastrear as fontes, com guardrails e auditoria.",
    "incorrect_explanations": {
      "A": "Os chatbots são um caso de uso central, especialmente combinando NLU (Lex) e LLMs para respostas ricas.",
      "B": "A geração de e-mails com contexto histórico é uma aplicação comum para acelerar o agente.",
      "D": "Os LLMs geram e atualizam FAQs a partir de bases internas, reduzindo o esforço manual."
    }
  },
  {
    "id": "aif-c01-ai_services-128",
    "certification_id": "AIF-C01",
    "domain": "AI_SERVICES",
    "difficulty": "easy",
    "tier": "FREE",
    "required_selection_count": 1,
    "active": true,
    "question_text": "Qual é uma vantagem chave de usar os serviços de IA generativa da AWS para construir aplicações em termos de velocidade de desenvolvimento?",
    "option_a": "Eles escrevem automaticamente todo o código para você",
    "option_b": "Eles fornecem um tempo de lançamento no mercado mais rápido",
    "option_c": "Eles eliminam a necessidade de testes",
    "option_d": "Eles garantem a implantação instantânea",
    "correct_answers": [
      "B"
    ],
    "explanation_detailed": "Com o Amazon Bedrock, PartyRock e integrações com o Amazon Kendra, você pode prototipar rapidamente sem gerenciar a infraestrutura do modelo. Os modelos de fundação prontos para uso reduzem o trabalho de treinamento, e os SDKs simplificam a integração. O Amazon SageMaker JumpStart fornece exemplos, notebooks e soluções para acelerar POCs e MVPs. O resultado é um tempo menor da ideação ao piloto, com pipelines reprodutíveis posteriormente no SageMaker para produção. Ainda é necessário testar, monitorar e governar o ciclo, mas o tempo de lançamento no mercado cai substancialmente.",
    "incorrect_explanations": {
      "A": "A geração de código pode ajudar, mas as integrações e validações ainda exigem desenvolvimento.",
      "C": "Os testes continuam essenciais para a qualidade, a segurança e as métricas de negócios.",
      "D": "A implantação requer a configuração de endpoints, segurança e observabilidade; não é 'instantânea'."
    }
  },
  {
    "id": "aif-c01-ai_fundamentals-129",
    "certification_id": "AIF-C01",
    "domain": "AI_FUNDAMENTALS",
    "difficulty": "easy",
    "tier": "FREE",
    "required_selection_count": 1,
    "active": true,
    "question_text": "O que é não determinismo no contexto de IA generativa?",
    "option_a": "Um tipo de arquitetura de modelo",
    "option_b": "Um método de pré-processamento de dados",
    "option_c": "A propriedade de produzir saídas diferentes para a mesma entrada",
    "option_d": "Uma técnica para melhorar a precisão do modelo",
    "correct_answers": [
      "C"
    ],
    "explanation_detailed": "Os LLMs frequentemente amostram o próximo token de distribuições de probabilidade; parâmetros como temperatura e top-p controlam essa variabilidade. Assim, a mesma entrada pode gerar saídas diferentes entre as execuções. Para reprodutibilidade, use uma semente fixa e baixa temperatura; para criatividade, aumente a temperatura/top-p. Em ambientes de produção (Amazon Bedrock ou endpoints no Amazon SageMaker), escolha os perfis de acordo com o caso de uso: fluxos de 'fato concreto' exigem menos aleatoriedade; conteúdo criativo permite mais variação. Os logs e o versionamento de prompts ajudam a auditar e comparar as execuções.",
    "incorrect_explanations": {
      "A": "A arquitetura (como um Transformer) não define o comportamento estocástico por si só; a amostragem o faz.",
      "B": "O pré-processamento prepara os dados; não explica as diferentes saídas para a mesma entrada.",
      "D": "O não determinismo não é uma técnica de precisão; é uma característica da geração estocástica."
    }
  },
  {
    "id": "aif-c01-ai_services-130",
    "certification_id": "AIF-C01",
    "domain": "AI_SERVICES",
    "difficulty": "easy",
    "tier": "FREE",
    "required_selection_count": 1,
    "active": true,
    "question_text": "Qual serviço da AWS é projetado para ajudar os desenvolvedores a começar rapidamente com modelos pré-treinados para IA generativa?",
    "option_a": "Amazon EC2",
    "option_b": "Amazon SageMaker JumpStart",
    "option_c": "Amazon RDS",
    "option_d": "Amazon CloudFront",
    "correct_answers": [
      "B"
    ],
    "explanation_detailed": "O Amazon SageMaker JumpStart oferece catálogos de modelos pré-treinados, notebooks de exemplo e soluções de referência para acelerar os POCs. Você pode lançar instâncias de treinamento/implantação com poucos cliques, comparar modelos e adaptá-los ao seu caso de uso. Para IA generativa, o JumpStart inclui modelos de linguagem, visão e difusão, com pipelines prontos para ajuste fino e inferência. Ele é complementar ao Amazon Bedrock, que fornece acesso via API a FMs gerenciados; o JumpStart facilita a experimentação e a operacionalização dentro do ecossistema do SageMaker.",
    "incorrect_explanations": {
      "A": "O EC2 fornece computação bruta. Útil, mas não oferece catálogos e modelos de ML prontos.",
      "C": "O RDS é um banco de dados relacional. Não acelera a exploração de modelos de IA.",
      "D": "O CloudFront é uma CDN para distribuição de conteúdo, não um hub para modelos de ML."
    }
  },
  {
    "id": "aif-c01-ml_development-131",
    "certification_id": "AIF-C01",
    "domain": "ML_DEVELOPMENT",
    "difficulty": "medium",
    "tier": "FREE",
    "required_selection_count": 1,
    "active": true,
    "question_text": "O que é a Geração Aumentada por Recuperação (RAG)?",
    "option_a": "Uma técnica para gerar novos dados",
    "option_b": "Um método para combinar informações recuperadas com a geração de modelos",
    "option_c": "Um tipo de arquitetura de modelo",
    "option_d": "Um algoritmo de compressão de dados",
    "correct_answers": [
      "B"
    ],
    "explanation_detailed": "O RAG busca evidências em fontes externas (documentos, wikis, bancos de dados) e injeta o contexto relevante no prompt antes da geração. Isso embasa a resposta, reduz as alucinações e permite atualizar o 'conhecimento' sem treinar novamente o modelo. Na AWS, use o Amazon Kendra ou o Amazon OpenSearch Service para recuperação, armazene as incorporações no índice e chame o LLM via Amazon Bedrock. Registre as fontes, implemente guardrails e meça a qualidade com avaliações automáticas e humanas. O RAG separa a 'memória de conteúdo' dos parâmetros do modelo, favorecendo a governança e o custo.",
    "incorrect_explanations": {
      "A": "O RAG não se limita a criar novos dados; ele integra busca + geração com base em evidências.",
      "C": "É um padrão de sistema, não uma arquitetura específica como um Transformer.",
      "D": "Não envolve compressão; lida com a busca semântica e o condicionamento do prompt."
    }
  },
  {
    "id": "aif-c01-ai_services-132",
    "certification_id": "AIF-C01",
    "domain": "AI_SERVICES",
    "difficulty": "medium",
    "tier": "FREE",
    "required_selection_count": 1,
    "active": true,
    "question_text": "Qual serviço da AWS é adequado para armazenar incorporações (embeddings) em um banco de dados vetorial?",
    "option_a": "Amazon S3",
    "option_b": "Amazon RDS",
    "option_c": "Amazon OpenSearch Service",
    "option_d": "Amazon EC2",
    "correct_answers": [
      "C"
    ],
    "explanation_detailed": "O Amazon OpenSearch Service suporta k-NN (k-vizinhos mais próximos) e índices vetoriais, permitindo o armazenamento e a recuperação de incorporações com consultas de similaridade. Isso é ideal para a busca semântica e o RAG. O pipeline típico gera incorporações (SageMaker ou Bedrock), indexa vetores e metadados no OpenSearch e, mediante consulta, retorna os mais similares para compor o prompt enviado ao LLM. O S3 armazena objetos; o RDS é relacional; o EC2 é computação geral. Para busca por proximidade em altas dimensões, os índices vetoriais do OpenSearch fornecem escalabilidade, filtragem de metadados e integração com o ecossistema da AWS.",
    "incorrect_explanations": {
      "A": "O S3 armazena blobs; não oferece busca vetorial nativa por similaridade.",
      "B": "O RDS é relacional; pode armazenar vetores, mas não oferece nativamente uma busca k-NN eficiente.",
      "D": "O EC2 hospeda software, mas não é um serviço de banco de dados vetorial gerenciado."
    }
  },
  {
    "id": "aif-c01-ai_fundamentals-133",
    "certification_id": "AIF-C01",
    "domain": "AI_FUNDAMENTALS",
    "difficulty": "medium",
    "tier": "FREE",
    "required_selection_count": 1,
    "active": true,
    "question_text": "Qual é o objetivo principal de ajustar o parâmetro de temperatura na inferência?",
    "option_a": "Controlar a temperatura física do servidor",
    "option_b": "Ajustar a criatividade ou a aleatoriedade da saída do modelo",
    "option_c": "Aumentar a velocidade de processamento do modelo",
    "option_d": "Reduzir o consumo de energia",
    "correct_answers": [
      "B"
    ],
    "explanation_detailed": "A temperatura controla a 'planicidade' da distribuição de probabilidade sobre os próximos tokens. Valores altos tornam a distribuição mais uniforme (mais criatividade/variação); valores baixos a tornam mais nítida (respostas mais previsíveis). Em fluxos de produção nos endpoints do Amazon Bedrock ou do SageMaker, defina uma temperatura baixa para respostas factuais e consistentes; aumente-a para brainstorming e ideação. Combine com top-p e limites de comprimento para equilibrar o custo por token. Registre os parâmetros por chamada para reproduzir os comportamentos quando necessário.",
    "incorrect_explanations": {
      "A": "Não tem relação com o hardware térmico. É um hiperparâmetro de amostragem na geração.",
      "C": "A temperatura não acelera o modelo; ela influencia a diversidade da linguagem gerada.",
      "D": "O consumo de energia está mais relacionado à carga e ao hardware do que à temperatura de amostragem."
    }
  },
  {
    "id": "aif-c01-ai_fundamentals-134",
    "certification_id": "AIF-C01",
    "domain": "AI_FUNDAMENTALS",
    "difficulty": "medium",
    "tier": "FREE",
    "required_selection_count": 1,
    "active": true,
    "question_text": "O que é um prompt de cadeia de pensamento?",
    "option_a": "Uma cadeia física usada em hardware de IA",
    "option_b": "Um prompt que incentiva o modelo a mostrar seu processo de raciocínio",
    "option_c": "Um método de vincular vários modelos de IA",
    "option_d": "Uma técnica para criptografar prompts",
    "correct_answers": [
      "B"
    ],
    "explanation_detailed": "A cadeia de pensamento instrui o modelo a decompor o problema em etapas intermediárias antes da resposta final, melhorando as tarefas de raciocínio (cálculo, lógica, planejamento). Em ambientes empresariais, prefira o 'raciocínio estruturado' com verificações e regras para evitar o vazamento de raciocínios sensíveis. Na AWS, você pode acoplar validações e chamadas de função (Agents for Amazon Bedrock) para executar etapas verificadas e registrar evidências. Avalie o impacto no custo (mais tokens) e na latência. Para auditoria, registre prompts, saídas e fontes (Kendra/OpenSearch) e defina guardrails para remover conteúdo inadequado.",
    "incorrect_explanations": {
      "A": "Não é um componente de hardware; é uma técnica de engenharia de prompt.",
      "C": "Não vincula vários modelos por si só; lida com como o modelo organiza a resposta.",
      "D": "Não fornece confidencialidade/criptografia; apenas guia a forma do raciocínio gerado."
    }
  },
  {
    "id": "aif-c01-ml_development-135",
    "certification_id": "AIF-C01",
    "domain": "ML_DEVELOPMENT",
    "difficulty": "medium",
    "tier": "FREE",
    "required_selection_count": 1,
    "active": true,
    "question_text": "Qual das seguintes NÃO é um método típico para o ajuste fino de um modelo de fundação?",
    "option_a": "Ajuste de instrução",
    "option_b": "Aprendizado por transferência",
    "option_c": "Ajuste físico",
    "option_d": "Pré-treinamento contínuo",
    "correct_answers": [
      "C"
    ],
    "explanation_detailed": "Os FMs são adaptados pelo ajuste de instrução (treinamento em pares de instrução-resposta), ajuste fino clássico (aprendizado por transferência) ou pré-treinamento contínuo em domínios específicos. Essas técnicas refinam os pesos para novas tarefas, estilos e vocabulário. Na AWS, o Amazon SageMaker oferece pipelines de ajuste fino e o Amazon Bedrock fornece opções de ajuste gerenciado para certos provedores. O 'ajuste físico' não existe: os ajustes ocorrem no espaço de parâmetros do modelo. A escolha depende dos dados disponíveis, do custo, da governança e da necessidade de reter o conhecimento geral do FM.",
    "incorrect_explanations": {
      "A": "O ajuste de instrução é uma prática comum para ensinar o modelo a seguir comandos com qualidade.",
      "B": "O aprendizado por transferência é a base para a adaptação eficiente, aproveitando o pré-treinamento.",
      "D": "O pré-treinamento contínuo estende a base do modelo com dados de domínio adicionais."
    }
  },
  {
    "id": "aif-c01-ml_development-136",
    "certification_id": "AIF-C01",
    "domain": "ML_DEVELOPMENT",
    "difficulty": "medium",
    "tier": "FREE",
    "required_selection_count": 1,
    "active": true,
    "question_text": "Para que serve a métrica ROUGE na avaliação de modelos de fundação?",
    "option_a": "Para medir a vermelhidão da saída do modelo",
    "option_b": "Para avaliar a qualidade dos resumos gerados",
    "option_c": "Para calcular a eficiência energética do modelo",
    "option_d": "Para determinar a velocidade de processamento do modelo",
    "correct_answers": [
      "B"
    ],
    "explanation_detailed": "O ROUGE compara a sobreposição de n-gramas, subsequências e pares entre o resumo gerado e um ou mais resumos de referência. É útil para avaliar a sumarização abstrativa e extrativa. Em pipelines da AWS, você pode executar a avaliação automática em trabalhos do Amazon SageMaker, armazenar as métricas no Amazon S3 e visualizar no CloudWatch ou em painéis. O ROUGE não mede a compreensão profunda, então combine-o com métricas humanas (adequação, factualidade) e, quando aplicável, RAG com citabilidade (Kendra/OpenSearch) para reduzir as alucinações.",
    "incorrect_explanations": {
      "A": "O nome 'ROUGE' é um acrônimo; não mede cor. Ele avalia sobreposições textuais.",
      "C": "A eficiência energética requer métricas de hardware/uso; o ROUGE avalia a qualidade da sumarização.",
      "D": "A velocidade é latência/taxa de transferência, não capturada pelo ROUGE."
    }
  },
  {
    "id": "aif-c01-ai_services-137",
    "certification_id": "AIF-C01",
    "domain": "AI_SERVICES",
    "difficulty": "medium",
    "tier": "FREE",
    "required_selection_count": 1,
    "active": true,
    "question_text": "Qual é o objetivo principal de usar o Agents for Amazon Bedrock?",
    "option_a": "Contratar agentes humanos para tarefas de IA",
    "option_b": "Lidar com tarefas de várias etapas em aplicações de IA",
    "option_c": "Manter fisicamente o hardware de IA",
    "option_d": "Reduzir o custo dos serviços de IA",
    "correct_answers": [
      "B"
    ],
    "explanation_detailed": "O Agents for Amazon Bedrock orquestra fluxos de várias etapas: interpretando instruções, chamando ferramentas/integrações, consultando bases (Kendra/OpenSearch) e executando ações antes de responder. Eles encapsulam o raciocínio estruturado e a chamada de função, reduzindo a lógica personalizada no lado do cliente. No uso corporativo, os recursos, as políticas e as validações são definidos para manter a rastreabilidade e a segurança. O foco é decompor tarefas complexas (por exemplo, abrir um tíquete, buscar contexto, resumir, registrar) com consistência, não reduzir os custos diretamente — embora a automação eficiente possa impactar as despesas.",
    "incorrect_explanations": {
      "A": "Não envolve a contratação de pessoas; são orquestradores baseados em modelos.",
      "C": "O hardware é gerenciado pela AWS; os agentes operam na camada de aplicação/orquestração.",
      "D": "Eles podem otimizar as operações, mas o objetivo principal é a coordenação de etapas e ferramentas."
    }
  },
  {
    "id": "aif-c01-ml_development-138",
    "certification_id": "AIF-C01",
    "domain": "ML_DEVELOPMENT",
    "difficulty": "medium",
    "tier": "FREE",
    "required_selection_count": 1,
    "active": true,
    "question_text": "Qual das seguintes é uma consideração chave ao selecionar um modelo pré-treinado?",
    "option_a": "A popularidade do modelo nas redes sociais",
    "option_b": "O tamanho físico do servidor que hospeda o modelo",
    "option_c": "As capacidades de comprimento de entrada/saída do modelo",
    "option_d": "O esquema de cores da documentação do modelo",
    "correct_answers": [
      "C"
    ],
    "explanation_detailed": "A janela de contexto (tokens de entrada/saída) determina se o modelo pode lidar com seus prompts e documentos. Para o RAG, janelas maiores reduzem a necessidade de truncamento, mas aumentam o custo. Além disso, avalie idiomas, ferramentas (chamada de função), segurança, desempenho e latência. Na AWS, compare FMs no Amazon Bedrock e alternativas no SageMaker JumpStart. Valide com testes em seu domínio e monitore as métricas de negócios em produção.",
    "incorrect_explanations": {
      "A": "A popularidade não garante a adequação; avalie as métricas e os requisitos do caso de uso.",
      "B": "O tamanho do servidor é um detalhe operacional; a janela de contexto é um fator de viabilidade direto.",
      "D": "As cores da documentação não afetam a capacidade ou a qualidade do modelo."
    }
  },
  {
    "id": "aif-c01-responsible_ai-139",
    "certification_id": "AIF-C01",
    "domain": "RESPONSIBLE_AI",
    "difficulty": "medium",
    "tier": "FREE",
    "required_selection_count": 1,
    "active": true,
    "question_text": "O que é sequestro de prompt (prompt hijacking) no contexto de engenharia de prompt?",
    "option_a": "Um método de otimizar prompts",
    "option_b": "Uma técnica para roubar prompts de concorrentes",
    "option_c": "Um ataque onde o modelo é enganado para ignorar o prompt pretendido",
    "option_d": "Uma maneira de acelerar o processamento de prompts",
    "correct_answers": [
      "C"
    ],
    "explanation_detailed": "O sequestro de prompt ocorre quando entradas maliciosas injetam instruções que fazem com que o modelo ignore as diretrizes originais (por exemplo, em conteúdo recuperado via RAG). Mitigue com delimitação estrita de instruções, filtros de conteúdo, normalização de fontes, 'encapsulamento de contexto' e validações pós-geração. Na AWS, combine as Guardrails do Amazon Bedrock, a verificação de fontes (Kendra/OpenSearch) e o monitoramento com CloudWatch/CloudTrail. Mantenha listas de bloqueio, higienização e testes adversários contínuos.",
    "incorrect_explanations": {
      "A": "Não é mera otimização; é um vetor de ataque que altera o comportamento esperado.",
      "B": "Não se trata de 'roubar' os prompts de outros, mas de manipular seu modelo por meio de entradas.",
      "D": "Não acelera o processamento; compromete a integridade e a segurança das respostas."
    }
  },
  {
    "id": "aif-c01-ml_development-140",
    "certification_id": "AIF-C01",
    "domain": "ML_DEVELOPMENT",
    "difficulty": "medium",
    "tier": "FREE",
    "required_selection_count": 1,
    "active": true,
    "question_text": "Qual é o objetivo principal do ajuste de instrução em modelos de fundação?",
    "option_a": "Ensinar o modelo a seguir instruções específicas",
    "option_b": "Reduzir o tamanho do modelo",
    "option_c": "Aumentar a velocidade de processamento do modelo",
    "option_d": "Alterar a linguagem de programação do modelo",
    "correct_answers": [
      "A"
    ],
    "explanation_detailed": "O ajuste de instrução treina o FM em pares de instrução-resposta selecionados para que ele aprenda a obedecer a solicitações em linguagem natural com os formatos esperados. Isso melhora a aderência a políticas, estilo e estrutura de saída. Na AWS, use pipelines no Amazon SageMaker para ajuste fino ou opções gerenciadas no Amazon Bedrock quando disponíveis. A técnica não reduz o tamanho nem altera a linguagem; ela refina o comportamento para tarefas e padrões de resposta específicos.",
    "incorrect_explanations": {
      "B": "A compressão/destilação pode reduzir o tamanho, mas esse não é o objetivo do ajuste de instrução.",
      "C": "A velocidade depende do hardware, das otimizações e do tamanho; o ajuste de instrução foca no comportamento.",
      "D": "Não altera a linguagem de programação; padroniza como o modelo segue as instruções."
    }
  },
  {
    "id": "aif-c01-ml_development-141",
    "certification_id": "AIF-C01",
    "domain": "ML_DEVELOPMENT",
    "difficulty": "medium",
    "tier": "FREE",
    "required_selection_count": 1,
    "active": true,
    "question_text": "Para que o BERTScore é usado na avaliação de modelos de fundação?",
    "option_a": "Para medir a eficiência energética do modelo",
    "option_b": "Para avaliar a qualidade do texto gerado",
    "option_c": "Para calcular a velocidade de processamento do modelo",
    "option_d": "Para determinar o valor de mercado do modelo",
    "correct_answers": [
      "B"
    ],
    "explanation_detailed": "O BERTScore mede a similaridade semântica entre o texto gerado e o de referência usando incorporações contextualizadas (por exemplo, BERT). Em vez de apenas contar n-gramas (como o ROUGE), ele captura relações semânticas mais sutis. Na AWS, você pode executar avaliações em trabalhos do Amazon SageMaker, salvar as métricas no Amazon S3 e visualizar no CloudWatch. Combine o BERTScore com a avaliação humana e a verificação de fatos (RAG + Kendra/OpenSearch) para uma visão robusta da qualidade.",
    "incorrect_explanations": {
      "A": "A eficiência energética requer métricas de hardware/consumo, não similaridade semântica.",
      "C": "A velocidade é latência/taxa de transferência. O BERTScore avalia a qualidade textual.",
      "D": "O valor de mercado não é uma métrica técnica da qualidade da geração."
    }
  },
  {
    "id": "aif-c01-ml_development-142",
    "certification_id": "AIF-C01",
    "domain": "ML_DEVELOPMENT",
    "difficulty": "easy",
    "tier": "FREE",
    "required_selection_count": 1,
    "active": true,
    "question_text": "Qual é o principal benefício de usar o aprendizado em contexto para a personalização de modelos de fundação?",
    "option_a": "Não requer dados de treinamento adicionais",
    "option_b": "Sempre produz resultados perfeitos",
    "option_c": "Reduz o tamanho do modelo",
    "option_d": "Elimina a necessidade de prompts",
    "correct_answers": [
      "A"
    ],
    "explanation_detailed": "O aprendizado em contexto mostra alguns exemplos diretamente no prompt para guiar o comportamento do LLM, sem treinar novamente. É útil quando há poucos dados e necessidade de uma resposta imediata. No Amazon Bedrock, você estrutura o prompt com instruções, formato de saída e 2 a 5 exemplos, combinando opcionalmente o RAG (Kendra/OpenSearch) para embasamento. Monitore o custo por token e a latência, pois os exemplos aumentam o contexto. Se a tarefa exigir forte consistência, evolua para o ajuste fino no Amazon SageMaker.",
    "incorrect_explanations": {
      "B": "Nenhuma técnica garante a perfeição; avalie e monitore.",
      "C": "Não altera o tamanho do modelo; apenas guia o comportamento por meio de exemplos no prompt.",
      "D": "Depende de prompts bem estruturados; não os elimina."
    }
  },
  {
    "id": "aif-c01-ml_development-143",
    "certification_id": "AIF-C01",
    "domain": "ML_DEVELOPMENT",
    "difficulty": "medium",
    "tier": "FREE",
    "required_selection_count": 1,
    "active": true,
    "question_text": "Qual é um risco potencial de usar o aprendizado de tiro zero (zero-shot) na engenharia de prompt?",
    "option_a": "O modelo pode ter um desempenho ruim em tarefas para as quais não foi explicitamente treinado",
    "option_b": "O modelo se recusará a gerar qualquer saída",
    "option_c": "O modelo só funcionará com dados numéricos",
    "option_d": "O modelo consumirá energia excessiva",
    "correct_answers": [
      "A"
    ],
    "explanation_detailed": "O tiro zero pede ao modelo para realizar uma tarefa sem exemplos no prompt. Em domínios específicos ou formatos estritos, isso pode degradar a qualidade, levando a erros e alucinações. Para mitigar, adicione exemplos (poucas tentativas, few-shot), use o RAG com Kendra ou OpenSearch para embasamento e valide com verificações automáticas. Em produção no Amazon Bedrock, monitore as métricas de precisão e reelabore os prompts/fluxos quando o desempenho cair.",
    "incorrect_explanations": {
      "B": "Os modelos geralmente respondem; o problema é a qualidade, não a ausência total de saída.",
      "C": "Os LLMs trabalham com texto, não apenas com dados numéricos. A limitação é semântica e instrucional.",
      "D": "O consumo depende da carga/infraestrutura; o tiro zero não implica gasto excessivo de energia."
    }
  },
  {
    "id": "aif-c01-ml_development-144",
    "certification_id": "AIF-C01",
    "domain": "ML_DEVELOPMENT",
    "difficulty": "medium",
    "tier": "FREE",
    "required_selection_count": 1,
    "active": true,
    "question_text": "Qual é o objetivo principal do aprendizado por reforço com feedback humano (RLHF) no treinamento de modelos de fundação?",
    "option_a": "Reduzir o consumo de energia do modelo",
    "option_b": "Melhorar o desempenho do modelo com base em avaliações humanas",
    "option_c": "Aumentar o tamanho do modelo",
    "option_d": "Traduzir o modelo para diferentes idiomas",
    "correct_answers": [
      "B"
    ],
    "explanation_detailed": "O RLHF usa as preferências humanas para treinar um modelo de recompensa e, em seguida, otimizar o FM por meio de reforço para respostas mais alinhadas. Isso melhora a utilidade, a segurança e o estilo. Na AWS, você pode treinar componentes no Amazon SageMaker, gerenciar dados rotulados no S3 e avaliar os resultados com pipelines automatizados. Ainda requer curadoria e governança para evitar enviesar o modelo para padrões inadequados.",
    "incorrect_explanations": {
      "A": "O objetivo não é a eficiência energética; é o alinhamento com as preferências humanas.",
      "C": "O RLHF não aumenta o tamanho do modelo por si só; ele ajusta o comportamento.",
      "D": "A tradução envolve modelos/serviços como o Amazon Translate, não o RLHF."
    }
  },
  {
    "id": "aif-c01-ml_development-145",
    "certification_id": "AIF-C01",
    "domain": "ML_DEVELOPMENT",
    "difficulty": "easy",
    "tier": "FREE",
    "required_selection_count": 1,
    "active": true,
    "question_text": "Qual das seguintes NÃO é uma consideração típica ao preparar dados para o ajuste fino (fine-tuning) de um modelo de fundação?",
    "option_a": "Curadoria de dados",
    "option_b": "Tamanho dos dados",
    "option_c": "Rotulagem de dados",
    "option_d": "Codificação de cores dos dados",
    "correct_answers": [
      "D"
    ],
    "explanation_detailed": "A preparação de dados envolve seleção, limpeza, balanceamento, desduplicação, anonimização e rotulagem consistente. O tamanho afeta a generalização e o custo; a curadoria define a qualidade e a cobertura dos casos. Na AWS, use o AWS Glue/SageMaker Processing para transformação e o S3 como um data lake. A 'codificação de cores dos dados' não é relevante para texto/código e raramente para imagens, onde o que importa são anotações e formatos corretos. Dados bem preparados resultam em um ajuste fino mais estável e útil.",
    "incorrect_explanations": {
      "A": "A curadoria decide o que entra no treinamento e garante a representatividade e a qualidade.",
      "B": "O tamanho impacta o desempenho e o custo; poucos dados tendem ao sobreajuste (overfitting).",
      "C": "A rotulagem consistente é vital para o ajuste de instruções/ajuste fino com pares de alta qualidade."
    }
  },
  {
    "id": "aif-c01-ml_development-146",
    "certification_id": "AIF-C01",
    "domain": "ML_DEVELOPMENT",
    "difficulty": "medium",
    "tier": "FREE",
    "required_selection_count": 1,
    "active": true,
    "question_text": "O que é a criação de modelos de prompt (prompt templating) no contexto da engenharia de prompt?",
    "option_a": "Um método de impressão física de prompts",
    "option_b": "Uma técnica para criar estruturas de prompt reutilizáveis",
    "option_c": "Uma forma de criptografar prompts",
    "option_d": "Um processo de tradução de prompts para diferentes idiomas",
    "correct_answers": [
      "B"
    ],
    "explanation_detailed": "A criação de modelos de prompt define modelos padronizados com seções claras: objetivo, contexto, dados de entrada, formato de saída, exemplos e restrições. Isso melhora a consistência, facilita o versionamento e a medição A/B. Na AWS, armazene modelos no S3, gerencie versões via CodeCommit/CodePipeline e teste no SageMaker ou Bedrock. A padronização de prompts reduz a variação, acelera novos casos de uso e apoia a governança e a auditoria de alterações.",
    "incorrect_explanations": {
      "A": "Não se trata de impressão; é o design lógico de instruções para LLMs.",
      "C": "A segurança é obtida com KMS/controle de acesso; a criação de modelos de prompt não criptografa.",
      "D": "Os modelos podem ser traduzidos, mas a técnica em si não é um processo de tradução."
    }
  },
  {
    "id": "aif-c01-ml_development-147",
    "certification_id": "AIF-C01",
    "domain": "ML_DEVELOPMENT",
    "difficulty": "easy",
    "tier": "FREE",
    "required_selection_count": 1,
    "active": true,
    "question_text": "Qual é a principal vantagem de usar o aprendizado com poucos exemplos (few-shot learning) na engenharia de prompt?",
    "option_a": "Não requer exemplos no prompt",
    "option_b": "Permite que o modelo aprenda com um pequeno número de exemplos",
    "option_c": "Sempre produz resultados perfeitos",
    "option_d": "Reduz o consumo de energia do modelo",
    "correct_answers": [
      "B"
    ],
    "explanation_detailed": "O aprendizado com poucos exemplos inclui alguns exemplos representativos no prompt para guiar o estilo, formato e raciocínio sem retreinar o modelo. Isso melhora a adesão a padrões específicos e reduz as ambiguidades do aprendizado sem exemplos (zero-shot). No Bedrock, combine o aprendizado com poucos exemplos com RAG para respostas fundamentadas e controle os custos limitando o número de exemplos e tokens. Registre as versões do prompt e avalie com métricas de qualidade para garantir a estabilidade.",
    "incorrect_explanations": {
      "A": "O aprendizado com poucos exemplos depende de exemplos; o aprendizado sem exemplos (zero-shot) é o caso sem exemplos.",
      "C": "Não há garantia de perfeição; validação e monitoramento ainda são necessários.",
      "D": "O consumo está ligado a tokens/hardware; o aprendizado com poucos exemplos aumenta os tokens no prompt."
    }
  },
  {
    "id": "aif-c01-ai_services-148",
    "certification_id": "AIF-C01",
    "domain": "AI_SERVICES",
    "difficulty": "medium",
    "tier": "FREE",
    "required_selection_count": 1,
    "active": true,
    "question_text": "Qual serviço da AWS é adequado para armazenar embeddings em um banco de dados relacional?",
    "option_a": "Amazon DynamoDB",
    "option_b": "Amazon S3",
    "option_c": "Amazon Aurora",
    "option_d": "Amazon EC2",
    "correct_answers": [
      "C"
    ],
    "explanation_detailed": "Embora os mecanismos de vetores (como o OpenSearch) sejam ideais, é possível persistir embeddings em um banco de dados relacional para integrações transacionais. O Amazon Aurora (compatível com MySQL/PostgreSQL) oferece escalabilidade e suporte para tipos numéricos e extensões que facilitam o armazenamento de vetores e metadados, bem como consultas que combinam filtros relacionais com IDs de vetores pré-selecionados. Use o Aurora para manter a consistência com sistemas transacionais e o OpenSearch quando precisar de busca k-NN em grande escala e baixa latência. Gere embeddings no SageMaker/Bedrock e gerencie dados no S3 como uma camada de origem.",
    "incorrect_explanations": {
      "A": "O DynamoDB é um banco de dados NoSQL de chave-valor/documento; ele pode armazenar vetores, mas não oferece SQL relacional.",
      "B": "O S3 armazena objetos; não oferece busca vetorial nativa por similaridade.",
      "D": "O EC2 é computação; não é um serviço de banco de dados gerenciado."
    }
  },
  {
    "id": "aif-c01-ai_fundamentals-149",
    "certification_id": "AIF-C01",
    "domain": "AI_FUNDAMENTALS",
    "difficulty": "medium",
    "tier": "FREE",
    "required_selection_count": 1,
    "active": true,
    "question_text": "Qual é uma consideração chave ao avaliar se um modelo de fundação atende efetivamente aos objetivos de negócio?",
    "option_a": "A popularidade do modelo nas redes sociais",
    "option_b": "O tamanho físico do servidor que hospeda o modelo",
    "option_c": "O impacto do modelo no engajamento do usuário",
    "option_d": "O esquema de cores da interface de usuário do modelo",
    "correct_answers": [
      "C"
    ],
    "explanation_detailed": "Avalie os resultados de negócio: engajamento, conversão, CSAT, LTV e tempo economizado. Métricas técnicas (latência, custo por 1.000 tokens) importam, mas precisam se conectar aos KPIs. Na AWS, colete telemetria no CloudWatch, experimente com o Evidently, armazene logs no S3 e visualize em dashboards (OpenSearch/Kibana). Realize testes A/B com prompts/modelos para validar o impacto real e evitar otimizações que não movem as métricas de negócio.",
    "incorrect_explanations": {
      "A": "A popularidade não garante eficácia para o seu contexto e objetivos.",
      "B": "As dimensões físicas não refletem o valor entregue ao usuário.",
      "D": "O esquema de cores da interface de usuário não mede o sucesso do negócio."
    }
  },
  {
    "id": "aif-c01-ai_fundamentals-150",
    "certification_id": "AIF-C01",
    "domain": "AI_FUNDAMENTALS",
    "difficulty": "easy",
    "tier": "FREE",
    "required_selection_count": 1,
    "active": true,
    "question_text": "Qual é o objetivo principal dos prompts negativos na engenharia de prompt?",
    "option_a": "Fazer o modelo gerar emoções negativas",
    "option_b": "Dizer ao modelo o que evitar em sua saída",
    "option_c": "Reduzir o consumo de energia do modelo",
    "option_d": "Diminuir a velocidade de processamento do modelo",
    "correct_answers": [
      "B"
    ],
    "explanation_detailed": "Prompts negativos especificam o que não deve aparecer na resposta (por exemplo, 'não inclua PII', 'não invente citações', 'sem HTML'). Em fluxos de produção no Amazon Bedrock, combine prompts negativos com validações pós-processamento e guardrails para aplicar políticas. Isso reduz alucinações e conteúdo indesejado, melhora a conformidade e torna a saída mais previsível. A técnica não altera diretamente o consumo de energia ou a latência; é um controle de conteúdo.",
    "incorrect_explanations": {
      "A": "Não define 'emoções negativas'; define restrições de conteúdo e formato.",
      "C": "A energia depende da carga e do hardware; os prompts negativos não impactam diretamente isso.",
      "D": "A velocidade é uma função do modelo/infraestrutura; os negativos são instruções sem efeito direto na latência."
    }
  },
  {
    "id": "aif-c01-ml_development-151",
    "certification_id": "AIF-C01",
    "domain": "ML_DEVELOPMENT",
    "difficulty": "medium",
    "tier": "FREE",
    "required_selection_count": 1,
    "active": true,
    "question_text": "O que é o pré-treinamento contínuo no contexto de modelos de fundação?",
    "option_a": "Um método de retreinar constantemente o modelo com novos dados",
    "option_b": "Uma técnica para treinar modelos 24 horas por dia, 7 dias por semana",
    "option_c": "Uma forma de treinar modelos usando matemática contínua",
    "option_d": "Um processo de treinamento de modelos em uma superfície física contínua",
    "correct_answers": [
      "A"
    ],
    "explanation_detailed": "O pré-treinamento contínuo (atualização contínua/pré-treinamento) atualiza um modelo de fundação já pré-treinado com dados de domínio adicionais para expandir a cobertura e reduzir a obsolescência, preservando o conhecimento geral. O foco é adaptar vocabulário, estilo e novos fatos sem 'esquecer' o que já sabe (mitigando o esquecimento catastrófico com técnicas como regularização, mistura de dados e checkpoints). Na AWS, armazene o corpus no Amazon S3, orquestre trabalhos de treinamento no Amazon SageMaker, rastreie métricas no Amazon CloudWatch e gerencie versões com o Model Registry. Use avaliações automáticas e humanas para verificar ganhos reais e políticas de dados (IAM, KMS) para segurança. Não confunda com o ajuste fino de instruções; aqui o objetivo é estender o pré-treinamento base com textos amplos adicionais.",
    "incorrect_explanations": {
      "B": "Treinar 24/7 descreve um cronograma, não a técnica de atualizar o conhecimento do modelo com dados adicionais.",
      "C": "'Matemática contínua' não caracteriza o processo; trata-se de expandir o corpus e refinar parâmetros.",
      "D": "Não envolve superfícies físicas; é uma atualização estatística dos pesos com novos dados."
    }
  },
  {
    "id": "aif-c01-responsible_ai-152",
    "certification_id": "AIF-C01",
    "domain": "RESPONSIBLE_AI",
    "difficulty": "medium",
    "tier": "FREE",
    "required_selection_count": 1,
    "active": true,
    "question_text": "O que é envenenamento de prompt (prompt poisoning) no contexto dos riscos da engenharia de prompt?",
    "option_a": "Um método de otimização de prompts",
    "option_b": "Uma técnica para melhorar a qualidade do prompt",
    "option_c": "Um ataque onde conteúdo malicioso é inserido nos dados de treinamento ou prompts",
    "option_d": "Uma forma de acelerar o processamento do prompt",
    "correct_answers": [
      "C"
    ],
    "explanation_detailed": "O envenenamento de prompt ocorre quando um invasor injeta instruções ou conteúdo malicioso no material que o modelo consome (por exemplo, documentos em RAG ou dados de ajuste), induzindo respostas incorretas, vazamento de dados ou não conformidade com as políticas. Mitigue com validação de fontes e sanitização de conteúdo, filtros de PII e tópicos, isolamento entre instruções do sistema e contexto recuperado, verificações pós-geração e auditoria. No Amazon Bedrock, use Guardrails para bloquear classes de conteúdo, políticas de PII e termos proibidos; com o Amazon Kendra/OpenSearch, controle a relevância, metadados e proveniência. Registre eventos no CloudTrail/CloudWatch e aplique IAM/KMS para proteger pipelines e repositórios.",
    "incorrect_explanations": {
      "A": "A otimização busca melhorar as respostas; o envenenamento visa a manipulação hostil do comportamento.",
      "B": "A melhoria do prompt não injeta vetores maliciosos; o envenenamento é adversarial.",
      "D": "Não acelera o processamento; introduz um risco de segurança e integridade."
    }
  },
  {
    "id": "aif-c01-ml_development-153",
    "certification_id": "AIF-C01",
    "domain": "ML_DEVELOPMENT",
    "difficulty": "medium",
    "tier": "FREE",
    "required_selection_count": 1,
    "active": true,
    "question_text": "Para que serve a pontuação BLEU na avaliação de modelos de fundação?",
    "option_a": "Para medir a eficiência energética do modelo",
    "option_b": "Para avaliar a qualidade de traduções automáticas",
    "option_c": "Para calcular a velocidade de processamento do modelo",
    "option_d": "Para determinar o valor de mercado do modelo",
    "correct_answers": [
      "B"
    ],
    "explanation_detailed": "A pontuação BLEU avalia a tradução automática comparando n-gramas da saída com traduções de referência humanas e penalizando frases muito curtas (penalidade de brevidade). É objetiva e barata, mas limitada: não captura bem sinônimos ou fluência global. Use o BLEU junto com métricas complementares (COMET, BERTScore) e revisão humana para decisões de produção. Na AWS, execute avaliações em trabalhos no Amazon SageMaker, armazene resultados no S3 e crie dashboards no CloudWatch ou OpenSearch. Para fluxos de trabalho de tradução práticos, o Amazon Translate fornece um serviço gerenciado; para modelos personalizados, o SageMaker JumpStart e o Bedrock permitem experimentar alternativas que podem ser avaliadas com BLEU.",
    "incorrect_explanations": {
      "A": "A eficiência energética requer telemetria de hardware/uso; o BLEU mede a sobreposição textual.",
      "C": "Velocidade/latência não é medida pelo BLEU; é uma métrica de desempenho do sistema.",
      "D": "O valor de mercado não é uma métrica técnica da qualidade da tradução."
    }
  },
  {
    "id": "aif-c01-ml_development-154",
    "certification_id": "AIF-C01",
    "domain": "ML_DEVELOPMENT",
    "difficulty": "medium",
    "tier": "FREE",
    "required_selection_count": 1,
    "active": true,
    "question_text": "Qual é o principal benefício de usar o aprendizado por transferência (transfer learning) para a personalização de modelos de fundação?",
    "option_a": "Não requer treinamento adicional",
    "option_b": "Permite que o modelo aproveite o conhecimento de um domínio para outro",
    "option_c": "Sempre produz resultados perfeitos",
    "option_d": "Reduz o tamanho do modelo a zero",
    "correct_answers": [
      "B"
    ],
    "explanation_detailed": "O aprendizado por transferência reutiliza representações aprendidas durante o pré-treinamento para acelerar a adaptação a novas tarefas/domínios com menos dados e custo. O modelo mantém o conhecimento geral e ajusta as camadas finais ou parâmetros específicos (por exemplo, LoRA) para estilo/terminologia. Na AWS, use o Amazon SageMaker para ajuste fino controlado e experimentos reprodutíveis; o JumpStart fornece modelos e notebooks prontos. Valide os ganhos com métricas específicas do negócio e avalie o viés/robustez com o SageMaker Clarify. Controle versões e políticas de acesso (Model Registry, IAM/KMS) e monitore o desvio pós-implantação com o Model Monitor.",
    "incorrect_explanations": {
      "A": "Mesmo com transferência, há treinamento/adaptação, embora menos do que do zero.",
      "C": "Não há garantia de perfeição; depende dos dados, da avaliação e da governança.",
      "D": "Não elimina o modelo; reduz o esforço de treinamento, não o tamanho a zero."
    }
  },
  {
    "id": "aif-c01-ai_fundamentals-155",
    "certification_id": "AIF-C01",
    "domain": "AI_FUNDAMENTALS",
    "difficulty": "medium",
    "tier": "FREE",
    "required_selection_count": 1,
    "active": true,
    "question_text": "Qual é o objetivo principal do espaço latente do modelo no contexto da engenharia de prompt?",
    "option_a": "Armazenar fisicamente o modelo",
    "option_b": "Representar o entendimento e o conhecimento interno do modelo",
    "option_c": "Aumentar a velocidade de processamento do modelo",
    "option_d": "Reduzir o consumo de energia do modelo",
    "correct_answers": [
      "B"
    ],
    "explanation_detailed": "O espaço latente é a representação interna onde significados, relações e padrões são codificados em vetores/ativações. Prompts eficazes 'navegam' neste espaço, condicionando a geração para regiões desejadas (estilo, tom, formato), sem acesso direto aos vetores. Em modelos de imagem (difusão), o espaço latente guia o conteúdo/estética; em LLMs, estrutura o próximo token com base no histórico. Na AWS, você manipula esse comportamento via Amazon Bedrock (parâmetros de inferência, few-shot) e integra a recuperação (Kendra/OpenSearch) para ancoragem factual. Métricas e testes A/B no SageMaker/CloudWatch confirmam se a orientação do prompt realmente move a saída para a região desejada do espaço latente.",
    "incorrect_explanations": {
      "A": "Não é um meio físico; é uma representação matemática nas ativações e pesos.",
      "C": "A velocidade depende do hardware e das otimizações; o espaço latente representa a semântica.",
      "D": "O consumo de energia não é uma função do espaço latente; é da carga/infraestrutura."
    }
  },
  {
    "id": "aif-c01-responsible_ai-156",
    "certification_id": "AIF-C01",
    "domain": "RESPONSIBLE_AI",
    "difficulty": "easy",
    "tier": "FREE",
    "required_selection_count": 1,
    "active": true,
    "question_text": "Qual das seguintes NÃO é uma característica da IA responsável?",
    "option_a": "Justiça (Fairness)",
    "option_b": "Robustez",
    "option_c": "Lucratividade",
    "option_d": "Inclusividade",
    "correct_answers": [
      "C"
    ],
    "explanation_detailed": "A IA responsável está ancorada na justiça, transparência, segurança, privacidade, robustez, governança e inclusão. 'Lucratividade' é um objetivo de negócio, não um princípio ético. Na AWS, combine Guardrails para Amazon Bedrock para filtros de conteúdo/PII, SageMaker Clarify para detectar vieses, Model Cards para documentação e A2I para revisão humana. Implemente auditoria (CloudTrail), controle de acesso (IAM/KMS) e monitoramento contínuo (Model Monitor). Tenha políticas de dados e avaliações de impacto. Foque nos efeitos distribucionais, não apenas em métricas agregadas.",
    "incorrect_explanations": {
      "A": "A justiça é central: tratamento equitativo entre grupos e contextos.",
      "B": "A robustez garante o desempenho sob variações e ataques.",
      "D": "A inclusão mitiga exclusões sistêmicas e erros em grupos minoritários."
    }
  },
  {
    "id": "aif-c01-responsible_ai-157",
    "certification_id": "AIF-C01",
    "domain": "RESPONSIBLE_AI",
    "difficulty": "medium",
    "tier": "FREE",
    "required_selection_count": 1,
    "active": true,
    "question_text": "Qual é o objetivo principal dos Guardrails para Amazon Bedrock?",
    "option_a": "Proteger fisicamente o hardware de IA",
    "option_b": "Identificar e aplicar recursos de IA responsável",
    "option_c": "Aumentar o desempenho do modelo",
    "option_d": "Reduzir o consumo de energia",
    "correct_answers": [
      "B"
    ],
    "explanation_detailed": "Os Guardrails para Amazon Bedrock aplicam políticas de uso responsável: filtros de PII, bloqueio de categorias sensíveis, limites de toxicidade, proteção contra injeção de prompt e conformidade de resposta. Ele opera na camada de aplicação, independente do modelo de fundação, com logs para auditoria e ajustes iterativos. Combine com IAM/KMS para segurança, CloudWatch para monitoramento e validações pós-processamento. Em fluxos RAG, envolva o Kendra/OpenSearch com proveniência e sanitização. Guardrails não é uma ferramenta de desempenho ou hardware; é uma camada de política de conteúdo e segurança para manter as saídas dentro dos padrões definidos pelo negócio.",
    "incorrect_explanations": {
      "A": "Não lida com datacenter/hardware; foca em conteúdo e política.",
      "C": "Pode evitar respostas ruins, mas não é um mecanismo de aceleração do modelo.",
      "D": "O consumo de energia depende da carga/infraestrutura; os guardrails não o reduzem diretamente."
    }
  },
  {
    "id": "aif-c01-responsible_ai-158",
    "certification_id": "AIF-C01",
    "domain": "RESPONSIBLE_AI",
    "difficulty": "medium",
    "tier": "FREE",
    "required_selection_count": 1,
    "active": true,
    "question_text": "Qual das seguintes é uma consideração chave na seleção de modelos responsáveis?",
    "option_a": "A popularidade do modelo",
    "option_b": "O impacto ambiental do modelo",
    "option_c": "O país de origem do modelo",
    "option_d": "O esquema de cores do modelo",
    "correct_answers": [
      "B"
    ],
    "explanation_detailed": "A seleção responsável pondera desempenho, segurança, viés, privacidade, riscos de IP e pegada ambiental (energia/CO₂). Na AWS, escolha modelos no Amazon Bedrock ou SageMaker JumpStart considerando a janela de contexto, suporte a ferramentas, qualidade, custo por token e requisitos regulatórios. Avalie o consumo e a escalabilidade por região (residência de dados) e adote as melhores práticas do pilar de Sustentabilidade do Well-Architected Framework. Documente as justificativas nos Model Cards e execute testes de justiça com o Clarify. 'Popularidade' e estética não substituem a devida diligência técnica e ética.",
    "incorrect_explanations": {
      "A": "A popularidade não indica adequação para o seu caso, riscos e custos.",
      "C": "A origem geográfica não determina qualidade/ética por si só; avalie as evidências.",
      "D": "As cores não têm relação com responsabilidade ou desempenho."
    }
  },
  {
    "id": "aif-c01-responsible_ai-159",
    "certification_id": "AIF-C01",
    "domain": "RESPONSIBLE_AI",
    "difficulty": "medium",
    "tier": "FREE",
    "required_selection_count": 1,
    "active": true,
    "question_text": "Qual é um risco legal potencial de trabalhar com IA generativa?",
    "option_a": "Lesões físicas aos usuários",
    "option_b": "Reivindicações de violação de propriedade intelectual",
    "option_c": "Aumento nas contas de eletricidade",
    "option_d": "Redução da velocidade da internet",
    "correct_answers": [
      "B"
    ],
    "explanation_detailed": "O conteúdo gerado pode se assemelhar a obras protegidas, usar marcas registradas ou incorporar materiais sensíveis. Mitigue com políticas de dados, filtros de conteúdo (Guardrails), verificações de similaridade, fontes autorizadas e proveniência em RAG (Kendra/OpenSearch). Documente usos, licenças e limitações nos Model Cards. Na AWS, restrinja o acesso com IAM/KMS, audite com o CloudTrail e monitore fluxos no CloudWatch. Estabeleça revisão humana (A2I) quando houver alto risco e defina termos de uso claros para clientes internos. Também aborde a privacidade/PII e a confidencialidade de prompts e saídas.",
    "incorrect_explanations": {
      "A": "A IA generativa opera digitalmente; o risco principal é legal/conformidade, não físico.",
      "C": "O custo de energia é operacional, não um risco legal direto.",
      "D": "A largura de banda não define a exposição legal de IP."
    }
  },
  {
    "id": "aif-c01-responsible_ai-160",
    "certification_id": "AIF-C01",
    "domain": "RESPONSIBLE_AI",
    "difficulty": "easy",
    "tier": "FREE",
    "required_selection_count": 1,
    "active": true,
    "question_text": "Qual das seguintes NÃO é uma característica de conjuntos de dados importantes para a IA responsável?",
    "option_a": "Inclusividade",
    "option_b": "Diversidade",
    "option_c": "Tamanho",
    "option_d": "Representação equilibrada",
    "correct_answers": [
      "C"
    ],
    "explanation_detailed": "O tamanho ajuda, mas não garante justiça. Conjuntos de dados responsáveis priorizam a diversidade de fontes, a representação equilibrada entre grupos e a documentação de proveniência e consentimento. Na AWS, gerencie dados no S3/Lake Formation, catalogue no Glue Data Catalog, aplique IAM/KMS e execute avaliações de viés com o SageMaker Clarify por subgrupos. Registre decisões e limitações nos Model Cards e mantenha processos de atualização para corrigir lacunas. Valide a qualidade e a cobertura dos rótulos, não apenas o volume.",
    "incorrect_explanations": {
      "A": "A inclusividade reduz lacunas de cobertura e erros sistêmicos.",
      "B": "A diversidade mitiga o viés de amostragem e melhora a generalização.",
      "D": "O balanceamento impede que o modelo favoreça classes/grupos dominantes."
    }
  },
  {
    "id": "aif-c01-ai_fundamentals-161",
    "certification_id": "AIF-C01",
    "domain": "AI_FUNDAMENTALS",
    "difficulty": "easy",
    "tier": "FREE",
    "required_selection_count": 1,
    "active": true,
    "question_text": "O que é sobreajuste (overfitting) no contexto de modelos de IA?",
    "option_a": "Quando um modelo tem um desempenho muito bom nos dados de treinamento, mas ruim em novos dados",
    "option_b": "Quando um modelo é grande demais para caber na memória",
    "option_c": "Quando um modelo gera saídas que são muito longas",
    "option_d": "Quando um modelo consome muita energia",
    "correct_answers": [
      "A"
    ],
    "explanation_detailed": "O sobreajuste ocorre quando o modelo aprende o ruído e as peculiaridades dos dados de treinamento, falhando em generalizar. Os sintomas incluem uma grande lacuna entre as métricas de treinamento e validação, e previsões instáveis. As mitigações incluem regularização, parada antecipada, aumento de dados, validação cruzada e dados mais representativos. Na AWS, use o SageMaker Experiments para rastrear métricas, o Debugger para detectar problemas de treinamento e o Model Monitor para observar desvios após a implantação. Documente as limitações nos Model Cards e avalie por subgrupos com o Clarify para entender onde a generalização é fraca.",
    "incorrect_explanations": {
      "B": "A limitação de memória é uma restrição de infraestrutura, não a definição de sobreajuste.",
      "C": "O comprimento da saída é uma escolha de inferência; não caracteriza o sobreajuste.",
      "D": "O consumo de energia não define a generalização do modelo."
    }
  },
  {
    "id": "aif-c01-ai_services-162",
    "certification_id": "AIF-C01",
    "domain": "AI_SERVICES",
    "difficulty": "medium",
    "tier": "FREE",
    "required_selection_count": 1,
    "active": true,
    "question_text": "Qual serviço da AWS é projetado para ajudar a detectar e monitorar o viés em modelos de aprendizado de máquina?",
    "option_a": "Amazon EC2",
    "option_b": "Amazon S3",
    "option_c": "Amazon SageMaker Clarify",
    "option_d": "Amazon RDS",
    "correct_answers": [
      "C"
    ],
    "explanation_detailed": "O SageMaker Clarify detecta viés em dados e modelos, gera relatórios por atributos (por exemplo, gênero, região), produz explicações (SHAP) e suporta o monitoramento em produção. Integre o Clarify em pipelines de ML no SageMaker para avaliar antes da implantação e use o Model Monitor para continuar verificando após a implantação. Combine com os Model Cards para registrar contexto, métricas e limitações, e com o A2I quando a decisão exigir revisão humana. EC2, S3 e RDS não fornecem essa análise de viés nativamente.",
    "incorrect_explanations": {
      "A": "O EC2 fornece computação, não uma ferramenta nativa de viés/explicabilidade.",
      "B": "O S3 é um armazenamento de objetos; não avalia o viés.",
      "D": "O RDS é um banco de dados relacional; não mede o viés do modelo."
    }
  },
  {
    "id": "aif-c01-responsible_ai-163",
    "certification_id": "AIF-C01",
    "domain": "RESPONSIBLE_AI",
    "difficulty": "easy",
    "tier": "FREE",
    "required_selection_count": 1,
    "active": true,
    "question_text": "Qual é a principal diferença entre modelos de IA transparentes e não transparentes?",
    "option_a": "Modelos transparentes são sempre mais precisos",
    "option_b": "Modelos transparentes permitem a compreensão de seu processo de tomada de decisão",
    "option_c": "Modelos transparentes são sempre menores em tamanho",
    "option_d": "Modelos transparentes consomem menos energia",
    "correct_answers": [
      "B"
    ],
    "explanation_detailed": "Transparência significa explicar como as entradas influenciam as saídas, quais atributos importam e sob quais condições o modelo falha. As técnicas incluem: modelos intrinsecamente interpretáveis (lineares, árvores), explicações locais (LIME/SHAP), relatórios (Model Cards) e documentação de dados. Na AWS, o Clarify gera explicações e detecção de viés; os Model Cards registram o contexto de uso; o CloudTrail/Config audita as alterações. A transparência não implica maior precisão ou menor custo; é um atributo de governança e confiança.",
    "incorrect_explanations": {
      "A": "A transparência não garante precisão superior; pode até haver um trade-off com a complexidade.",
      "C": "O tamanho não define a interpretabilidade; depende da estrutura e das técnicas de explicação.",
      "D": "O consumo de energia é uma questão de infraestrutura, não de interpretabilidade."
    }
  },
  {
    "id": "aif-c01-responsible_ai-164",
    "certification_id": "AIF-C01",
    "domain": "RESPONSIBLE_AI",
    "difficulty": "medium",
    "tier": "FREE",
    "required_selection_count": 1,
    "active": true,
    "question_text": "Qual ferramenta pode ser usada para documentar informações do modelo para transparência?",
    "option_a": "Amazon SageMaker Model Cards",
    "option_b": "Amazon EC2",
    "option_c": "Amazon S3",
    "option_d": "Amazon RDS",
    "correct_answers": [
      "A"
    ],
    "explanation_detailed": "Os SageMaker Model Cards consolidam o propósito, dados de treinamento, métricas, avaliações de viés/robustez, riscos e recomendações de uso, promovendo transparência e governança. Integre com o Clarify para anexar resultados de justiça/explicabilidade e com o Model Registry para versionamento e trilha de auditoria. Armazene artefatos no S3 e monitore em produção com o Model Monitor. EC2, S3 e RDS são úteis para infraestrutura, mas não substituem a documentação padronizada do ciclo de vida do modelo.",
    "incorrect_explanations": {
      "B": "O EC2 fornece computação; não é uma ferramenta de documentação de modelos.",
      "C": "O S3 armazena artefatos, mas não estrutura os model cards.",
      "D": "O RDS é um banco de dados; não fornece os model cards."
    }
  },
  {
    "id": "aif-c01-responsible_ai-165",
    "certification_id": "AIF-C01",
    "domain": "RESPONSIBLE_AI",
    "difficulty": "medium",
    "tier": "FREE",
    "required_selection_count": 1,
    "active": true,
    "question_text": "Qual é um possível trade-off entre a segurança e a transparência do modelo?",
    "option_a": "Modelos mais seguros são sempre menos transparentes",
    "option_b": "Modelos transparentes são sempre menos seguros",
    "option_c": "Maior transparência pode revelar vulnerabilidades",
    "option_d": "Não há trade-offs entre segurança e transparência",
    "correct_answers": [
      "C"
    ],
    "explanation_detailed": "Explicações detalhadas e documentação extensa podem expor limites, características sensíveis ou protocolos, o que facilita ataques (por exemplo, extração de modelo, evasão, injeção de prompt). Mitigue fornecendo explicações úteis ao usuário sem vazar sinais exploráveis, agregando informações e controlando o acesso. Na AWS, use IAM/KMS para segregar ambientes, CloudTrail para auditoria, Guardrails para políticas de conteúdo e testes adversariais contínuos. Transparência e segurança são compatíveis, mas exigem um design deliberado.",
    "incorrect_explanations": {
      "A": "Não é uma regra geral; bons designs alcançam segurança e transparência juntas.",
      "B": "A transparência não implica insegurança inevitável; depende do escopo e dos controles.",
      "D": "Existem trade-offs práticos; negar isso ignora o risco operacional."
    }
  },
  {
    "id": "aif-c01-responsible_ai-166",
    "certification_id": "AIF-C01",
    "domain": "RESPONSIBLE_AI",
    "difficulty": "easy",
    "tier": "FREE",
    "required_selection_count": 1,
    "active": true,
    "question_text": "O que é o design centrado no ser humano no contexto da IA explicável?",
    "option_a": "Projetar sistemas de IA que se pareçam com humanos",
    "option_b": "Criar sistemas de IA que priorizem as necessidades e a compreensão humanas",
    "option_c": "Usar humanos em vez de IA para todas as tarefas",
    "option_d": "Projetar sistemas de IA que só podem ser usados por humanos",
    "correct_answers": [
      "B"
    ],
    "explanation_detailed": "O design centrado no ser humano adapta interfaces, explicações e controles para decisões seguras e auditáveis. Explique em linguagem clara, mostre fontes (RAG), permita contestação e feedback e forneça confiança calibrada (pontuações, limiares). Na AWS, combine Bedrock + Kendra/OpenSearch para citabilidade, Clarify para importar explicações, A2I para revisão humana e Model Cards para orientar o uso. O objetivo é tornar a IA útil e compreensível, não antropomórfica.",
    "incorrect_explanations": {
      "A": "A aparência humana é irrelevante para a explicabilidade.",
      "C": "Não elimina a IA; equilibra a automação e a intervenção humana.",
      "D": "O uso não é exclusivo; trata-se de acessibilidade e compreensão."
    }
  },
  {
    "id": "aif-c01-responsible_ai-167",
    "certification_id": "AIF-C01",
    "domain": "RESPONSIBLE_AI",
    "difficulty": "easy",
    "tier": "FREE",
    "required_selection_count": 1,
    "active": true,
    "question_text": "Qual dos seguintes NÃO é um efeito típico do viés em sistemas de IA?",
    "option_a": "Tratamento injusto de certos grupos demográficos",
    "option_b": "Melhora da precisão geral",
    "option_c": "Potenciais questões legais",
    "option_d": "Perda de confiança do usuário",
    "correct_answers": [
      "B"
    ],
    "explanation_detailed": "O viés tende a reduzir a precisão em subgrupos, gerar injustiça, criar risco legal e erodir a confiança. Os comportamentos incluem falsos positivos/negativos desequilibrados e respostas inconsistentes entre as populações. Na AWS, use o Clarify para medir o viés, o Model Monitor para monitorar o desvio de distribuição e o A2I para revisão humana em decisões críticas. Mitigue com dados diversos, reamostragem, reponderação e restrições de justiça, registrando os resultados nos Model Cards.",
    "incorrect_explanations": {
      "A": "O tratamento desigual é uma consequência típica quando os dados são enviesados.",
      "C": "As falhas de justiça expõem a litígios e escrutínio regulatório.",
      "D": "Os usuários perdem a confiança quando percebem resultados injustos/inconsistentes."
    }
  },
  {
    "id": "aif-c01-responsible_ai-168",
    "certification_id": "AIF-C01",
    "domain": "RESPONSIBLE_AI",
    "difficulty": "medium",
    "tier": "FREE",
    "required_selection_count": 1,
    "active": true,
    "question_text": "Qual é o objetivo principal da análise de subgrupos na IA responsável?",
    "option_a": "Dividir a equipe de desenvolvimento em subgrupos",
    "option_b": "Analisar o desempenho do modelo em diferentes grupos demográficos",
    "option_c": "Reduzir o tamanho do modelo",
    "option_d": "Aumentar a velocidade de processamento do modelo",
    "correct_answers": [
      "B"
    ],
    "explanation_detailed": "A análise de subgrupos mede o desempenho por grupos (por exemplo, idade, região, idioma) para detectar assimetrias e injustiças. Compare métricas (precisão/recall, MAE) por grupo, aplique testes estatísticos e defina metas mínimas de qualidade. Na AWS, use o Clarify para relatórios por atributo e o Model Monitor para observar desvios ao longo do tempo. Registre as limitações nos Model Cards e implemente ações corretivas (recoleta de dados, reponderação, novas características).",
    "incorrect_explanations": {
      "A": "Não se trata de organogramas; é uma avaliação estatística por população.",
      "C": "O tamanho do modelo por si só não resolve os desequilíbrios por grupos.",
      "D": "A velocidade não evidencia justiça; foque nas métricas por subgrupo."
    }
  },
  {
    "id": "aif-c01-responsible_ai-169",
    "certification_id": "AIF-C01",
    "domain": "RESPONSIBLE_AI",
    "difficulty": "easy",
    "tier": "FREE",
    "required_selection_count": 1,
    "active": true,
    "question_text": "Qual das seguintes é uma consideração chave para a diversidade de conjuntos de dados na IA responsável?",
    "option_a": "Usar dados de apenas uma fonte",
    "option_b": "Garantir a representação de vários grupos demográficos",
    "option_c": "Usar o maior conjunto de dados disponível, independentemente do conteúdo",
    "option_d": "Usar apenas os dados mais recentes",
    "correct_answers": [
      "B"
    ],
    "explanation_detailed": "A diversidade de dados reduz o viés de amostragem e melhora a generalização. Busque origens, cenários e idiomas/dialetos variados, com documentação de proveniência, consentimento e políticas de retenção. Na AWS, catalogue com o Glue Data Catalog, controle o acesso com o Lake Formation/IAM e use o Clarify para medir a cobertura por grupo. Priorize a qualidade e a representatividade, não apenas o volume ou a recenticidade.",
    "incorrect_explanations": {
      "A": "Uma única fonte aumenta o risco de viés e baixa cobertura.",
      "C": "Maior volume sem curadoria não garante justiça.",
      "D": "A recenticidade ajuda, mas não substitui a diversidade e a cobertura."
    }
  },
  {
    "id": "aif-c01-responsible_ai-170",
    "certification_id": "AIF-C01",
    "domain": "RESPONSIBLE_AI",
    "difficulty": "easy",
    "tier": "FREE",
    "required_selection_count": 1,
    "active": true,
    "question_text": "O que é veracidade no contexto da IA responsável?",
    "option_a": "A velocidade com que o sistema de IA opera",
    "option_b": "A veracidade e precisão das saídas do sistema de IA",
    "option_c": "O tamanho do modelo de IA",
    "option_d": "O custo de operação do sistema de IA",
    "correct_answers": [
      "B"
    ],
    "explanation_detailed": "A veracidade requer respostas corretas, verificáveis e consistentes. Em soluções RAG, inclua fontes citáveis (Kendra/OpenSearch) no prompt para aumentar a fidelidade factual. Aplique guardrails contra informações proibidas e processos de validação humana (A2I) quando necessário. Em produção, rastreie a precisão, as taxas de correção e os relatórios de erros no CloudWatch e mantenha os Model Cards descrevendo os limites de uso e a confiabilidade.",
    "incorrect_explanations": {
      "A": "A velocidade é latência; não mede a verdade factual.",
      "C": "O tamanho não implica maior veracidade.",
      "D": "O custo é operacional; a veracidade é qualidade informacional."
    }
  },
  {
    "id": "aif-c01-responsible_ai-171",
    "certification_id": "AIF-C01",
    "domain": "RESPONSIBLE_AI",
    "difficulty": "medium",
    "tier": "FREE",
    "required_selection_count": 1,
    "active": true,
    "question_text": "Qual dos seguintes NÃO é um método típico para melhorar a interpretabilidade do modelo?",
    "option_a": "Usar modelos mais simples",
    "option_b": "Fornecer classificações de importância de características",
    "option_c": "Aumentar o tamanho do modelo",
    "option_d": "Gerar explicações legíveis por humanos",
    "correct_answers": [
      "C"
    ],
    "explanation_detailed": "A interpretabilidade melhora com estruturas mais simples (árvores, regressões), explicações locais (LIME/SHAP) e relatórios claros (Model Cards). Aumentar o tamanho tende a reduzir a transparência. Na AWS, o Clarify gera importâncias/SHAP e os Model Cards documentam o propósito, os dados e os limites. Em aplicações de alto risco, combine a revisão humana (A2I) com explicações e regras para decisões auditáveis.",
    "incorrect_explanations": {
      "A": "Modelos simples facilitam a compreensão e a auditoria.",
      "B": "As importâncias ajudam a ver os direcionadores da decisão.",
      "D": "Explicações legíveis conectam o técnico ao usuário final."
    }
  },
  {
    "id": "aif-c01-ai_services-172",
    "certification_id": "AIF-C01",
    "domain": "AI_SERVICES",
    "difficulty": "easy",
    "tier": "FREE",
    "required_selection_count": 1,
    "active": true,
    "question_text": "Qual é o objetivo principal do Amazon Augmented AI (A2I) na IA responsável?",
    "option_a": "Substituir trabalhadores humanos por IA",
    "option_b": "Facilitar a revisão humana de previsões de IA",
    "option_c": "Aumentar o tamanho do modelo de IA",
    "option_d": "Reduzir o consumo de energia dos sistemas de IA",
    "correct_answers": [
      "B"
    ],
    "explanation_detailed": "O Amazon A2I insere humanos no ciclo para revisar previsões de IA em cenários de baixa confiança, auditar amostragens ou exceções. Ele se integra com endpoints do SageMaker, Model Monitor e fluxos de rotulagem. Defina políticas de roteamento (limiares), colete feedback e use-o para corrigir dados, refinar prompts ou ajustar modelos. Em aplicações regulamentadas, o A2I ajuda a demonstrar governança e controle humano eficaz.",
    "incorrect_explanations": {
      "A": "O objetivo não é substituir humanos; é supervisionar e validar a IA.",
      "C": "O tamanho do modelo não é afetado pelo A2I.",
      "D": "A energia não é o foco; é a qualidade e a conformidade."
    }
  },
  {
    "id": "aif-c01-responsible_ai-173",
    "certification_id": "AIF-C01",
    "domain": "RESPONSIBLE_AI",
    "difficulty": "medium",
    "tier": "FREE",
    "required_selection_count": 1,
    "active": true,
    "question_text": "Qual das seguintes é uma consideração chave ao avaliar a justiça de um sistema de IA?",
    "option_a": "A velocidade de processamento do sistema",
    "option_b": "O consumo de energia do sistema",
    "option_c": "O impacto do sistema em diferentes grupos demográficos",
    "option_d": "A popularidade do sistema entre os usuários",
    "correct_answers": [
      "C"
    ],
    "explanation_detailed": "A justiça requer a medição do desempenho por subgrupos, a investigação de disparidades, a compreensão das causas (dados/algoritmo) e a aplicação de mitigação. Na AWS, use o Clarify para relatórios por atributo, o Model Monitor para vigilância contínua e os Model Cards para documentar os limites. Conecte as métricas técnicas às consequências reais (erros custosos para grupos específicos) e adote políticas de revisão humana quando a incerteza for alta.",
    "incorrect_explanations": {
      "A": "A latência não mede o tratamento equitativo.",
      "B": "A energia é sustentabilidade, não justiça.",
      "D": "A popularidade não revela viés ou impacto desigual."
    }
  },
  {
    "id": "aif-c01-ai_fundamentals-174",
    "certification_id": "AIF-C01",
    "domain": "AI_FUNDAMENTALS",
    "difficulty": "easy",
    "tier": "FREE",
    "required_selection_count": 1,
    "active": true,
    "question_text": "O que é subajuste (underfitting) no contexto de modelos de IA?",
    "option_a": "Quando um modelo é pequeno demais para caber na memória",
    "option_b": "Quando um modelo tem um desempenho ruim tanto nos dados de treinamento quanto em novos dados",
    "option_c": "Quando um modelo gera saídas que são muito curtas",
    "option_d": "Quando um modelo consome pouca energia",
    "correct_answers": [
      "B"
    ],
    "explanation_detailed": "O subajuste ocorre quando o modelo é simples demais para capturar padrões, resultando em alto erro tanto no treinamento quanto na validação. Mitigue com modelos mais expressivos, melhores características e mais épocas. Na AWS, use o SageMaker Experiments para comparar arquiteturas/hiperparâmetros e o Debugger para identificar gargalos. Monitore as métricas no CloudWatch e registre os resultados e limitações nos Model Cards.",
    "incorrect_explanations": {
      "A": "Memória insuficiente não define subajuste; é uma limitação operacional.",
      "C": "O comprimento da saída não indica falta de capacidade de modelagem.",
      "D": "O baixo consumo de energia não caracteriza o subajuste."
    }
  },
  {
    "id": "aif-c01-responsible_ai-175",
    "certification_id": "AIF-C01",
    "domain": "RESPONSIBLE_AI",
    "difficulty": "medium",
    "tier": "FREE",
    "required_selection_count": 1,
    "active": true,
    "question_text": "Qual dos seguintes NÃO é um benefício típico do uso de modelos de código aberto para transparência?",
    "option_a": "Capacidade de inspecionar o código do modelo",
    "option_b": "Melhorias impulsionadas pela comunidade",
    "option_c": "Desempenho perfeito garantido",
    "option_d": "Potencial para auditorias independentes",
    "correct_answers": [
      "C"
    ],
    "explanation_detailed": "Modelos de código aberto permitem inspeção, auditorias externas, reprodutibilidade e personalização, o que ajuda na transparência e confiança. Eles não garantem precisão perfeita. Na AWS, hospede o treinamento no SageMaker/EC2, controle o acesso via IAM, registre experimentos e métricas e crie Model Cards. Avalie licenças e riscos de IP e aplique o Clarify para medições de viés e explicações, independentemente de serem abertos ou proprietários.",
    "incorrect_explanations": {
      "A": "O código aberto permite inspeção e revisões detalhadas.",
      "B": "A comunidade pode descobrir bugs e melhorar o desempenho.",
      "D": "O código disponível facilita auditorias independentes e governança."
    }
  },
  {
    "id": "aif-c01-ml_development-176",
    "certification_id": "AIF-C01",
    "domain": "ML_DEVELOPMENT",
    "difficulty": "medium",
    "tier": "FREE",
    "required_selection_count": 1,
    "active": true,
    "question_text": "Qual é o objetivo principal da análise da qualidade dos rótulos na IA responsável?",
    "option_a": "Melhorar a aparência visual dos rótulos",
    "option_b": "Garantir a precisão e consistência dos rótulos de dados",
    "option_c": "Reduzir o número de rótulos usados",
    "option_d": "Aumentar a velocidade de processamento do modelo",
    "correct_answers": [
      "B"
    ],
    "explanation_detailed": "Rótulos de baixa qualidade geram ruído e viés. Avalie o acordo entre anotadores, distribuições por classe e exemplos ambíguos. Na AWS, use o SageMaker Ground Truth para rotulagem com controle de qualidade, o A2I para revisão humana e o Clarify para verificar o impacto dos rótulos nas métricas por subgrupo. Monitore o desvio dos rótulos após a implantação com o Model Monitor e atualize os conjuntos de dados à medida que novos padrões surgem.",
    "incorrect_explanations": {
      "A": "A estética é irrelevante; precisão/consistência importam.",
      "C": "Menos rótulos não resolve o problema; qualidade e cobertura são críticas.",
      "D": "A velocidade vem da infraestrutura; a qualidade dos rótulos afeta a precisão."
    }
  },
  {
    "id": "aif-c01-responsible_ai-177",
    "certification_id": "AIF-C01",
    "domain": "RESPONSIBLE_AI",
    "difficulty": "easy",
    "tier": "FREE",
    "required_selection_count": 1,
    "active": true,
    "question_text": "Qual das seguintes é uma consequência potencial do uso de conjuntos de dados enviesados no treinamento de IA?",
    "option_a": "Melhora do desempenho do modelo para todos os grupos",
    "option_b": "Resultados injustos ou discriminatórios para certos grupos",
    "option_c": "Redução do consumo de energia",
    "option_d": "Tempos de treinamento do modelo mais rápidos",
    "correct_answers": [
      "B"
    ],
    "explanation_detailed": "Dados enviesados levam o modelo a replicar assimetrias, produzindo decisões injustas para grupos sub-representados. Na AWS, use o Clarify para medir o viés e simular impactos, o A2I para revisão humana de casos críticos e os Model Cards para registrar riscos. Atualize os conjuntos de dados com fontes diversas e realize reponderação/reamostragem. Monitore continuamente as métricas por subgrupo em produção com o Model Monitor.",
    "incorrect_explanations": {
      "A": "O desempenho pode piorar para segmentos inteiros, não melhorar para todos.",
      "C": "A energia não é uma consequência direta do viés dos dados.",
      "D": "O tempo de treinamento não é determinado pela justiça dos dados."
    }
  },
  {
    "id": "aif-c01-responsible_ai-178",
    "certification_id": "AIF-C01",
    "domain": "RESPONSIBLE_AI",
    "difficulty": "medium",
    "tier": "FREE",
    "required_selection_count": 1,
    "active": true,
    "question_text": "Qual é o objetivo principal das práticas responsáveis na seleção de modelos?",
    "option_a": "Sempre escolher o maior modelo disponível",
    "option_b": "Selecionar modelos baseados apenas em métricas de desempenho",
    "option_c": "Equilibrar desempenho com considerações éticas e sustentabilidade",
    "option_d": "Escolher o modelo mais caro",
    "correct_answers": [
      "C"
    ],
    "explanation_detailed": "A seleção responsável pondera precisão, custo, privacidade, viés, robustez e pegada ambiental. Na AWS, compare FMs do Bedrock e opções no SageMaker JumpStart, avaliando a janela de contexto, ferramentas, segurança, custo por token e requisitos legais. Documente as decisões nos Model Cards e teste a justiça com o Clarify. Operacionalize a governança com o CloudTrail/Config e revisões periódicas.",
    "incorrect_explanations": {
      "A": "Maior nem sempre é melhor; custo/latência podem torná-lo inviável.",
      "B": "O desempenho por si só ignora riscos e conformidade.",
      "D": "O preço não é um indicador de adequação ou responsabilidade."
    }
  },
  {
    "id": "aif-c01-responsible_ai-179",
    "certification_id": "AIF-C01",
    "domain": "RESPONSIBLE_AI",
    "difficulty": "medium",
    "tier": "FREE",
    "required_selection_count": 1,
    "active": true,
    "question_text": "Qual das seguintes NÃO é uma característica típica de uma fonte de dados curada para IA responsável?",
    "option_a": "Precisão verificada",
    "option_b": "Proveniência conhecida",
    "option_c": "Maior tamanho possível",
    "option_d": "Métodos de coleta éticos",
    "correct_answers": [
      "C"
    ],
    "explanation_detailed": "A curadoria prioriza qualidade, consentimento, licenças, rastreabilidade e representatividade. O tamanho por si só não garante justiça ou veracidade. Na AWS, armazene no S3 com políticas do Lake Formation, catalogue no Glue, proteja com KMS/IAM e documente em dicionários de dados. Audite os fluxos com o CloudTrail e avalie o viés/impactos com o Clarify. Prefira menos dados de alta qualidade a mais dados não governados.",
    "incorrect_explanations": {
      "A": "A verificação da precisão é um requisito para a confiabilidade.",
      "B": "A proveniência clara permite auditoria e conformidade.",
      "D": "A coleta ética reduz os riscos legais e reputacionais."
    }
  },
  {
    "id": "aif-c01-responsible_ai-180",
    "certification_id": "AIF-C01",
    "domain": "RESPONSIBLE_AI",
    "difficulty": "medium",
    "tier": "FREE",
    "required_selection_count": 1,
    "active": true,
    "question_text": "Qual é o objetivo principal das auditorias humanas em sistemas de IA responsáveis?",
    "option_a": "Substituir sistemas de IA por trabalhadores humanos",
    "option_b": "Verificar e validar as saídas e processos do sistema de IA",
    "option_c": "Aumentar a velocidade de processamento do sistema de IA",
    "option_d": "Reduzir o consumo de energia do sistema de IA",
    "correct_answers": [
      "B"
    ],
    "explanation_detailed": "As auditorias humanas revisam processos, dados, modelos e saídas para verificar conformidade, ética e desempenho no mundo real. Elas incluem amostragem de casos, replicação de resultados, verificação de logs e validação de políticas. Na AWS, extraia trilhas com o CloudTrail, métricas no CloudWatch, relatórios do Clarify e documentação dos Model Cards; colete revisões via A2I. Auditorias independentes aumentam a confiança e atendem aos requisitos regulatórios.",
    "incorrect_explanations": {
      "A": "O objetivo é supervisionar e validar, não substituir toda a automação.",
      "C": "A velocidade não é o alvo; é a qualidade e a conformidade.",
      "D": "A energia não é o foco de uma auditoria de IA per se."
    }
  },
  {
    "id": "aif-c01-responsible_ai-181",
    "certification_id": "AIF-C01",
    "domain": "RESPONSIBLE_AI",
    "difficulty": "easy",
    "tier": "FREE",
    "required_selection_count": 1,
    "active": true,
    "question_text": "Qual serviço da AWS é usado principalmente para gerenciar acesso e permissões para sistemas de IA?",
    "option_a": "Amazon S3",
    "option_b": "AWS IAM",
    "option_c": "Amazon EC2",
    "option_d": "Amazon RDS",
    "correct_answers": [
      "B"
    ],
    "explanation_detailed": "O AWS Identity and Access Management (IAM) define usuários, funções, políticas e permissões granulares para recursos (S3, SageMaker, Bedrock, KMS). É a base da segurança 'na' nuvem. Combine com o KMS para chaves, o CloudTrail para auditoria e SCPs/Organizations para controle em escala. Controle o acesso a dados de treinamento, endpoints e artefatos de modelo. S3/EC2/RDS são serviços de dados/computação, não o mecanismo central de autorização.",
    "incorrect_explanations": {
      "A": "O S3 armazena objetos; as permissões são governadas via IAM/políticas de bucket.",
      "C": "O EC2 é computação; não gerencia identidades/permissões.",
      "D": "O RDS é um banco de dados relacional; o acesso é mediado por IAM/credenciais."
    }
  },
  {
    "id": "aif-c01-responsible_ai-182",
    "certification_id": "AIF-C01",
    "domain": "RESPONSIBLE_AI",
    "difficulty": "medium",
    "tier": "FREE",
    "required_selection_count": 1,
    "active": true,
    "question_text": "Qual é o objetivo principal do Amazon Macie na segurança de IA?",
    "option_a": "Gerar modelos de IA",
    "option_b": "Descobrir e proteger dados sensíveis",
    "option_c": "Aumentar o desempenho do modelo",
    "option_d": "Reduzir o consumo de energia",
    "correct_answers": [
      "B"
    ],
    "explanation_detailed": "O Amazon Macie identifica automaticamente PII e dados sensíveis em buckets do S3 usando ML e correspondência de padrões, sinalizando exposições e alterações de política. Integre com o CloudWatch/CloudTrail para alertas, aplique KMS/IAM/Lake Formation para remediação e registre as soluções. Em pipelines de IA, o Macie ajuda a garantir que os dados usados no treinamento/inferência sejam adequadamente protegidos.",
    "incorrect_explanations": {
      "A": "O Macie não treina modelos para você; foca na descoberta/proteção de dados.",
      "C": "O desempenho do modelo não está no escopo do Macie.",
      "D": "A energia não é o alvo; é a segurança dos dados."
    }
  },
  {
    "id": "aif-c01-responsible_ai-183",
    "certification_id": "AIF-C01",
    "domain": "RESPONSIBLE_AI",
    "difficulty": "easy",
    "tier": "FREE",
    "required_selection_count": 1,
    "active": true,
    "question_text": "A que se refere o Modelo de Responsabilidade Compartilhada da AWS?",
    "option_a": "Compartilhamento de modelos de IA entre clientes",
    "option_b": "Divisão de responsabilidades de segurança entre a AWS e o cliente",
    "option_c": "Compartilhamento de custos entre a AWS e o cliente",
    "option_d": "Divisão de tarefas de IA entre humanos e máquinas",
    "correct_answers": [
      "B"
    ],
    "explanation_detailed": "A AWS protege a infraestrutura global ('da' nuvem); os clientes protegem dados, configurações, identidades e aplicações ('na' nuvem). Para IA, os clientes definem políticas de dados, chaves (KMS), acesso (IAM), guardrails e auditoria (CloudTrail). A AWS oferece serviços seguros, mas a configuração e operação corretas são responsabilidade do cliente. Alinhe as responsabilidades na documentação e nos Model Cards quando os modelos expõem riscos específicos.",
    "incorrect_explanations": {
      "A": "Não trata da troca de modelos entre clientes.",
      "C": "O custeio não define segurança e governança.",
      "D": "Humano vs. máquina é design de produto, não o modelo de responsabilidade."
    }
  },
  {
    "id": "aif-c01-responsible_ai-184",
    "certification_id": "AIF-C01",
    "domain": "RESPONSIBLE_AI",
    "difficulty": "easy",
    "tier": "FREE",
    "required_selection_count": 1,
    "active": true,
    "question_text": "Qual dos seguintes NÃO é um método típico para proteger sistemas de IA?",
    "option_a": "Criptografia",
    "option_b": "Controle de acesso",
    "option_c": "Compartilhamento público de dados",
    "option_d": "Gerenciamento de vulnerabilidades",
    "correct_answers": [
      "C"
    ],
    "explanation_detailed": "A proteção inclui criptografia (KMS), controle de acesso (IAM/Lake Formation), varredura de vulnerabilidades (Inspector), monitoramento (GuardDuty/CloudTrail) e segmentação de rede (VPC/WAF). O compartilhamento público de dados amplifica o risco. Em IA, aplique guardrails no Bedrock, sanitização em RAG e revisão humana (A2I) para decisões críticas. Mantenha uma trilha de auditoria e respostas a incidentes documentadas.",
    "incorrect_explanations": {
      "A": "A criptografia é uma base para a confidencialidade e conformidade.",
      "B": "O acesso de menor privilégio reduz as superfícies de ataque.",
      "D": "Vulnerabilidades não corrigidas expõem dados e pipelines."
    }
  },
  {
    "id": "aif-c01-responsible_ai-185",
    "certification_id": "AIF-C01",
    "domain": "RESPONSIBLE_AI",
    "difficulty": "medium",
    "tier": "FREE",
    "required_selection_count": 1,
    "active": true,
    "question_text": "O que é linhagem de dados no contexto da segurança de IA?",
    "option_a": "Um método de criptografia de dados",
    "option_b": "Rastrear a origem e as transformações dos dados",
    "option_c": "Um tipo de arquitetura de modelo de IA",
    "option_d": "Uma forma de aumentar a velocidade de processamento de dados",
    "correct_answers": [
      "B"
    ],
    "explanation_detailed": "A linhagem de dados mapeia de onde os dados vieram, quais transformações sofreram e quem os acessou/alterou. É essencial para auditoria, reprodutibilidade e investigação de incidentes. Na AWS, catalogue no Glue Data Catalog, governe com o Lake Formation, registre trabalhos no Glue/Athena e use o CloudTrail/CloudWatch para trilhas de acesso e execução. Para IA, vincule conjuntos de dados/versões no SageMaker Model Registry e documente nos Model Cards.",
    "incorrect_explanations": {
      "A": "A criptografia protege; a linhagem explica a trajetória e o uso.",
      "C": "A arquitetura do modelo não é rastreamento de dados.",
      "D": "A velocidade é uma questão de desempenho, não de linhagem."
    }
  },
  {
    "id": "aif-c01-responsible_ai-186",
    "certification_id": "AIF-C01",
    "domain": "RESPONSIBLE_AI",
    "difficulty": "medium",
    "tier": "FREE",
    "required_selection_count": 1,
    "active": true,
    "question_text": "Qual serviço da AWS é usado para detectar ameaças de segurança em sistemas de IA?",
    "option_a": "Amazon Macie",
    "option_b": "Amazon S3",
    "option_c": "Amazon EC2",
    "option_d": "Amazon RDS",
    "correct_answers": [
      "A"
    ],
    "explanation_detailed": "O Macie encontra PII e dados sensíveis expostos no S3 e alerta sobre configurações arriscadas, o que ajuda a prevenir vazamentos e abusos em pipelines de IA. Combine com o CloudWatch/CloudTrail para alertas e com o Lake Formation/IAM/KMS para remediação. Para detecção de ameaças em contas e tráfego, é complementado pelo GuardDuty; para vulnerabilidades em cargas de trabalho, use o Inspector. O foco aqui é proteger os dados sensíveis usados pela IA.",
    "incorrect_explanations": {
      "B": "O S3 armazena; não detecta ameaças por si só.",
      "C": "O EC2 é computação, não um serviço de detecção de ameaças.",
      "D": "O RDS é um banco de dados relacional; não realiza varreduras de segurança."
    }
  },
  {
    "id": "aif-c01-responsible_ai-187",
    "certification_id": "AIF-C01",
    "domain": "RESPONSIBLE_AI",
    "difficulty": "medium",
    "tier": "FREE",
    "required_selection_count": 1,
    "active": true,
    "question_text": "O que é injeção de prompt no contexto da segurança de IA?",
    "option_a": "Um método de otimização de prompts",
    "option_b": "Uma vulnerabilidade de segurança onde uma entrada maliciosa manipula o comportamento da IA",
    "option_c": "Uma técnica para acelerar o processamento da IA",
    "option_d": "Uma forma de reduzir o consumo de energia da IA",
    "correct_answers": [
      "B"
    ],
    "explanation_detailed": "A injeção de prompt engana o modelo para que ele ignore as instruções originais e siga comandos maliciosos presentes na entrada ou em documentos recuperados (RAG). Mitigue com delimitação clara de funções, sanitização de conteúdo, listas de permissão/negação, validações pós-geração e Guardrails para Amazon Bedrock para filtrar instruções/PII proibidas. Monitore o uso e os incidentes via CloudTrail/CloudWatch e restrinja a fontes confiáveis com o Kendra/OpenSearch com metadados.",
    "incorrect_explanations": {
      "A": "Otimizar um prompt visa a qualidade; a injeção é um ataque.",
      "C": "Não acelera; altera o comportamento para fins maliciosos.",
      "D": "A energia não é diretamente afetada; é um risco de integridade."
    }
  },
  {
    "id": "aif-c01-responsible_ai-188",
    "certification_id": "AIF-C01",
    "domain": "RESPONSIBLE_AI",
    "difficulty": "easy",
    "tier": "FREE",
    "required_selection_count": 1,
    "active": true,
    "question_text": "Qual dos seguintes NÃO é um padrão típico de conformidade regulatória para sistemas de IA?",
    "option_a": "ISO",
    "option_b": "SOC",
    "option_c": "HTML",
    "option_d": "Leis de responsabilidade de algoritmos",
    "correct_answers": [
      "C"
    ],
    "explanation_detailed": "A conformidade envolve padrões como ISO/IEC (por exemplo, 27001), relatórios SOC e legislação de responsabilidade algorítmica. HTML é uma linguagem de marcação, não um padrão de conformidade. Na AWS, acesse relatórios no AWS Artifact, registre auditorias com o CloudTrail, monitore configurações com o Config e colete evidências com o Audit Manager. Mantenha os Model Cards, políticas e trilhas para demonstrar conformidade contínua.",
    "incorrect_explanations": {
      "A": "A ISO abrange práticas de segurança/gerenciamento; é relevante.",
      "B": "O SOC avalia controles organizacionais; é um padrão aceito.",
      "D": "Regulamentações algorítmicas são cada vez mais exigidas."
    }
  },
  {
    "id": "aif-c01-responsible_ai-189",
    "certification_id": "AIF-C01",
    "domain": "RESPONSIBLE_AI",
    "difficulty": "easy",
    "tier": "FREE",
    "required_selection_count": 1,
    "active": true,
    "question_text": "Qual serviço da AWS é usado para monitoramento e avaliação contínua de recursos?",
    "option_a": "Amazon EC2",
    "option_b": "AWS Config",
    "option_c": "Amazon S3",
    "option_d": "Amazon RDS",
    "correct_answers": [
      "B"
    ],
    "explanation_detailed": "O AWS Config rastreia as configurações dos recursos e as compara com regras/políticas, gerando um histórico e notificações de desvios. É útil para a governança de pipelines de IA (S3, SageMaker, KMS, IAM), garantindo que os ambientes permaneçam dentro dos padrões definidos. Integre com o CloudWatch Events/Rules para automações e com o CloudTrail para auditorias completas.",
    "incorrect_explanations": {
      "A": "O EC2 executa cargas de trabalho; o monitoramento de conformidade é feito pelo Config.",
      "C": "O S3 armazena; não avalia a conformidade dos recursos.",
      "D": "O RDS é um banco de dados; não audita configurações globais."
    }
  },
  {
    "id": "aif-c01-responsible_ai-190",
    "certification_id": "AIF-C01",
    "domain": "RESPONSIBLE_AI",
    "difficulty": "easy",
    "tier": "FREE",
    "required_selection_count": 1,
    "active": true,
    "question_text": "Qual é o objetivo principal do AWS Artifact na governança de IA?",
    "option_a": "Gerar modelos de IA",
    "option_b": "Fornecer acesso a relatórios de conformidade da AWS",
    "option_c": "Aumentar o desempenho do modelo",
    "option_d": "Reduzir o consumo de energia",
    "correct_answers": [
      "B"
    ],
    "explanation_detailed": "O AWS Artifact fornece relatórios de conformidade (ISO, SOC, PCI, etc.) e acordos para apoiar auditorias e devida diligência. Combine com o Audit Manager para coletar evidências de controles internos, o CloudTrail para trilhas de API e o Config para a postura dos recursos. Use esses artefatos para demonstrar a governança de soluções de IA que dependem da infraestrutura da AWS.",
    "incorrect_explanations": {
      "A": "O Artifact não treina modelos; é um repositório de conformidade.",
      "C": "O desempenho do modelo não é o objetivo do Artifact.",
      "D": "A energia não é o foco; é a conformidade."
    }
  },
  {
    "id": "aif-c01-responsible_ai-191",
    "certification_id": "AIF-C01",
    "domain": "RESPONSIBLE_AI",
    "difficulty": "easy",
    "tier": "FREE",
    "required_selection_count": 1,
    "active": true,
    "question_text": "Qual dos seguintes NÃO faz normalmente parte de uma estratégia de governança de dados?",
    "option_a": "Gerenciamento do ciclo de vida dos dados",
    "option_b": "Políticas de retenção de dados",
    "option_c": "Políticas de compartilhamento público de dados",
    "option_d": "Monitoramento de dados",
    "correct_answers": [
      "C"
    ],
    "explanation_detailed": "A governança de dados define funções, políticas de acesso, retenção, qualidade e monitoramento, priorizando a segurança e a conformidade. O compartilhamento público indiscriminado contradiz os princípios de proteção. Na AWS, implemente Lake Formation/IAM/KMS, catálogos no Glue e auditoria via CloudTrail/Config. Documente processos e mantenha evidências com o Audit Manager.",
    "incorrect_explanations": {
      "A": "O ciclo de vida (criação, uso, retenção, exclusão) é central.",
      "B": "A retenção atende a leis e políticas e minimiza o risco.",
      "D": "O monitoramento detecta desvios e incidentes."
    }
  },
  {
    "id": "aif-c01-responsible_ai-192",
    "certification_id": "AIF-C01",
    "domain": "RESPONSIBLE_AI",
    "difficulty": "easy",
    "tier": "FREE",
    "required_selection_count": 1,
    "active": true,
    "question_text": "Qual é o objetivo principal do AWS CloudTrail na governança de IA?",
    "option_a": "Gerar modelos de IA",
    "option_b": "Registrar chamadas de API e atividade da conta",
    "option_c": "Aumentar o desempenho do modelo",
    "option_d": "Reduzir o consumo de energia",
    "correct_answers": [
      "B"
    ],
    "explanation_detailed": "O CloudTrail registra quem fez o quê, quando e de onde, criando uma trilha de auditoria para recursos (S3, SageMaker, Bedrock, IAM). É essencial para investigações, conformidade e controles de mudança. Integre com o CloudWatch Logs/Events para alertas e respostas automatizadas. Para governança, combine com o Config, Audit Manager e Model Cards para uma visão completa do ciclo de vida do modelo.",
    "incorrect_explanations": {
      "A": "Não treina modelos; fornece auditoria de API.",
      "C": "O desempenho não é o foco do CloudTrail.",
      "D": "O consumo de energia não é monitorado pelo CloudTrail."
    }
  },
  {
    "id": "aif-c01-responsible_ai-193",
    "certification_id": "AIF-C01",
    "domain": "RESPONSIBLE_AI",
    "difficulty": "medium",
    "tier": "FREE",
    "required_selection_count": 1,
    "active": true,
    "question_text": "Qual das seguintes é uma consideração chave na engenharia de dados segura para IA?",
    "option_a": "Maximizar a coleta de dados sem considerar a qualidade",
    "option_b": "Implementar tecnologias que aprimoram a privacidade",
    "option_c": "Tornar todos os dados publicamente acessíveis",
    "option_d": "Usar apenas armazenamento de dados não criptografado",
    "correct_answers": [
      "B"
    ],
    "explanation_detailed": "A engenharia segura inclui criptografia (KMS), controle de acesso (IAM/Lake Formation), mascaramento/anonimização, minimização de dados e segregação de ambientes. Na AWS, catalogue no Glue, armazene no S3 com políticas, audite com o CloudTrail e monitore a postura com o Config. Tecnologias que aprimoram a privacidade reduzem o risco e o viés de exposição sem bloquear casos de uso legítimos.",
    "incorrect_explanations": {
      "A": "Coletar sem critérios aumenta o risco e o custo, sem garantir a qualidade.",
      "C": "O acesso público é o oposto da segurança de dados.",
      "D": "Armazenar sem criptografia viola as melhores práticas."
    }
  },
  {
    "id": "aif-c01-responsible_ai-194",
    "certification_id": "AIF-C01",
    "domain": "RESPONSIBLE_AI",
    "difficulty": "medium",
    "tier": "FREE",
    "required_selection_count": 1,
    "active": true,
    "question_text": "Qual é o objetivo principal da Matriz de Escopo de Segurança de IA Generativa?",
    "option_a": "Gerar modelos de IA",
    "option_b": "Fornecer um framework para avaliar os riscos de segurança da IA",
    "option_c": "Aumentar o desempenho do modelo",
    "option_d": "Reduzir o consumo de energia",
    "correct_answers": [
      "B"
    ],
    "explanation_detailed": "Uma matriz de escopo de segurança de IA generativa mapeia os riscos por camadas (dados, modelo, orquestração, interface) e por responsabilidades (equipe, provedor, cliente), orientando os controles: guardrails, PII, proveniência, detecção de abuso e auditoria. Na AWS, aplique os Guardrails do Bedrock, IAM/KMS, CloudTrail/Config, Kendra/OpenSearch com metadados e A2I para revisão humana. O objetivo é avaliar e priorizar sistematicamente a mitigação.",
    "incorrect_explanations": {
      "A": "Não cria modelos; estrutura a avaliação de risco.",
      "C": "O desempenho não é o foco principal; é a segurança.",
      "D": "A energia não é o alvo da matriz de segurança."
    }
  },
  {
    "id": "aif-c01-responsible_ai-195",
    "certification_id": "AIF-C01",
    "domain": "RESPONSIBLE_AI",
    "difficulty": "easy",
    "tier": "FREE",
    "required_selection_count": 1,
    "active": true,
    "question_text": "Qual serviço da AWS é usado para avaliações de segurança automatizadas?",
    "option_a": "Amazon EC2",
    "option_b": "Amazon Inspector",
    "option_c": "Amazon S3",
    "option_d": "Amazon RDS",
    "correct_answers": [
      "B"
    ],
    "explanation_detailed": "O Amazon Inspector avalia continuamente as cargas de trabalho (EC2, ECR, Lambda) em busca de vulnerabilidades conhecidas e exposição de rede. Em ambientes de IA, ajuda a manter imagens limpas e hosts seguros para pipelines de dados/treinamento. Combine com o GuardDuty para detecção de ameaças, o Config para postura e o CloudTrail para auditoria. Remedeie as descobertas com automações e controle de mudanças.",
    "incorrect_explanations": {
      "A": "O EC2 é computação; o Inspector é o que avalia as vulnerabilidades.",
      "C": "O S3 armazena dados; não realiza verificações de CVE.",
      "D": "O RDS não realiza varreduras de segurança de cargas de trabalho."
    }
  },
  {
    "id": "aif-c01-responsible_ai-196",
    "certification_id": "AIF-C01",
    "domain": "RESPONSIBLE_AI",
    "difficulty": "easy",
    "tier": "FREE",
    "required_selection_count": 1,
    "active": true,
    "question_text": "O que é residência de dados no contexto da governança de IA?",
    "option_a": "A localização física onde os dados são armazenados",
    "option_b": "A duração pela qual os dados são mantidos",
    "option_c": "A velocidade com que os dados são processados",
    "option_d": "O formato em que os dados são armazenados",
    "correct_answers": [
      "A"
    ],
    "explanation_detailed": "A residência de dados especifica a região/país onde os dados são armazenados e sob quais leis eles se enquadram. Na AWS, escolha regiões, use políticas do S3/Lake Formation e restrições de replicação, e documente os requisitos nos Model Cards e políticas. Para IA, confirme que os dados de treinamento/inferência estão em conformidade com as regras de localização e transferência.",
    "incorrect_explanations": {
      "B": "Isso é retenção, não residência.",
      "C": "A velocidade é desempenho; residência é localização legal.",
      "D": "O formato descreve o esquema/serialização, não a localização."
    }
  },
  {
    "id": "aif-c01-responsible_ai-197",
    "certification_id": "AIF-C01",
    "domain": "RESPONSIBLE_AI",
    "difficulty": "easy",
    "tier": "FREE",
    "required_selection_count": 1,
    "active": true,
    "question_text": "Qual dos seguintes NÃO é uma consideração típica na segurança de aplicações de IA?",
    "option_a": "Detecção de ameaças",
    "option_b": "Gerenciamento de vulnerabilidades",
    "option_c": "Maximização do compartilhamento público de dados",
    "option_d": "Proteção da infraestrutura",
    "correct_answers": [
      "C"
    ],
    "explanation_detailed": "A segurança de aplicações de IA requer monitoramento de ameaças (GuardDuty), gerenciamento de vulnerabilidades (Inspector), proteção da infraestrutura (VPC, WAF, IAM/KMS) e controle de dados (Macie, Lake Formation). Compartilhar dados publicamente vai contra a confidencialidade e a conformidade. Adicione guardrails no Bedrock e trilhas de auditoria (CloudTrail).",
    "incorrect_explanations": {
      "A": "A detecção de ameaças é um pilar essencial.",
      "B": "Vulnerabilidades não corrigidas expõem o ambiente.",
      "D": "A infraestrutura protegida reduz a superfície de ataque."
    }
  },
  {
    "id": "aif-c01-responsible_ai-198",
    "certification_id": "AIF-C01",
    "domain": "RESPONSIBLE_AI",
    "difficulty": "easy",
    "tier": "FREE",
    "required_selection_count": 1,
    "active": true,
    "question_text": "Qual é o objetivo principal do AWS Trusted Advisor na governança de IA?",
    "option_a": "Gerar modelos de IA",
    "option_b": "Fornecer orientação em tempo real para melhorar o ambiente AWS",
    "option_c": "Aumentar o desempenho do modelo",
    "option_d": "Reduzir o consumo de energia",
    "correct_answers": [
      "B"
    ],
    "explanation_detailed": "O Trusted Advisor fornece recomendações de custo, desempenho, segurança, tolerância a falhas e limites de serviço. Em ambientes de IA, ajuda a manter contas saudáveis: permissões excessivas, recursos subutilizados, backups e cotas. Combine com o Config/CloudTrail para governança contínua e com Guardrails/Clarify para camadas de responsabilidade específicas de IA.",
    "incorrect_explanations": {
      "A": "Não treina modelos; avalia as melhores práticas da conta.",
      "C": "O desempenho do modelo não é o alvo; é a postura do ambiente.",
      "D": "A energia não é um foco direto do Trusted Advisor."
    }
  },
  {
    "id": "aif-c01-responsible_ai-199",
    "certification_id": "AIF-C01",
    "domain": "RESPONSIBLE_AI",
    "difficulty": "easy",
    "tier": "FREE",
    "required_selection_count": 1,
    "active": true,
    "question_text": "Qual dos seguintes é um aspecto chave da integridade dos dados em sistemas de IA?",
    "option_a": "Garantir que os dados permaneçam inalterados e não corrompidos",
    "option_b": "Tornar todos os dados publicamente acessíveis",
    "option_c": "Usar apenas os maiores conjuntos de dados disponíveis",
    "option_d": "Armazenar todos os dados em um único local",
    "correct_answers": [
      "A"
    ],
    "explanation_detailed": "A integridade garante que os dados não sejam alterados indevidamente. Use versionamento (S3), checksums, logs de acesso (CloudTrail), criptografia (KMS) e políticas de mudança controladas (Lake Formation/IAM). Para IA, vincule conjuntos de dados e modelos no Model Registry e documente as transformações no Glue para rastrear a consistência entre dados e resultados.",
    "incorrect_explanations": {
      "B": "O acesso público compromete a confidencialidade e a integridade.",
      "C": "O tamanho não garante a integridade dos dados.",
      "D": "Concentrar em um local cria risco; a integridade requer controle, não centralização cega."
    }
  },
  {
    "id": "aif-c01-responsible_ai-200",
    "certification_id": "AIF-C01",
    "domain": "RESPONSIBLE_AI",
    "difficulty": "easy",
    "tier": "FREE",
    "required_selection_count": 1,
    "active": true,
    "question_text": "Qual é o objetivo principal da criptografia em repouso na segurança de IA?",
    "option_a": "Proteger os dados enquanto estão sendo transmitidos",
    "option_b": "Proteger os dados armazenados",
    "option_c": "Aumentar a velocidade de processamento de dados",
    "option_d": "Reduzir o consumo de energia",
    "correct_answers": [
      "B"
    ],
    "explanation_detailed": "A criptografia em repouso protege os dados armazenados (S3, EBS, RDS) usando chaves gerenciadas pelo AWS KMS. Em pipelines de IA, garanta que conjuntos de dados, artefatos de treinamento e modelos sejam criptografados. Combine com criptografia em trânsito (TLS), IAM para acesso de menor privilégio e auditoria via CloudTrail. Isso reduz o risco de exposição em caso de perda de mídia ou acesso não autorizado.",
    "incorrect_explanations": {
      "A": "Em trânsito é TLS; em repouso é armazenamento.",
      "C": "A criptografia não acelera o processamento; protege a confidencialidade.",
      "D": "A energia não é o alvo da criptografia; é um controle de segurança."
    }
  },
  {
    "id": "aif-c01-responsible_ai-201",
    "certification_id": "AIF-C01",
    "domain": "RESPONSIBLE_AI",
    "difficulty": "medium",
    "tier": "FREE",
    "required_selection_count": 1,
    "active": true,
    "question_text": "Qual dos seguintes NÃO faz normalmente parte dos protocolos de governança para sistemas de IA?",
    "option_a": "Revisões regulares de políticas",
    "option_b": "Requisitos de treinamento de pessoal",
    "option_c": "Maximização da complexidade do modelo",
    "option_d": "Padrões de transparência",
    "correct_answers": [
      "C"
    ],
    "explanation_detailed": "A governança define políticas, forma equipes, cria padrões de transparência (Model Cards), realiza auditorias periódicas e mede riscos. A complexidade do modelo não é um objetivo de governança. Na AWS, use o Audit Manager, Artifact, CloudTrail, Config e logs do Clarify/Model Monitor para evidências. Revise periodicamente os guardrails e os fluxos de revisão humana.",
    "incorrect_explanations": {
      "A": "As revisões mantêm as políticas alinhadas com os riscos reais.",
      "B": "O treinamento permite uma operação segura e ética.",
      "D": "A transparência é um pilar para a confiança e a auditoria."
    }
  },
  {
    "id": "aif-c01-responsible_ai-202",
    "certification_id": "AIF-C01",
    "domain": "RESPONSIBLE_AI",
    "difficulty": "medium",
    "tier": "FREE",
    "required_selection_count": 1,
    "active": true,
    "question_text": "Qual é o objetivo principal do AWS Audit Manager na governança de IA?",
    "option_a": "Gerar modelos de IA",
    "option_b": "Auditar continuamente o uso da AWS para conformidade",
    "option_c": "Aumentar o desempenho do modelo",
    "option_d": "Reduzir o consumo de energia",
    "correct_answers": [
      "B"
    ],
    "explanation_detailed": "O AWS Audit Manager coleta automaticamente evidências de múltiplos serviços (CloudTrail, Config, etc.) para avaliar a conformidade com frameworks. Em IA, isso apoia auditorias de dados, acesso, treinamento e inferência. Combine com o Artifact para relatórios oficiais, com os Model Cards para documentação do modelo e com o Clarify/Model Monitor para métricas anexadas.",
    "incorrect_explanations": {
      "A": "O Audit Manager não treina modelos; coleta evidências.",
      "C": "O desempenho do modelo não está no escopo do serviço.",
      "D": "A energia não é o foco; é a conformidade."
    }
  },
  {
    "id": "aif-c01-responsible_ai-203",
    "certification_id": "AIF-C01",
    "domain": "RESPONSIBLE_AI",
    "difficulty": "medium",
    "tier": "FREE",
    "required_selection_count": 1,
    "active": true,
    "question_text": "Qual das seguintes é uma consideração chave na proteção da infraestrutura de IA?",
    "option_a": "Maximizar o acesso público aos sistemas de IA",
    "option_b": "Implementar medidas de segurança de rede",
    "option_c": "Usar apenas os maiores modelos disponíveis",
    "option_d": "Armazenar todos os dados em um único local",
    "correct_answers": [
      "B"
    ],
    "explanation_detailed": "A segurança de rede inclui segmentação (VPC), listas de controle de acesso (NACLs), Grupos de Segurança, WAF e proteção contra DDoS. Em pipelines de IA, isole os endpoints do SageMaker/Bedrock atrás de camadas de API e autenticação, registre o tráfego (VPC Flow Logs) e audite com o CloudTrail. Combine com IAM/KMS e monitoramento contínuo para reduzir as superfícies de ataque.",
    "incorrect_explanations": {
      "A": "O acesso público aumenta o risco de exploração.",
      "C": "O tamanho do modelo não protege a infraestrutura.",
      "D": "Centralizar tudo aumenta o risco; prefira controles e redundância."
    }
  },
  {
    "id": "aif-c01-responsible_ai-204",
    "certification_id": "AIF-C01",
    "domain": "RESPONSIBLE_AI",
    "difficulty": "easy",
    "tier": "FREE",
    "required_selection_count": 1,
    "active": true,
    "question_text": "Qual é o objetivo principal do catálogo de dados na governança de IA?",
    "option_a": "Tornar todos os dados publicamente acessíveis",
    "option_b": "Organizar e inventariar os ativos de dados",
    "option_c": "Aumentar a velocidade de processamento de dados",
    "option_d": "Reduzir os custos de armazenamento de dados",
    "correct_answers": [
      "B"
    ],
    "explanation_detailed": "Um catálogo (por exemplo, AWS Glue Data Catalog) descreve conjuntos de dados, esquemas, locais, proprietários e classificações de sensibilidade. Facilita a descoberta, o controle de acesso (Lake Formation/IAM) e a auditoria (CloudTrail). Para IA, vincule conjuntos de dados a modelos no Model Registry e documente nos Model Cards para rastreabilidade de ponta a ponta.",
    "incorrect_explanations": {
      "A": "Um catálogo não implica acesso público; auxilia na governança.",
      "C": "A velocidade depende da arquitetura e da computação, não do catálogo.",
      "D": "Os custos podem melhorar indiretamente, mas não é o objetivo central."
    }
  },
  {
    "id": "aif-c01-responsible_ai-205",
    "certification_id": "AIF-C01",
    "domain": "RESPONSIBLE_AI",
    "difficulty": "easy",
    "tier": "FREE",
    "required_selection_count": 1,
    "active": true,
    "question_text": "Qual dos seguintes NÃO é tipicamente um componente de uma estratégia de gerenciamento do ciclo de vida dos dados?",
    "option_a": "Criação de dados",
    "option_b": "Retenção de dados",
    "option_c": "Exclusão de dados",
    "option_d": "Compartilhamento público de dados",
    "correct_answers": [
      "D"
    ],
    "explanation_detailed": "O gerenciamento do ciclo de vida abrange criação, ingestão, classificação, armazenamento, uso, retenção e exclusão segura. O compartilhamento público não é um componente padrão. Na AWS, combine o S3 Lifecycle, Lake Formation/IAM, KMS, Glue Catalog e auditorias com o CloudTrail/Config. Documente os processos para auditorias e conformidade.",
    "incorrect_explanations": {
      "A": "A criação/ingestão inicia o ciclo e precisa de governança.",
      "B": "A retenção cumpre as leis e políticas.",
      "C": "A exclusão controlada evita retenção indevida e riscos."
    }
  },
  {
    "id": "aif-c01-ml_development-206",
    "certification_id": "AIF-C01",
    "domain": "ML_DEVELOPMENT",
    "difficulty": "medium",
    "tier": "FREE",
    "required_selection_count": 2,
    "active": true,
    "question_text": "Uma empresa implantou um modelo de detecção de fraudes e quer garantir a precisão contínua e a revisão humana quando necessário. Quais serviços ou recursos da AWS devem ser usados? (Selecione DUAS.)",
    "option_a": "Amazon SageMaker Model Monitor",
    "option_b": "Amazon A2I (Amazon Augmented AI)",
    "option_c": "Amazon Bedrock",
    "option_d": "Amazon SageMaker Ground Truth",
    "correct_answers": [
      "A",
      "B"
    ],
    "explanation_detailed": "O SageMaker Model Monitor detecta desvios de dados e qualidade em endpoints de produção, gerando métricas e alertas quando as distribuições mudam ou quando o desempenho se degrada. Isso permite acionar o retreinamento ou revisões. O Amazon A2I insere a revisão humana com base em critérios (baixa confiança, amostragem periódica, casos de risco) para validar previsões, coletar feedback e criar conjuntos de erros para correção. O Bedrock é útil para IA generativa, mas não é o serviço de monitoramento específico para este cenário. O Ground Truth é para rotulagem de dados; relevante para criar conjuntos de dados, mas não monitora endpoints de produção nem orquestra revisões de previsão em tempo real.",
    "incorrect_explanations": {
      "C": "O Bedrock orquestra FMs; não monitora as métricas de um modelo de fraude existente.",
      "D": "O Ground Truth rotula dados; não monitora o desvio do endpoint nem aciona a revisão humana em produção."
    }
  }
]