[
{
"id": "aif-c01-responsible_ai-001",
"certification_id": "AIF-C01",
"domain": "RESPONSIBLE_AI",
"difficulty": "medium",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "An AI practitioner must write a transparency and explainability report for quarterly demand-forecasting ML models. What should be included to meet explainability requirements for stakeholders?",
"option_a": "Training code.",
"option_b": "Partial Dependence Plots (PDPs).",
"option_c": "Sample training data.",
"option_d": "Model convergence tables.",
"correct_answers": ["B"],
"explanation_detailed": "Partial Dependence Plots (PDPs) visualize the marginal effect of one or two input features on a model’s prediction while averaging out the influence of other variables. They provide a feature-to-outcome view that non-technical stakeholders can grasp, supporting transparency and interpretability without disclosing proprietary data or complex internals. PDPs help reveal monotonicity, diminishing returns, and interaction hints. In AWS, you can generate feature-attribution artifacts with Amazon SageMaker Clarify and complement them with PDP-like analyses in notebooks. Together with documentation of data lineage and evaluation metrics, PDPs make it clear how specific drivers (price, seasonality, promotions) move the forecast, which is central to responsible reporting and model risk management.",
"incorrect_explanations": {
"A": "Publishing raw training code alone does not explain how individual features influence predictions. It aids reproducibility but offers limited stakeholder-level interpretability without model-behavior visualizations.",
"C": "Sample data shows inputs but not their causal or marginal contribution to outcomes. It can raise privacy concerns and still leaves stakeholders unsure how the model uses features.",
"D": "Convergence tables focus on optimization progress or loss curves, not on feature impact. They help engineers, but they do not translate into stakeholder-friendly explanations."
}
},
{
"id": "aif-c01-ai_services-002",
"certification_id": "AIF-C01",
"domain": "AI_SERVICES",
"difficulty": "medium",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "A law firm wants an AI application using large language models to read legal documents and extract key points. Which solution fits?",
"option_a": "Build a named entity recognition (NER) automation only.",
"option_b": "Create a content recommendation engine.",
"option_c": "Develop a summarization chatbot.",
"option_d": "Develop a multilingual translation system.",
"correct_answers": ["C"],
"explanation_detailed": "A summarization chatbot powered by an LLM is designed to condense long, technical texts into concise, salient points while preserving legal nuance. Using Amazon Bedrock, you can invoke foundation models to perform extractive or abstractive summarization and layer retrieval over case repositories if needed. Guardrails can enforce tone and policy, while Amazon Kendra or a Bedrock knowledge base can improve grounding on firm documents. Compared to a pure NER pipeline, summarization covers broader insights such as issue spotting, holdings, and obligations, not just entities. This aligns with the firm’s need to read documents and surface key points interactively, with governance and logging via CloudWatch and Bedrock invocation logging.",
"incorrect_explanations": {
"A": "NER extracts entities (names, dates, statutes) but misses higher-level key points, arguments, or obligations that summarization captures.",
"B": "Recommendation engines rank or suggest content; they do not read a document and extract its salient points.",
"D": "Translation changes language, not content abstraction. It does not identify key legal ideas in the original document."
}
},
{
"id": "aif-c01-ai_fundamentals-003",
"certification_id": "AIF-C01",
"domain": "AI_FUNDAMENTALS",
"difficulty": "medium",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "A company must classify human genes into 20 categories and wants to document how internal model mechanics affect outputs. Which algorithm best fits?",
"option_a": "Decision Trees.",
"option_b": "Linear Regression.",
"option_c": "Logistic Regression.",
"option_d": "Neural Networks.",
"correct_answers": ["A"],
"explanation_detailed": "Decision trees provide native interpretability by exposing the hierarchical splits that lead to a prediction. Each node’s condition shows which gene features drive the decision boundary, enabling straightforward, auditable explanations suitable for regulated or scientific contexts. Compared to neural networks, trees require no post-hoc explanation to understand mechanics. They also scale to multi-class problems naturally. On AWS, you can train tree-based models with Amazon SageMaker (e.g., XGBoost or built-in algorithms) and use SageMaker Clarify for feature importance and bias checks. While logistic regression offers interpretability, it assumes linear decision boundaries; tree models capture non-linear interactions between gene features while staying transparent.",
"incorrect_explanations": {
"B": "Linear regression targets continuous outcomes and assumes linearity, which is unsuitable for multi-class categorical outputs and complex interactions.",
"C": "Logistic regression can be interpretable but becomes less practical with 20 classes and non-linear feature interactions common in genomics.",
"D": "Neural networks can be accurate but are opaque by default; documenting internal mechanics requires extra tooling and expertise."
}
},
{
"id": "aif-c01-ai_fundamentals-004",
"certification_id": "AIF-C01",
"domain": "AI_FUNDAMENTALS",
"difficulty": "easy",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "A plant-disease image classifier needs a metric showing how many images were classified correctly. Which metric should be used?",
"option_a": "R-squared score.",
"option_b": "Accuracy.",
"option_c": "Root Mean Squared Error (RMSE).",
"option_d": "Learning rate.",
"correct_answers": ["B"],
"explanation_detailed": "Accuracy measures the proportion of correct predictions over all predictions, which directly answers “how many images were classified correctly.” For balanced, single-label multi-class problems, accuracy is an intuitive primary metric. In AWS, you can compute it during SageMaker training and track it in CloudWatch or SageMaker Experiments. If classes are imbalanced, supplement accuracy with precision, recall, and F1, and inspect confusion matrices to understand class-specific errors. Metrics such as RMSE or R-squared are regression-oriented, while the learning rate is a hyperparameter, not an evaluation metric. Thus, accuracy is the cleanest top-level indicator for a plant leaf disease classifier’s correctness.",
"incorrect_explanations": {
"A": "R-squared quantifies variance explained for regression, not classification correctness across discrete labels.",
"C": "RMSE is a regression error metric; it does not represent categorical hit/miss outcomes.",
"D": "Learning rate is a training hyperparameter, not a performance metric."
}
},
{
"id": "aif-c01-ai_services-005",
"certification_id": "AIF-C01",
"domain": "AI_SERVICES",
"difficulty": "easy",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "A company uses a pretrained LLM for a product-recommendation chatbot. They need short outputs in a specific language. What aligns the LLM responses with expectations?",
"option_a": "Prompt tuning and instruction refinement.",
"option_b": "Choose a different LLM size.",
"option_c": "Increase temperature.",
"option_d": "Increase Top-K.",
"correct_answers": ["A"],
"explanation_detailed": "Clear, constraint-driven prompts steer generation style: target language, maximum tokens, bullet format, tone, and examples of desired replies. In Amazon Bedrock, you can specify system and user messages to enforce brevity and language, add few-shot examples, and set max output tokens. Lower temperature and decoding limits can further reduce verbosity, but the biggest lever is instruction quality. Model size changes capacity, not adherence to style. Top-K and temperature control randomness, not high-level policy. Combine explicit instructions with guardrails and invocation parameters to reliably produce concise, on-brand, language-specific responses without retraining the underlying model.",
"incorrect_explanations": {
"B": "Model size affects capability and latency, not adherence to stylistic constraints you can enforce with clear instructions.",
"C": "Higher temperature increases randomness and verbosity risk; it works against consistency and brevity.",
"D": "Top-K adjusts sampling diversity, not overall style control or required language constraints."
}
},
{
"id": "aif-c01-ai_services-006",
"certification_id": "AIF-C01",
"domain": "AI_SERVICES",
"difficulty": "medium",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "Using Amazon SageMaker in production, inputs can be up to 1 GB and processing up to 1 hour, with near real-time latency required. Which inference option fits?",
"option_a": "Real-time inference.",
"option_b": "Serverless inference.",
"option_c": "Asynchronous inference.",
"option_d": "Batch transform.",
"correct_answers": ["C"],
"explanation_detailed": "SageMaker Asynchronous Inference handles large payloads and long processing times without holding an open HTTP connection. Clients submit requests and later retrieve results from an output S3 location, achieving near real-time responsiveness at scale while decoupling front-end latency from back-end processing. Real-time endpoints are better for sub-second payloads. Serverless inference targets spiky, low-latency traffic with smaller payloads. Batch transform is for offline, deferred processing of large datasets with no immediacy requirement. Asynchronous Inference provides the best balance for big inputs, long compute windows, queueing, and near real-time delivery.",
"incorrect_explanations": {
"A": "Real-time endpoints keep connections open and are not optimized for very large payloads or hour-long processing.",
"B": "Serverless inference suits low-latency, bursty workloads with smaller payloads, not 1 GB and hour-long runs.",
"D": "Batch transform is offline and not designed for near real-time responsiveness."
}
},
{
"id": "aif-c01-ai_fundamentals-007",
"certification_id": "AIF-C01",
"domain": "AI_FUNDAMENTALS",
"difficulty": "easy",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "A team wants to avoid training from scratch and adapt pretrained, domain-specific models for related tasks. Which strategy should they use?",
"option_a": "Increase the number of epochs.",
"option_b": "Transfer learning.",
"option_c": "Decrease the number of epochs.",
"option_d": "Unsupervised learning.",
"correct_answers": ["B"],
"explanation_detailed": "Transfer learning starts from a pretrained model’s learned representations and fine-tunes them on a related downstream task, dramatically reducing data, compute, and time. This is effective when source and target domains share structure (e.g., legal to contract summaries, medical images to related modalities). On AWS, use Amazon SageMaker training or Amazon Bedrock fine-tuning endpoints to adapt foundation models, and leverage MLOps features—Model Registry, Pipelines—for repeatability. Adjust learning rates, unfreeze layers selectively, and monitor overfitting. Transfer learning typically outperforms training from scratch when labeled data is limited and accelerates deployment without sacrificing accuracy.",
"incorrect_explanations": {
"A": "More epochs on a randomly initialized model increases cost and overfitting risk without leveraging prior knowledge.",
"C": "Fewer epochs does not enable knowledge reuse; it simply truncates training time.",
"D": "Unsupervised learning finds structure without labels and does not directly adapt a pretrained supervised model to a new labeled task."
}
},
{
"id": "aif-c01-ai_services-008",
"certification_id": "AIF-C01",
"domain": "AI_SERVICES",
"difficulty": "medium",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "A solution must generate images for protective goggles with high annotation accuracy and minimal mislabeling risk. What should be used?",
"option_a": "Human-in-the-loop validation with Amazon SageMaker Ground Truth Plus.",
"option_b": "Data augmentation using an Amazon Bedrock knowledge base.",
"option_c": "Image recognition with Amazon Rekognition.",
"option_d": "Data summarization using Amazon QuickSight Q.",
"correct_answers": ["A"],
"explanation_detailed": "For high-stakes computer vision datasets, human-in-the-loop (HITL) annotation workflows reduce errors. Amazon SageMaker Ground Truth Plus provides managed labeling with expert workforces, multi-stage review, and built-in quality controls. You can define label taxonomies, consensus strategies, and auditing processes, which is crucial when safety equipment images require precise bounding boxes or segmentation. Rekognition offers prebuilt detection but does not solve dataset creation quality. A Bedrock knowledge base augments LLM context, not labels for images. QuickSight Q supports BI Q&A, not vision annotation. Ground Truth Plus delivers rigorous accuracy controls and metrics needed to minimize mislabeling risk.",
"incorrect_explanations": {
"B": "A Bedrock knowledge base augments LLM context for retrieval, not image labeling accuracy or dataset creation.",
"C": "Rekognition detects objects and scenes but does not manage bespoke annotation workflows or label quality assurance.",
"D": "QuickSight Q is a business intelligence natural-language feature, unrelated to image dataset annotation."
}
},
{
"id": "aif-c01-ai_services-009",
"certification_id": "AIF-C01",
"domain": "AI_SERVICES",
"difficulty": "medium",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "A Bedrock foundation model must access encrypted objects in Amazon S3 that use SSE-S3. The model fails to access the bucket. What should be ensured?",
"option_a": "The role assumed by Amazon Bedrock has permissions to decrypt using the correct encryption key.",
"option_b": "Set S3 buckets to public and enable internet access.",
"option_c": "Use prompt engineering to instruct the model to look into S3.",
"option_d": "Ensure the S3 data has no sensitive information.",
"correct_answers": ["A"],
"explanation_detailed": "Accessing encrypted S3 data requires the invoking service role to have appropriate permissions to read and decrypt the objects. With AWS services such as Amazon Bedrock integrating with S3, the IAM role must include the necessary S3 actions (for example, s3:GetObject) and decryption permissions consistent with the bucket’s encryption settings. In tightly controlled environments, pair this with VPC endpoints and restrictive bucket policies scoped to the role. While encryption mode details differ (KMS vs. S3-managed), the governing principle remains: grant the service role the least-privilege rights to read and decrypt. Do not make buckets public or rely on prompts to bypass authorization.",
"incorrect_explanations": {
"B": "Public access weakens security posture and violates least privilege without fixing missing role permissions.",
"C": "Prompt instructions cannot override IAM or S3 authorization; access control is enforced by AWS, not the model.",
"D": "Sanitizing content does not address the authorization failure when reading encrypted objects."
}
},
{
"id": "aif-c01-ai_services-010",
"certification_id": "AIF-C01",
"domain": "AI_SERVICES",
"difficulty": "medium",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "A company will build an ML model on Amazon SageMaker and needs to share and manage features across multiple teams during development. What should they use?",
"option_a": "Amazon SageMaker Feature Store",
"option_b": "Amazon SageMaker Data Wrangler",
"option_c": "Amazon SageMaker Clarify",
"option_d": "Amazon SageMaker Model Cards",
"correct_answers": ["A"],
"explanation_detailed": "Amazon SageMaker Feature Store provides a centralized, versioned repository for online and offline features. It enables consistent feature definitions across teams, reduces training/serving skew, and supports low-latency online retrieval for inference. Offline stores back experiments and batch training. Integration with SageMaker Pipelines and Model Registry supports MLOps governance. Data Wrangler focuses on preparation and transformation, Clarify on bias/explainability, and Model Cards on documentation. Using Feature Store ensures that data scientists and engineers reuse vetted features, improve reproducibility, and accelerate model delivery while maintaining lineage and access control through IAM and encryption at rest.",
"incorrect_explanations": {
"B": "Data Wrangler streamlines data prep and visualization; it does not provide a shared, versioned repository for cross-team feature reuse.",
"C": "Clarify explains models and detects bias; it does not manage feature storage and online serving.",
"D": "Model Cards document model details for governance; they are not a feature repository."
}
},
{
"id": "aif-c01-ai_services-011",
"certification_id": "AIF-C01",
"domain": "AI_SERVICES",
"difficulty": "easy",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "A company wants generative AI to boost developer productivity and will use Amazon Q Developer. What capability is aligned?",
"option_a": "Create code snippets, trace references, and manage open-source license obligations.",
"option_b": "Run an application without managing servers.",
"option_c": "Enable voice commands for coding and natural-language search.",
"option_d": "Convert audio files to text documents with ML models.",
"correct_answers": ["A"],
"explanation_detailed": "Amazon Q Developer is a generative AI assistant that accelerates coding tasks: drafting functions, unit tests, and refactors, while surfacing references and license considerations from dependencies. It integrates into IDEs and AWS services to recommend code and explanations in context. Serverless execution is an AWS compute pattern (e.g., Lambda), not a developer assistant function. Speech-to-text belongs to Amazon Transcribe, and voice assistants are separate capabilities. Q Developer’s value is targeted productivity in authoring, reviewing, and understanding code, while preserving compliance by tracking OSS licenses and using context from repositories and tickets.",
"incorrect_explanations": {
"B": "Serverless compute (e.g., AWS Lambda) addresses runtime management, not code generation and developer assistance.",
"C": "Voice control is not the core capability; Q Developer focuses on code generation, explanations, and integration with developer tools.",
"D": "Transcribing audio is Amazon Transcribe’s role, not Q Developer’s coding productivity function."
}
},
{
"id": "aif-c01-ai_services-012",
"certification_id": "AIF-C01",
"domain": "AI_SERVICES",
"difficulty": "medium",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "A financial institution uses Amazon Bedrock inside a VPC with no internet egress due to compliance. Which option enables private service connectivity?",
"option_a": "AWS PrivateLink",
"option_b": "Amazon Macie",
"option_c": "Amazon CloudFront",
"option_d": "Internet gateway",
"correct_answers": ["A"],
"explanation_detailed": "AWS PrivateLink provides private connectivity between VPCs and supported AWS services without traversing the public internet. It establishes interface VPC endpoints for Bedrock where available, keeping traffic within the AWS network to meet compliance requirements. Macie is for data-privacy discovery in S3. CloudFront is a CDN that relies on public endpoints. An Internet Gateway would break the “no internet egress” policy. With PrivateLink and tight IAM, you maintain least-privilege access while logging via CloudWatch and CloudTrail. Combine with VPC endpoint policies and S3 gateway endpoints for a fully private data-and-model interaction path.",
"incorrect_explanations": {
"B": "Macie classifies sensitive data in S3; it does not provide private network connectivity.",
"C": "CloudFront accelerates content delivery over the internet and is not a private link mechanism.",
"D": "An Internet Gateway enables internet access, violating the no-egress constraint."
}
},
{
"id": "aif-c01-ai_fundamentals-013",
"certification_id": "AIF-C01",
"domain": "AI_FUNDAMENTALS",
"difficulty": "easy",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "An educational game asks probability questions like drawing a green ball from a jar with known counts. What is the simplest approach?",
"option_a": "Use supervised learning to train a regression model.",
"option_b": "Use reinforcement learning to return the probability.",
"option_c": "Compute the probability with straightforward rules and calculations.",
"option_d": "Use unsupervised learning to estimate density.",
"correct_answers": ["C"],
"explanation_detailed": "This is a closed-form probability problem: P(green) = count(green)/count(total). Coding the rule is simpler, cheaper, and perfectly accurate, avoiding any ML overhead. In AWS, such logic can run in AWS Lambda or within an application backend without model training or hosting. ML is useful when relationships are unknown or complex; here, the relationship is explicit and deterministic. Training a model to approximate a trivial formula adds cost, latency, and risk of error. Keep ML for problems where it adds value, and apply direct computation for elementary combinatorial or probabilistic tasks.",
"incorrect_explanations": {
"A": "Regression would learn to approximate a known formula, adding training cost and possible error for a trivial task.",
"B": "Reinforcement learning is for sequential decision problems with rewards, not static closed-form probability calculations.",
"D": "Unsupervised density estimation is unnecessary when counts are explicit and formulaic."
}
},
{
"id": "aif-c01-ai_fundamentals-014",
"certification_id": "AIF-C01",
"domain": "AI_FUNDAMENTALS",
"difficulty": "easy",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "Which metric evaluates runtime efficiency of AI models?",
"option_a": "Customer satisfaction (CSAT).",
"option_b": "Training time per epoch.",
"option_c": "Average response time.",
"option_d": "Number of training instances.",
"correct_answers": ["C"],
"explanation_detailed": "Average response time (or latency) measures how quickly a model serves predictions, reflecting runtime efficiency. It is tracked at inference endpoints (for example, Amazon SageMaker real-time or serverless inference) and visualized in CloudWatch. Training time per epoch is a training metric, not serving efficiency. Instance count describes capacity, not efficiency. CSAT is a business outcome metric and may correlate with latency but does not measure it. Optimizing latency involves model compression, better hardware, batch sizing, and autoscaling policies, all observable through endpoint metrics and logs.",
"incorrect_explanations": {
"A": "CSAT is a perception metric and does not directly quantify serving latency.",
"B": "Epoch time relates to training speed, not production inference efficiency.",
"D": "Instance count indicates capacity; efficiency requires latency and throughput measurements."
}
},
{
"id": "aif-c01-ai_services-015",
"certification_id": "AIF-C01",
"domain": "AI_SERVICES",
"difficulty": "easy",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "A contact-center application needs insights from customer conversations by analyzing and extracting key information from call audio. Which AWS service should be used first?",
"option_a": "Amazon Lex to build a conversational bot.",
"option_b": "Amazon Transcribe to transcribe call recordings.",
"option_c": "Amazon SageMaker Model Monitor to extract information.",
"option_d": "Amazon Comprehend to create classification labels.",
"correct_answers": ["B"],
"explanation_detailed": "Start by converting audio to text using Amazon Transcribe, which provides accurate speech-to-text with timestamps, channel separation, and PII redaction. Once transcripts are available, you can apply Amazon Comprehend for sentiment, key phrases, entities, and custom classification. For generative summaries, invoke foundation models via Amazon Bedrock. Lex is used to build interactive bots, not offline analysis of recorded audio. Model Monitor tracks data/quality drift for deployed models, not basic transcription. This pipeline—Transcribe then NLP/LLM—unlocks structured insights for QA, compliance, and coaching.",
"incorrect_explanations": {
"A": "Lex is for building live conversational interfaces; it does not process historic audio recordings into text.",
"C": "Model Monitor detects drift in deployed models; it does not extract information from audio.",
"D": "Comprehend analyzes text; you first need transcripts created by Transcribe."
}
},
{
"id": "aif-c01-ai_fundamentals-016",
"certification_id": "AIF-C01",
"domain": "AI_FUNDAMENTALS",
"difficulty": "easy",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "A company has petabytes of unlabeled customer data to segment for marketing. Which learning approach fits?",
"option_a": "Supervised learning",
"option_b": "Unsupervised learning",
"option_c": "Reinforcement learning",
"option_d": "Reinforcement learning with human feedback (RLHF)",
"correct_answers": ["B"],
"explanation_detailed": "Unsupervised learning discovers patterns without labels, making it ideal for customer segmentation when only raw attributes exist. Clustering (e.g., k-means) or dimensionality reduction can group similar customers for targeted campaigns. On AWS, use SageMaker to train unsupervised algorithms and evaluate segments with downstream KPIs. Supervised methods require labeled targets, RL addresses sequential decision-making, and RLHF shapes reward functions for LLMs. Starting with unsupervised segmentation enables hypothesis generation that later can be validated with A/B testing and uplift modeling.",
"incorrect_explanations": {
"A": "Supervised learning requires labeled targets (e.g., churn yes/no), which are absent here.",
"C": "Reinforcement learning is for sequential decisions with rewards, not static segmentation.",
"D": "RLHF is a specialized technique for aligning generative models, not customer segmentation."
}
},
{
"id": "aif-c01-ai_services-017",
"certification_id": "AIF-C01",
"domain": "AI_SERVICES",
"difficulty": "medium",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "An AI search app must handle queries containing both text and images. What model type should be used?",
"option_a": "Multimodal embedding model",
"option_b": "Text-only embedding model",
"option_c": "Multimodal generation model",
"option_d": "Image generation model",
"correct_answers": ["A"],
"explanation_detailed": "Multimodal embedding models map heterogeneous inputs—text and images—into a shared vector space, enabling similarity search across modalities. This supports use cases such as “find images like this caption” or “match this photo to related documents.” With Amazon OpenSearch Service’s k-NN and vector indices, you can store embeddings and query nearest neighbors efficiently. Text-only embeddings miss image semantics, generation models synthesize outputs rather than index similarity. Multimodal embeddings, coupled with Bedrock-hosted encoders and vector databases, underpin robust cross-modal retrieval and RAG pipelines.",
"incorrect_explanations": {
"B": "Text-only embeddings cannot encode images; they break cross-modal retrieval requirements.",
"C": "Generation focuses on creating content, not producing comparable vectors for retrieval.",
"D": "Image generators synthesize pictures; they are not built for similarity search across text and images."
}
},
{
"id": "aif-c01-ai_services-018",
"certification_id": "AIF-C01",
"domain": "AI_SERVICES",
"difficulty": "medium",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "A Bedrock foundation model powering search must be made more accurate using company data. What should be provided for fine-tuning?",
"option_a": "Labeled data with prompt and completion fields",
"option_b": "A .txt file containing multiple lines in .csv format",
"option_c": "Provisioned throughput for Amazon Bedrock",
"option_d": "Train on newspapers and textbooks",
"correct_answers": ["A"],
"explanation_detailed": "For supervised fine-tuning of a foundation model, provide aligned prompt–completion pairs that reflect your domain and the desired outputs. This teaches the model to map enterprise-style queries to precise answers, formats, and constraints. On Amazon Bedrock, supported models expose fine-tuning APIs and expect structured JSONL with fields like instruction, input, and output. Provisioned throughput impacts capacity, not quality. Generic public corpora dilute domain specificity. Proper labeling, validation splits, and evaluation with business-relevant metrics ensure improved accuracy aligned to enterprise needs.",
"incorrect_explanations": {
"B": "Malformed files or arbitrary text do not supply the supervised signal needed for fine-tuning.",
"C": "Throughput affects scalability and latency, not accuracy of outputs.",
"D": "Training on generic data ignores company context, reducing domain alignment."
}
},
{
"id": "aif-c01-ai_fundamentals-019",
"certification_id": "AIF-C01",
"domain": "AI_FUNDAMENTALS",
"difficulty": "medium",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "An AI solution must verify whether an IP address is suspicious to protect an application. Which approach fits?",
"option_a": "Build a speech recognition system.",
"option_b": "Create a named entity recognition (NLP) system.",
"option_c": "Develop an anomaly detection system.",
"option_d": "Create a fraud prediction system.",
"correct_answers": ["C"],
"explanation_detailed": "Anomaly detection identifies patterns that deviate from normal traffic baselines, flagging suspicious IPs exhibiting abnormal request rates, geolocation shifts, or behavior. This can be done with unsupervised algorithms (e.g., Isolation Forest) when labeled attack data is limited. On AWS, use SageMaker to train and deploy anomaly detectors; pair with AWS WAF, CloudFront, and GuardDuty findings for defense-in-depth. Fraud models are typically supervised on labeled transactions. NER and speech recognition do not apply to network telemetry. Anomaly detection offers early warning and generalizes to novel threat signatures.",
"incorrect_explanations": {
"A": "Speech recognition transforms audio to text and is unrelated to network threat detection.",
"B": "NER extracts entities from text; it does not evaluate IP behavioral anomalies.",
"D": "Fraud prediction targets transactional patterns; IP reputation often benefits from unsupervised anomaly baselines."
}
},
{
"id": "aif-c01-ai_services-020",
"certification_id": "AIF-C01",
"domain": "AI_SERVICES",
"difficulty": "medium",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "Which Amazon OpenSearch Service capability enables vector database applications?",
"option_a": "Integration with Amazon S3 object storage",
"option_b": "Geospatial indexing and queries",
"option_c": "Scalable index management and k-NN nearest-neighbor search",
"option_d": "Real-time analytics on streaming data",
"correct_answers": ["C"],
"explanation_detailed": "Amazon OpenSearch Service supports k-NN vector indices that store high-dimensional embeddings and perform approximate nearest-neighbor search, enabling semantic similarity applications such as RAG, cross-modal search, and recommendation. You can scale shards and replicas, tune recall/latency trade-offs, and combine vector and keyword filters. S3 integration handles snapshots, geospatial is orthogonal to vectors, and streaming analytics use cases rely on OpenSearch ingest—not vector similarity. With Bedrock for embeddings and OpenSearch k-NN, you can build robust vector databases for production workloads.",
"incorrect_explanations": {
"A": "S3 snapshot integration is for backups and restores, not vector similarity search.",
"B": "Geospatial features target location queries, not high-dimensional embedding search.",
"D": "Streaming analytics is valuable but unrelated to nearest-neighbor vector retrieval."
}
},
{
"id": "aif-c01-ai_fundamentals-021",
"certification_id": "AIF-C01",
"domain": "AI_FUNDAMENTALS",
"difficulty": "easy",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "Which is a valid use case for generative AI models?",
"option_a": "Improve network security using intrusion detection systems.",
"option_b": "Create photorealistic images from textual descriptions for digital marketing.",
"option_c": "Improve database performance using optimized indexing.",
"option_d": "Analyze financial data to forecast stock market trends with ARIMA.",
"correct_answers": ["B"],
"explanation_detailed": "Generative models synthesize content—images, text, audio, or code—from prompts. For marketing, text-to-image generation can produce product visuals, lifestyle scenes, and variants quickly. With Amazon Bedrock, you can invoke image generation models through managed APIs and add guardrails to enforce brand policies. While AI aids security, database tuning, or time-series forecasting, those are not primarily generative tasks. Generative pipelines can be combined with retrieval for brand assets and SageMaker endpoints for approval workflows. This accelerates creative cycles while maintaining governance and cost controls.",
"incorrect_explanations": {
"A": "Intrusion detection focuses on anomaly or signature detection, not content generation.",
"C": "Index optimization is a database engineering task, not generative synthesis.",
"D": "Stock forecasting is predictive modeling; generative models target content creation."
}
},
{
"id": "aif-c01-ai_fundamentals-022",
"certification_id": "AIF-C01",
"domain": "AI_FUNDAMENTALS",
"difficulty": "easy",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "When choosing a foundation model in Amazon Bedrock, which concept determines how much information can fit in a single prompt?",
"option_a": "Temperature",
"option_b": "Context window",
"option_c": "Batch size",
"option_d": "Model size",
"correct_answers": ["B"],
"explanation_detailed": "The context window (context length) determines the maximum number of tokens the model can attend to across input and output. Larger windows allow longer prompts, more retrieved context, or multi-document conversations. In Bedrock, select models with sufficient context for your grounding strategy (e.g., RAG). Temperature controls randomness, not capacity. Batch size applies to throughput during training/inference, not per-request context capacity. Model size correlates with capability but does not guarantee a larger context. Align window size with your retrieval chunking and token budgets to avoid truncation.",
"incorrect_explanations": {
"A": "Temperature changes output randomness; it does not increase how many tokens fit in the prompt.",
"C": "Batch size affects parallelism/throughput, not prompt length capacity.",
"D": "Larger models may or may not have larger context windows; context length is a distinct parameter."
}
},
{
"id": "aif-c01-ai_services-023",
"certification_id": "AIF-C01",
"domain": "AI_SERVICES",
"difficulty": "easy",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "A company is building a customer-support chatbot with a foundation model. The bot must answer in the company’s tone. What should the team do?",
"option_a": "Set a very low token limit for outputs.",
"option_b": "Use batch inference to process detailed responses.",
"option_c": "Iteratively experiment and refine the prompt until the FM produces the desired style.",
"option_d": "Increase the temperature parameter.",
"correct_answers": ["C"],
"explanation_detailed": "Style alignment is primarily achieved through strong prompting: a system prompt that specifies tone, persona, brand vocabulary, and format, plus few-shot examples showing correct and incorrect answers. In Amazon Bedrock, you can enforce this with guardrails and max token settings to keep outputs concise. Temperature controls randomness, not brand tone. Batch inference is about throughput, not style. Token limits prevent verbosity but do not establish voice. Combine prompt iteration with evaluation and, if needed, fine-tuning on brand-approved conversation transcripts for best consistency.",
"incorrect_explanations": {
"A": "Lowering token limits reduces length, not tone alignment or stylistic consistency.",
"B": "Batch inference optimizes throughput; it does not shape style or tone.",
"D": "Higher temperature increases randomness and can drift from the target tone."
}
},
{
"id": "aif-c01-ai_services-024",
"certification_id": "AIF-C01",
"domain": "AI_SERVICES",
"difficulty": "easy",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "Using an LLM on Amazon Bedrock for sentiment analysis, how should the prompt be structured to classify snippets as positive or negative?",
"option_a": "Provide examples labeled positive or negative, followed by the new snippet to classify.",
"option_b": "Provide a detailed explanation of sentiment analysis and how LLMs work.",
"option_c": "Provide only the new snippet with no context.",
"option_d": "Provide the new snippet plus examples of unrelated tasks like summarization.",
"correct_answers": ["A"],
"explanation_detailed": "Few-shot prompting with labeled examples establishes the task, format, and decision criteria. Showing positive and negative samples teaches the model the boundary in your domain and improves consistency. In Bedrock, combine this with a clear instruction, output schema, and low temperature for determinism. Explanations of ML theory add no signal. Uncontextualized prompts yield variable outputs. Unrelated tasks confuse the instruction space. For higher accuracy, evaluate with labeled datasets and consider fine-tuning or using Amazon Comprehend for ready-made sentiment when your needs are standard.",
"incorrect_explanations": {
"B": "Theory descriptions do not provide task structure or decision boundaries for the model.",
"C": "Zero-context prompts lead to ambiguous, inconsistent classifications.",
"D": "Irrelevant examples pollute the prompt and degrade task adherence."
}
},
{
"id": "aif-c01-ai_services-025",
"certification_id": "AIF-C01",
"domain": "AI_SERVICES",
"difficulty": "medium",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "A security company runs FMs on Amazon Bedrock and must ensure only authorized users invoke models. They need to identify unauthorized access attempts to refine future IAM policies and roles. Which AWS service should be used?",
"option_a": "AWS Security Hub",
"option_b": "AWS CloudTrail",
"option_c": "Amazon GuardDuty",
"option_d": "AWS Trusted Advisor",
"correct_answers": ["B"],
"explanation_detailed": "AWS CloudTrail records API activity across AWS accounts, including Bedrock InvokeModel calls, failures, and access-denied events. Reviewing CloudTrail logs helps identify which principals attempted unauthorized actions and why (missing permissions, wrong roles, blocked resource policies). You can route events to CloudWatch Logs and create alarms or EventBridge rules for alerts. GuardDuty detects threats at the account/network level, Security Hub aggregates findings, and Trusted Advisor offers best-practice checks. For auditing access attempts to Bedrock specifically, CloudTrail is the authoritative source of evidence.",
"incorrect_explanations": {
"A": "Security Hub aggregates findings; it does not capture raw API access attempts for forensic analysis.",
"C": "GuardDuty flags suspicious behavior but is not the canonical record of Bedrock API calls and denials.",
"D": "Trusted Advisor provides guidance and checks, not detailed per-call access logs."
}
},
{
"id": "aif-c01-ai_services-026",
"certification_id": "AIF-C01",
"domain": "AI_SERVICES",
"difficulty": "medium",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "A company built an image-classification model and wants production predictions for a web app without managing infrastructure. What should they choose?",
"option_a": "Host the model in AWS Lambda.",
"option_b": "Use Amazon SageMaker Serverless Inference.",
"option_c": "Use Amazon API Gateway to host the model.",
"option_d": "Use AWS Fargate to host the model.",
"correct_answers": ["B"],
"explanation_detailed": "Amazon SageMaker Serverless Inference serves models without provisioning instances, automatically scaling based on traffic while charging for compute used during inference. It simplifies deployment for spiky or unpredictable loads and integrates with model registry, CI/CD, and observability. Lambda has short runtime and memory limits and is not ideal for typical ML frameworks. API Gateway fronts APIs; it does not run models. Fargate runs containers but requires you to manage model packaging, autoscaling, and GPU choices. Serverless Inference minimizes ops while preserving ML-specific deployment features.",
"incorrect_explanations": {
"A": "Lambda’s short timeouts and memory constraints limit many ML workloads and model sizes.",
"C": "API Gateway is a routing layer; it cannot execute ML models by itself.",
"D": "Fargate removes server management but still requires container ops and scaling logic for ML serving."
}
},
{
"id": "aif-c01-ai_services-027",
"certification_id": "AIF-C01",
"domain": "AI_SERVICES",
"difficulty": "medium",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "An AI company uses independent software vendors for periodic assessments and needs email notifications when new ISV compliance reports are available. Which AWS service should they use?",
"option_a": "AWS Control Tower",
"option_b": "AWS Artifact",
"option_c": "AWS Trusted Advisor",
"option_d": "AWS Security Hub",
"correct_answers": ["B"],
"explanation_detailed": "AWS Artifact is the portal for on-demand access to compliance reports and agreements from AWS and select ISVs. Teams can retrieve SOC, ISO, PCI, and other attestations centrally. While Artifact itself is a repository, you can integrate notifications via automation (e.g., polling with EventBridge Scheduler/Lambda) to alert stakeholders when new documents appear. Control Tower governs multi-account setups, Trusted Advisor provides optimization and best-practice checks, and Security Hub aggregates security findings. For compliance report access and distribution, Artifact is the primary source of truth.",
"incorrect_explanations": {
"A": "Control Tower manages account baselining and guardrails, not compliance document retrieval.",
"C": "Trusted Advisor focuses on cost, performance, and security checks, not compliance report delivery.",
"D": "Security Hub aggregates security findings; it does not host ISV compliance reports."
}
},
{
"id": "aif-c01-responsible_ai-028",
"certification_id": "AIF-C01",
"domain": "RESPONSIBLE_AI",
"difficulty": "medium",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "A company is building a conversational agent with an LLM and wants to reduce prompt-engineering attacks that coerce harmful or sensitive actions. What helps reduce this risk?",
"option_a": "Create a prompt template that trains the LLM to detect attack patterns.",
"option_b": "Increase the temperature parameter.",
"option_c": "Avoid LLMs not listed in Amazon SageMaker.",
"option_d": "Reduce the number of input tokens.",
"correct_answers": ["A"],
"explanation_detailed": "Defense-in-depth includes robust system prompts with explicit refusal policies, separators between instructions and user content, input/output validation, and attack-pattern detection (e.g., jailbreak cues). In Amazon Bedrock, combine templates with Guardrails for Bedrock to filter unsafe content and restrict tool actions. Temperature and token count do not address adversarial inputs. Model source listing does not guarantee safety against prompt injection. Incorporate retrieval whitelists, schema-validated outputs, and allow-list tool calls through Agents for Bedrock to further limit damage if instructions are subverted.",
"incorrect_explanations": {
"B": "Higher temperature increases randomness and can worsen adherence to safety policies.",
"C": "Model listing does not prevent prompt injection; safety comes from controls around the model.",
"D": "Shorter prompts alone do not neutralize adversarial content embedded by attackers."
}
},
{
"id": "aif-c01-responsible_ai-029",
"certification_id": "AIF-C01",
"domain": "RESPONSIBLE_AI",
"difficulty": "easy",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "Using a Generative AI Security Scope Matrix, which solution scope gives the company the greatest security responsibility?",
"option_a": "Use a third-party app with embedded generative AI features.",
"option_b": "Build an app using a third-party generative AI model (FM).",
"option_c": "Refine a third-party generative AI model (FM) with business data.",
"option_d": "Build and train a generative AI model from scratch with customer data.",
"correct_answers": ["D"],
"explanation_detailed": "Training a model from scratch transfers maximum responsibility to the company: data collection/consent, labeling quality, secure pipelines, model architecture, training infrastructure hardening, evaluation, red-teaming, and deployment controls. In AWS, this spans S3 data governance, SageMaker training security, KMS encryption, VPC isolation, ECR image scanning, and IAM least privilege. Using or fine-tuning third-party FMs shifts many controls to the provider. Embedded AI in SaaS leaves the least responsibility. The more you customize, the more you own the security surface and compliance obligations.",
"incorrect_explanations": {
"A": "Embedded AI in SaaS offloads the most responsibility to the vendor, not the company.",
"B": "Building on a third-party FM reduces responsibility versus training your own model stack.",
"C": "Fine-tuning adds responsibility but still leverages provider controls more than training from scratch."
}
},
{
"id": "aif-c01-ai_services-030",
"certification_id": "AIF-C01",
"domain": "AI_SERVICES",
"difficulty": "easy",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "An AI practitioner has a large photo database of animals and wants to automatically identify and categorize animals in images without manual effort. What strategy fits?",
"option_a": "Object detection",
"option_b": "Anomaly detection",
"option_c": "Named entity recognition",
"option_d": "Inpainting",
"correct_answers": ["A"],
"explanation_detailed": "Object detection localizes and classifies objects within images using bounding boxes or masks. Pretrained models adapted via transfer learning can recognize animal species and count occurrences. On AWS, Amazon Rekognition provides prebuilt labels, while SageMaker supports custom detectors (e.g., YOLO, Detectron2). Anomaly detection targets outliers, NER extracts entities from text, and inpainting fills missing image regions. For automatic categorization with minimal manual intervention, object detection and image classification are the appropriate computer vision approaches.",
"incorrect_explanations": {
"B": "Anomaly detection finds unusual patterns; it does not label known object classes in images.",
"C": "NER is an NLP technique applied to text, not images.",
"D": "Inpainting edits images by filling regions; it does not perform categorization."
}
},
{
"id": "aif-c01-ai_services-031",
"certification_id": "AIF-C01",
"domain": "AI_SERVICES",
"difficulty": "easy",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "A company wants to build with Amazon Bedrock on a limited budget and with flexibility, avoiding long-term commitments. Which pricing model fits?",
"option_a": "On-demand",
"option_b": "Model customization",
"option_c": "Provisioned throughput",
"option_d": "Spot instance",
"correct_answers": ["A"],
"explanation_detailed": "On-demand pricing in Amazon Bedrock charges per request/token without capacity reservations, ideal for experimentation and variable usage. Provisioned throughput reserves capacity for consistent, high-volume workloads at a committed rate. Model customization refers to fine-tuning cost, not a pricing plan for inference. Spot instances are EC2 compute pricing, not applicable to fully managed Bedrock model endpoints. Start on-demand, measure cost and latency, then consider provisioned throughput when traffic stabilizes and SLAs require predictable performance.",
"incorrect_explanations": {
"B": "Customization is an activity (fine-tuning), not a flexible pay-as-you-go inference pricing model.",
"C": "Provisioned throughput requires commitment and is optimized for steady high usage, not budget flexibility.",
"D": "Spot pricing applies to EC2 compute, not Bedrock’s managed model invocation."
}
},
{
"id": "aif-c01-ai_services-032",
"certification_id": "AIF-C01",
"domain": "AI_SERVICES",
"difficulty": "medium",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "Which AWS service helps a development team quickly deploy and consume a foundation model within its VPC?",
"option_a": "Amazon Personalize",
"option_b": "Amazon SageMaker JumpStart",
"option_c": "PartyRock (Amazon Bedrock playground)",
"option_d": "Amazon SageMaker endpoints",
"correct_answers": ["B"],
"explanation_detailed": "Amazon SageMaker JumpStart provides curated model catalogs, solution templates, and 1-click deployments into your account and VPC. Teams can stand up endpoints for foundation models or fine-tune quickly with integrated MLOps patterns. Personalize is a managed recommendation service, PartyRock is a public sandbox not tied to your VPC, and generic SageMaker endpoints require you to bring and configure the model. JumpStart accelerates secure, production-oriented deployments with guardrails, IAM, and observability baked in.",
"incorrect_explanations": {
"A": "Personalize targets recommendation use cases; it does not deploy general FMs into your VPC.",
"C": "PartyRock is a playground for experimentation, not a production VPC deployment path.",
"D": "Plain endpoints need a model and configuration; JumpStart streamlines selection and deployment."
}
},
{
"id": "aif-c01-responsible_ai-033",
"certification_id": "AIF-C01",
"domain": "RESPONSIBLE_AI",
"difficulty": "easy",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "How can companies use LLMs safely on Amazon Bedrock?",
"option_a": "Craft clear prompts and configure IAM roles/policies with least privilege.",
"option_b": "Enable AWS Audit Manager for automatic model assessments.",
"option_c": "Enable automatic model evaluation in Amazon Bedrock.",
"option_d": "Use CloudWatch Logs to make models explainable and monitor bias.",
"correct_answers": ["A"],
"explanation_detailed": "Secure LLM usage combines strong instruction design, least-privilege IAM, encryption, private connectivity (e.g., PrivateLink), and content controls (Guardrails for Bedrock). Clear prompts reduce ambiguity and misuse, while IAM restricts data and tool access to what’s necessary. Audit Manager helps evidence compliance but does not secure the model itself. Bedrock does not auto-assess all risks by default. CloudWatch captures logs and metrics but does not provide explainability or bias controls alone. Defense-in-depth includes prompt policies, retrieval allow-lists, schema-validated outputs, and continuous monitoring.",
"incorrect_explanations": {
"B": "Audit Manager assists with compliance evidence; it is not a direct control for LLM safety or access.",
"C": "There is no single toggle that ensures safe LLM use; you must implement multiple controls.",
"D": "Logs aid observability but do not enforce explainability or fairness guarantees."
}
},
{
"id": "aif-c01-ai_services-034",
"certification_id": "AIF-C01",
"domain": "AI_SERVICES",
"difficulty": "medium",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "A company has terabytes of data and wants an AI app that turns natural-language inputs into SQL for non-technical staff. What fits best?",
"option_a": "Generative Pre-trained Transformers (GPT)",
"option_b": "Residual neural network",
"option_c": "Support vector machine",
"option_d": "WaveNet",
"correct_answers": ["A"],
"explanation_detailed": "GPT-style LLMs excel at code synthesis from natural language, including SQL generation with schema hints and examples. On Amazon Bedrock, you can use foundation models to translate questions into SQL, validate against a schema, and execute in a controlled environment with guardrails. ResNets focus on vision tasks, SVMs are classical ML classifiers, and WaveNet targets audio. Add schema-aware validation, parameterized queries, and role-based access to prevent unsafe operations, and log prompts/results with CloudWatch for auditing.",
"incorrect_explanations": {
"B": "Residual networks are primarily used in computer vision and are not designed for text-to-SQL tasks.",
"C": "SVMs classify feature vectors; they do not generate executable SQL from natural language.",
"D": "WaveNet is an audio waveform generator; it is not a text-to-code model."
}
},
{
"id": "aif-c01-ai_fundamentals-035",
"certification_id": "AIF-C01",
"domain": "AI_FUNDAMENTALS",
"difficulty": "easy",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "A deep-learning object detector is deployed. When the model analyzes a new image to identify objects, what process is occurring?",
"option_a": "Training",
"option_b": "Inference",
"option_c": "Model deployment",
"option_d": "Bias mitigation",
"correct_answers": ["B"],
"explanation_detailed": "Inference is the application phase where a trained model processes new inputs to produce predictions. In production on Amazon SageMaker (real-time, serverless, or async endpoints), the model loads weights, executes forward passes, and returns detections (classes, boxes, scores). Training adjusts weights; deployment provisions serving infrastructure; bias mitigation addresses fairness during data/model prep. Monitoring latency, throughput, and accuracy drift with CloudWatch and Model Monitor ensures inference remains healthy over time.",
"incorrect_explanations": {
"A": "Training updates weights using labeled data; it does not describe using the model on unseen images.",
"C": "Deployment sets up the endpoint; it is not the act of predicting.",
"D": "Bias mitigation is a governance activity, not runtime prediction."
}
},
{
"id": "aif-c01-responsible_ai-036",
"certification_id": "AIF-C01",
"domain": "RESPONSIBLE_AI",
"difficulty": "medium",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "An AI practitioner generates images of people in different professions but finds biased outputs due to skewed input data. Which technique helps?",
"option_a": "Data augmentation for underrepresented classes",
"option_b": "Model monitoring for class distribution",
"option_c": "Retrieval-augmented generation (RAG)",
"option_d": "Watermark detection for images",
"correct_answers": ["A"],
"explanation_detailed": "Bias often stems from underrepresentation. Augmenting minority classes—through additional curated samples or transformations—helps balance the training distribution. Pair this with targeted collection and reweighting to correct skews. In SageMaker, use Clarify to measure pre-/post-training bias and track fairness metrics, then retrain with augmented data. Monitoring class distribution is diagnostic but not corrective, RAG affects text grounding, and watermark detection is unrelated. Balanced data is foundational to fairer generative outputs.",
"incorrect_explanations": {
"B": "Monitoring reveals skew but does not fix it; you must change the data or training objective.",
"C": "RAG improves factual grounding for language models, not visual class balance.",
"D": "Watermark checks provenance, not demographic or occupational representation in outputs."
}
},
{
"id": "aif-c01-ai_services-037",
"certification_id": "AIF-C01",
"domain": "AI_SERVICES",
"difficulty": "medium",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "A company is implementing Amazon Titan (FM) on Amazon Bedrock and must complement the model with relevant private data. What should they do?",
"option_a": "Use a different FM.",
"option_b": "Lower the temperature.",
"option_c": "Create a Bedrock knowledge base.",
"option_d": "Enable model invocation logging.",
"correct_answers": ["C"],
"explanation_detailed": "A Bedrock knowledge base indexes private documents, generates embeddings, and retrieves the most relevant chunks to ground model responses. This improves factuality without fine-tuning and keeps data under your control. Temperature affects randomness, not knowledge. Invocation logging aids observability. Switching models does not inject your domain content. Combine knowledge bases with guardrails, access control (IAM, VPC endpoints), and evaluation to keep answers accurate, contextual, and compliant.",
"incorrect_explanations": {
"A": "Changing models does not add your private knowledge; you need retrieval or fine-tuning.",
"B": "Lower temperature reduces randomness but does not supply domain facts.",
"D": "Logging is helpful for auditing but does not improve content grounding."
}
},
{
"id": "aif-c01-responsible_ai-038",
"certification_id": "AIF-C01",
"domain": "RESPONSIBLE_AI",
"difficulty": "medium",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "A medical company customizes a foundation model for diagnostics and must meet regulatory transparency. What should they use?",
"option_a": "Amazon Inspector for security and compliance setup",
"option_b": "Amazon SageMaker Clarify for metrics, reports, and example-based explanations",
"option_c": "Amazon Macie to encrypt and protect training data",
"option_d": "Collect more data and use Amazon Rekognition Custom Labels",
"correct_answers": ["B"],
"explanation_detailed": "Amazon SageMaker Clarify provides explainability (feature attributions such as SHAP), bias metrics, and reports at training and inference time. These artifacts support model cards and regulatory documentation, showing how inputs influence predictions and whether disparate impact exists. Inspector checks vulnerabilities in resources, Macie discovers sensitive data but does not explain models, and Rekognition relates to vision labeling. In regulated healthcare, combine Clarify with SageMaker Model Cards, data lineage, and human-in-the-loop review for a defensible transparency posture.",
"incorrect_explanations": {
"A": "Inspector targets vulnerability assessments; it does not generate model explanations or fairness metrics.",
"C": "Macie identifies sensitive data exposure; it is not an explainability tool.",
"D": "More data or different services do not replace formal explainability and bias reporting."
}
},
{
"id": "aif-c01-responsible_ai-039",
"certification_id": "AIF-C01",
"domain": "RESPONSIBLE_AI",
"difficulty": "medium",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "A company will deploy a conversational bot (fine-tuned via SageMaker JumpStart) and must demonstrate compliance with multiple regulatory frameworks. Which capabilities help? (Choose two.)",
"option_a": "Auto-scaling inference endpoints",
"option_b": "Threat detection",
"option_c": "Data protection",
"option_d": "Cost optimization",
"option_e": "Loosely coupled microservices",
"correct_answers": ["B", "C"],
"explanation_detailed": "Compliance narratives hinge on security controls: threat detection and data protection. AWS services such as GuardDuty, Security Hub, and CloudTrail help detect and aggregate suspicious activity and provide audit trails. Data protection covers encryption at rest/in transit (KMS, TLS), IAM least privilege, private networking (PrivateLink), and data minimization. Auto-scaling and microservices aid resilience and maintainability; cost optimization affects spend. For regulated chatbots, evidence strong detective controls and robust data-handling policies.",
"incorrect_explanations": {
"A": "Auto-scaling improves availability but does not directly address regulatory controls.",
"D": "Cost efficiency is valuable, but it is not a compliance control.",
"E": "Architecture style helps maintainability; compliance focuses on security and privacy controls."
}
},
{
"id": "aif-c01-ai_fundamentals-040",
"certification_id": "AIF-C01",
"domain": "AI_FUNDAMENTALS",
"difficulty": "easy",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "A team wants to raise a foundation model’s accuracy to an acceptable threshold. What action should they take?",
"option_a": "Reduce batch size.",
"option_b": "Increase the number of epochs.",
"option_c": "Decrease the number of epochs.",
"option_d": "Increase the temperature.",
"correct_answers": ["B"],
"explanation_detailed": "Increasing epochs gives the model more passes over the training set, often improving accuracy until overfitting begins. Monitor validation loss/metrics and apply early stopping. On SageMaker, track experiments and compare runs. Batch size affects optimization dynamics, not necessarily final accuracy. Temperature is an inference-time sampling parameter for generative models and unrelated to training. If gains stall, consider more data, transfer learning, regularization, or hyperparameter tuning via SageMaker Automatic Model Tuning.",
"incorrect_explanations": {
"A": "Batch size influences training dynamics but isn’t a guaranteed lever for accuracy.",
"C": "Fewer epochs reduce learning opportunity and often lower accuracy.",
"D": "Temperature changes output randomness at inference, not training quality."
}
},
{
"id": "aif-c01-ai_services-041",
"certification_id": "AIF-C01",
"domain": "AI_SERVICES",
"difficulty": "easy",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "A call-center wants to reduce the number of actions agents take to answer customer questions by deploying an LLM chatbot. Which business KPI best measures impact?",
"option_a": "Website engagement rate",
"option_b": "Average handle time (AHT)",
"option_c": "Corporate social responsibility",
"option_d": "Regulatory compliance",
"correct_answers": ["B"],
"explanation_detailed": "Average handle time reflects how efficiently customer issues are resolved. An effective chatbot that retrieves answers or automates steps should reduce AHT by pre-resolving or accelerating agent workflows. Website engagement is a marketing metric, CSR relates to ethics/sustainability, and compliance is a governance outcome. Pair AHT with containment rate, first-contact resolution, CSAT, and deflection metrics for a full picture. Use Amazon Bedrock for the LLM and integrate with enterprise systems via Agents for Bedrock.",
"incorrect_explanations": {
"A": "Site engagement is not directly tied to call-center efficiency or agent task reduction.",
"C": "CSR does not measure operational efficiency in support workflows.",
"D": "Compliance is necessary but does not quantify productivity gains from a chatbot."
}
},
{
"id": "aif-c01-ai_services-042",
"certification_id": "AIF-C01",
"domain": "AI_SERVICES",
"difficulty": "easy",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "What capability does Amazon SageMaker Clarify provide?",
"option_a": "Integrates a Retrieval-Augmented Generation (RAG) workflow",
"option_b": "Monitors production ML quality",
"option_c": "Documents critical details about ML models",
"option_d": "Identifies potential bias during data preparation",
"correct_answers": ["D"],
"explanation_detailed": "SageMaker Clarify measures bias in datasets and models and provides explainability via feature attributions (e.g., SHAP) at training and inference time. Reports help teams assess fairness and understand feature influence, supporting responsible AI. Model Monitor handles production drift/quality, not Clarify. Model Cards document models. RAG is a separate retrieval pipeline, often built with Bedrock and vector stores. Use Clarify in preprocessing, training, and post-deployment audits to track equity metrics and explain predictions.",
"incorrect_explanations": {
"A": "RAG involves retrieval and generation; Clarify focuses on bias and explainability, not retrieval.",
"B": "SageMaker Model Monitor tracks drift and quality; Clarify is for bias/explainability.",
"C": "Model Cards provide documentation; Clarify generates bias and explanation artifacts."
}
},
{
"id": "aif-c01-ai_fundamentals-043",
"certification_id": "AIF-C01",
"domain": "AI_FUNDAMENTALS",
"difficulty": "medium",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "A model predicts item prices well on training data but degrades significantly in production. What should the company do?",
"option_a": "Reduce the amount of training data.",
"option_b": "Add more hyperparameters to the model.",
"option_c": "Increase the amount of training data.",
"option_d": "Increase training time.",
"correct_answers": ["C"],
"explanation_detailed": "Generalization improves with more diverse, representative data that matches production conditions. Collect additional samples, reduce label noise, and refresh for seasonality or covariate shift. In SageMaker, retrain with new data and evaluate using holdout sets and backtesting. Adding hyperparameters or blindly training longer rarely fixes overfitting or data drift. Combine with feature engineering, regularization, and Model Monitor to detect drift early and trigger retraining pipelines.",
"incorrect_explanations": {
"A": "Less data often worsens generalization and increases overfitting risk.",
"B": "More hyperparameters increase complexity and may exacerbate overfitting without data improvements.",
"D": "Longer training can overfit further if data is not representative."
}
},
{
"id": "aif-c01-ai_services-044",
"certification_id": "AIF-C01",
"domain": "AI_SERVICES",
"difficulty": "medium",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "An e-commerce company wants to determine sentiment from written product reviews. Which AWS services satisfy this? (Choose two.)",
"option_a": "Amazon Lex",
"option_b": "Amazon Comprehend",
"option_c": "Amazon Polly",
"option_d": "Amazon Bedrock",
"option_e": "Amazon Rekognition",
"correct_answers": ["B", "D"],
"explanation_detailed": "Amazon Comprehend provides out-of-the-box sentiment analysis for text with APIs that return positive/negative/neutral/mixed labels and confidence. For customized tone, domain jargon, or multilingual nuance, an LLM on Amazon Bedrock can be prompted or fine-tuned to classify sentiment or extract rationales. Lex builds chatbots, Polly converts text to speech, and Rekognition analyzes images/video. Combine Comprehend for scale with targeted Bedrock prompts or fine-tuning for brand-specific sentiment rules.",
"incorrect_explanations": {
"A": "Lex orchestrates conversations; it does not analyze the sentiment of text corpora.",
"C": "Polly synthesizes speech from text, unrelated to sentiment classification.",
"E": "Rekognition is for vision tasks, not text sentiment."
}
},
{
"id": "aif-c01-ai_services-045",
"certification_id": "AIF-C01",
"domain": "AI_SERVICES",
"difficulty": "medium",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "A company will build an LLM chat interface for product manuals stored as PDFs. What is the most cost-effective approach?",
"option_a": "Add a single PDF to each user prompt via prompt engineering.",
"option_b": "Add all PDFs to each prompt via prompt engineering.",
"option_c": "Fine-tune a model on all PDFs and use it for prompts.",
"option_d": "Load the PDFs into a Bedrock knowledge base and use it for retrieval at query time.",
"correct_answers": ["D"],
"explanation_detailed": "A Bedrock knowledge base indexes PDFs into embeddings and retrieves only the relevant chunks per query, minimizing token usage and avoiding expensive fine-tuning. Stuffing entire documents into prompts is costly and often exceeds context limits. Fine-tuning for static manuals is unnecessary; retrieval keeps content fresh without retraining. This approach pairs well with guardrails, output schemas, and evaluation, delivering accurate, controlled answers at lower cost.",
"incorrect_explanations": {
"A": "Injecting a whole PDF into each prompt wastes tokens and risks truncation.",
"B": "Stuffing all PDFs is prohibitively costly and exceeds context windows.",
"C": "Fine-tuning is costly and inflexible for documents that change; retrieval is cheaper and fresher."
}
},
{
"id": "aif-c01-responsible_ai-046",
"certification_id": "AIF-C01",
"domain": "RESPONSIBLE_AI",
"difficulty": "medium",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "A social media company will use an LLM for content moderation and wants to evaluate outputs for bias and discrimination with minimal admin effort. Which data source should be used?",
"option_a": "User-generated content",
"option_b": "Moderation logs",
"option_c": "Content moderation guidelines",
"option_d": "Benchmark datasets",
"correct_answers": ["D"],
"explanation_detailed": "Benchmark datasets are pre-curated, standardized, and labeled for fairness/bias testing, enabling rapid, reproducible evaluation without heavy internal labeling. They help compare models and prompts consistently. User content and logs are valuable but require significant cleaning, labeling, and privacy handling. Guidelines inform policy but are not labeled test sets. Use benchmarks first, then validate with masked internal data to ensure alignment with platform norms and legal constraints.",
"incorrect_explanations": {
"A": "Raw user content demands heavy labeling and privacy work before fairness evaluation.",
"B": "Logs reflect past actions, not standardized test sets for bias measurement.",
"C": "Guidelines are policies, not datasets with labels for evaluation."
}
},
{
"id": "aif-c01-ai_services-047",
"certification_id": "AIF-C01",
"domain": "AI_SERVICES",
"difficulty": "easy",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "A marketing team will use a pretrained generative model to create campaign content and must ensure outputs align with brand voice and communication requirements. What should they do?",
"option_a": "Optimize architecture and hyperparameters of the base model.",
"option_b": "Add more layers to increase complexity.",
"option_c": "Craft effective prompts with clear instructions and contextual constraints.",
"option_d": "Pretrain a new generative model on a broad, diverse corpus.",
"correct_answers": ["C"],
"explanation_detailed": "Prompting defines persona, tone, do/don’t lists, style guides, and examples to constrain outputs to brand standards. In Bedrock, use system prompts, few-shot examples, and guardrails to filter disallowed themes. Architecture changes or pretraining are costly and unnecessary for style control. Start with prompts, then consider lightweight fine-tuning on approved brand assets for consistency at scale. Track outputs with human review loops for high-impact campaigns.",
"incorrect_explanations": {
"A": "Tuning base architecture is expensive and not required to enforce style guidelines.",
"B": "Adding layers raises complexity and cost without guaranteeing brand adherence.",
"D": "Pretraining a new model is overkill for brand voice; prompting and fine-tuning are sufficient."
}
},
{
"id": "aif-c01-responsible_ai-048",
"certification_id": "AIF-C01",
"domain": "RESPONSIBLE_AI",
"difficulty": "medium",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "A lending company is building a generative AI solution to offer discounts to new applicants and wants to minimize harmful bias. Which actions should they take? (Choose two.)",
"option_a": "Detect imbalances or disparities in the data.",
"option_b": "Ensure the model runs frequently.",
"option_c": "Evaluate model behavior to provide transparency to stakeholders.",
"option_d": "Use ROUGE to guarantee 100% accuracy.",
"option_e": "Ensure inference time is within accepted limits.",
"correct_answers": ["A", "C"],
"explanation_detailed": "Bias control starts with data audits: identify representation gaps and disparate impact across protected groups. Apply rebalancing, reweighting, or targeted collection. Evaluate behavior with fairness metrics and transparency artifacts (SageMaker Clarify, Model Cards). Frequency of runs, ROUGE (a summarization metric), and latency do not address fairness. Document governance, implement adverse-action explanations where applicable, and maintain monitoring to detect drift in both data and fairness metrics.",
"incorrect_explanations": {
"B": "Running often does not mitigate bias; data quality and evaluation do.",
"D": "ROUGE measures summary overlap; it does not assess fairness or guarantee accuracy.",
"E": "Latency is a performance metric, unrelated to bias control."
}
},
{
"id": "aif-c01-ai_services-049",
"certification_id": "AIF-C01",
"domain": "AI_SERVICES",
"difficulty": "medium",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "A company uses a Bedrock base model to summarize documents but trained a custom model to improve quality. What must they do to use the custom model via Bedrock?",
"option_a": "Purchase provisioned throughput for the custom model.",
"option_b": "Deploy the custom model to an Amazon SageMaker real-time endpoint.",
"option_c": "Register the model in SageMaker Model Registry only.",
"option_d": "Grant access to the custom model directly in Bedrock.",
"correct_answers": ["B"],
"explanation_detailed": "Custom models you train are typically hosted on SageMaker endpoints. Your application can orchestrate calls to Bedrock for base models and to SageMaker for your custom summarizer behind a single API layer. Provisioned throughput applies to Bedrock-managed models. Model Registry aids versioning/governance but does not serve traffic alone. Bedrock cannot automatically host your independently trained model unless integrated through your architecture. Use IAM and VPC endpoints to secure both paths and log inference with CloudWatch.",
"incorrect_explanations": {
"A": "Provisioned throughput is for Bedrock models; it does not host your custom model.",
"C": "Model Registry records versions; it does not create an inference endpoint.",
"D": "Bedrock does not directly host arbitrary external custom models without your deployment."
}
},
{
"id": "aif-c01-ai_services-050",
"certification_id": "AIF-C01",
"domain": "AI_SERVICES",
"difficulty": "medium",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "A company must select a Bedrock model for internal use and identify one that generates responses in employees’ preferred style. What should they do?",
"option_a": "Evaluate models using built-in prompt datasets only.",
"option_b": "Evaluate models with a human workforce and custom prompt datasets.",
"option_c": "Use public model rankings to pick the model.",
"option_d": "Use CloudWatch latency metrics while testing models.",
"correct_answers": ["B"],
"explanation_detailed": "Style alignment is subjective and domain-specific. Build a representative prompt set from real internal use cases and have human reviewers score outputs for tone, clarity, and policy compliance. Automate evaluation where possible, but keep human judgment central. Public leaderboards and latency metrics don’t capture style preferences. In Bedrock, test multiple FMs with the same prompts, apply guardrails, and consider lightweight fine-tuning on internal style guides or examples after selecting a candidate.",
"incorrect_explanations": {
"A": "Built-in prompts are generic and may not reflect your organization’s style needs.",
"C": "Public rankings rarely measure company-specific tone and constraints.",
"D": "Latency measures speed, not stylistic fit or quality."
}
},

{
"id": "aif-c01-ai_services-051",
"certification_id": "AIF-C01",
"domain": "AI_SERVICES",
"difficulty": "medium",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "An enterprise must pick an Amazon Bedrock model for internal use and identify a model that produces responses in the style preferred by employees. What should the company do?",
"option_a": "Evaluate models using built-in prompt datasets.",
"option_b": "Evaluate models with a human workforce and custom prompt datasets.",
"option_c": "Use public model leaderboards to choose the model.",
"option_d": "Use Amazon CloudWatch model invocation latency metrics during tests.",
"correct_answers": ["B"],
"explanation_detailed": "Style, tone, and task fit are subjective and domain-specific. The most reliable way to select an Amazon Bedrock foundation model for an organization’s preferred style is to run a structured human evaluation on prompts and references that reflect real internal use cases. Build a tailored prompt set from production data (sanitized) and ask a representative group of employees to rate outputs on tone, clarity, faithfulness, and policy adherence. Combine side-by-side comparisons and Likert ratings, then analyze inter-rater agreement. This method reveals preference alignment and failure modes that automated metrics and public leaderboards miss. On AWS, you can orchestrate evaluations with Bedrock model endpoints, store results in Amazon S3, and analyze in Amazon QuickSight or SageMaker notebooks.",
"incorrect_explanations": {
"A": "Built-in or generic prompt sets rarely mirror your company’s voice, domain terminology, or compliance constraints. They can screen gross quality but won’t measure alignment to your specific style preferences.",
"C": "Leaderboards optimize for public benchmarks, not your in-house voice, data, or safety policies. A top model on public tasks can still misalign with your brand style or compliance needs.",
"D": "Latency is an operational metric. It helps size and cost the solution but says nothing about stylistic alignment or response quality for your employees."
}
},
{
"id": "aif-c01-responsible_ai-052",
"certification_id": "AIF-C01",
"domain": "RESPONSIBLE_AI",
"difficulty": "easy",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "A college student copies generative-AI content to write essays. Which responsible AI challenge does this represent?",
"option_a": "Toxicity",
"option_b": "Hallucinations",
"option_c": "Plagiarism",
"option_d": "Privacy",
"correct_answers": ["C"],
"explanation_detailed": "Using model-generated text without attribution constitutes plagiarism. Responsible AI practices require clear provenance, citation, and policies that prevent misrepresenting generated content as original work. Educational and enterprise settings should set guidelines that require disclosure when AI tools assisted, and—where applicable—cite sources retrieved via augmented generation. On AWS, Guardrails for Amazon Bedrock can be configured to prompt disclosures and discourage misuse, and Amazon Bedrock logging can record prompts and outputs for audit. Content authenticity solutions and watermark detection (when available) can complement policy. The core issue here is not toxicity or privacy, but misattribution of authorship and failing to maintain academic integrity.",
"incorrect_explanations": {
"A": "Toxicity concerns harmful or offensive output. The scenario focuses on misattribution of authorship, not harmful language.",
"B": "Hallucinations are confident but incorrect outputs. Even if the text were accurate, copying it without attribution is still plagiarism.",
"D": "Privacy issues involve exposure of personal or sensitive data. The central problem here is improper attribution and authorship."
}
},
{
"id": "aif-c01-ai_services-053",
"certification_id": "AIF-C01",
"domain": "AI_SERVICES",
"difficulty": "medium",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "A company must train its own LLM using only private data and is concerned about environmental impact. Which Amazon EC2 instance type has the lowest environmental impact for training LLMs?",
"option_a": "EC2 C-series",
"option_b": "EC2 G-series",
"option_c": "EC2 P-series",
"option_d": "EC2 Trn-series",
"correct_answers": ["D"],
"explanation_detailed": "EC2 Trn instances (Trn1/Trn1n), powered by AWS Trainium, are purpose-built for energy- and cost-efficient training of deep learning models at scale. Relative to general GPU instances, Trainium can deliver higher performance per watt and lower total energy for large transformer workloads by providing higher throughput, mixed-precision acceleration, and optimized collective communications. Trainium integrates with the AWS Neuron SDK, PyTorch/XLA, and popular LLM stacks to reduce code changes while increasing hardware utilization. Using Trn can therefore reduce carbon and cost footprints for long-running LLM training compared to P-series GPUs, while C/G instances are not designed for large-scale training. Combine with managed storage (Amazon S3) and spot where appropriate to further optimize sustainability.",
"incorrect_explanations": {
"A": "C-series are compute-optimized CPUs, suitable for general compute but not efficient for large-scale deep learning training.",
"B": "G-series targets inference/graphics with smaller GPUs and is not optimized for cost- or energy-efficient LLM training.",
"C": "P-series GPUs are strong for training but typically consume more power per equivalent throughput than Trainium for transformer workloads."
}
},
{
"id": "aif-c01-ai_services-054",
"certification_id": "AIF-C01",
"domain": "AI_SERVICES",
"difficulty": "medium",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "A company will build a kid-friendly story generator on Amazon Bedrock and must ensure outputs and topics remain appropriate for children. Which AWS capability satisfies this?",
"option_a": "Amazon Rekognition",
"option_b": "Amazon Bedrock playgrounds",
"option_c": "Guardrails for Amazon Bedrock",
"option_d": "Agents for Amazon Bedrock",
"correct_answers": ["C"],
"explanation_detailed": "Guardrails for Amazon Bedrock provide policy controls to shape and constrain model behavior at inference time. You can define safety categories, disallowed topics, PII handling, and custom blocked word/phrase lists, then apply the guardrail to multiple Bedrock models consistently. This is ideal for a children’s storytelling app where you must filter violent, sexual, or age-inappropriate content and prevent the model from answering prompts outside acceptable scope. Guardrails also support response transformation (e.g., refusal or redaction) and logging for audits. Rekognition solves vision tasks, playgrounds are for experimentation, and Agents orchestrate tools/knowledge but do not enforce safety policies by themselves.",
"incorrect_explanations": {
"A": "Rekognition detects objects, text, and faces in images and video; it does not filter LLM text for age-appropriateness.",
"B": "Playgrounds are for ad-hoc testing; they don’t enforce production safety policies across endpoints.",
"D": "Agents orchestrate actions and tools. Without guardrails, they won’t inherently block unsafe or age-inappropriate content."
}
},
{
"id": "aif-c01-ai_fundamentals-055",
"certification_id": "AIF-C01",
"domain": "AI_FUNDAMENTALS",
"difficulty": "easy",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "A company must generate synthetic data based on existing data. Which model type fits?",
"option_a": "Generative Adversarial Network (GAN)",
"option_b": "XGBoost",
"option_c": "Residual neural network",
"option_d": "WaveNet",
"correct_answers": ["A"],
"explanation_detailed": "GANs pair a generator and a discriminator in a minimax game: the generator produces candidates and the discriminator distinguishes real from synthetic. Over training, the generator learns to produce realistic samples that match the distribution of real data. GANs are widely used to create images, tabular data, and even time series under privacy and imbalance constraints. On AWS, you can prototype in Amazon SageMaker using frameworks such as PyTorch or TensorFlow, track experiments with SageMaker Experiments, and store datasets in Amazon S3. XGBoost is a gradient-boosted decision tree algorithm for supervised tasks, residual networks are discriminative deep nets, and WaveNet targets audio generation, not general synthetic tabular/image data generation.",
"incorrect_explanations": {
"B": "XGBoost excels at classification/regression but does not generate new data distributions.",
"C": "ResNets are primarily discriminative architectures for vision; they are not generative models.",
"D": "WaveNet specializes in audio waveform generation; it’s inappropriate for generic synthetic data across modalities."
}
},
{
"id": "aif-c01-ai_services-056",
"certification_id": "AIF-C01",
"domain": "AI_SERVICES",
"difficulty": "medium",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "A device company wants demand forecasting for memory hardware, has no coding skills, and must analyze internal and external datasets. Which AWS solution fits?",
"option_a": "Store in Amazon S3 and train with built-in SageMaker algorithms from S3.",
"option_b": "Use SageMaker Data Wrangler and built-in algorithms directly.",
"option_c": "Use SageMaker Data Wrangler with Amazon Personalize Trending-Now.",
"option_d": "Use Amazon SageMaker Canvas to build models and forecasts no-code.",
"correct_answers": ["D"],
"explanation_detailed": "Amazon SageMaker Canvas provides a no-code interface for business users to build, evaluate, and deploy ML models using automated feature engineering and algorithm selection. It connects to data sources such as Amazon S3, Redshift, and third-party feeds, allowing users to create demand forecasts and what-if analyses without writing code. Canvas leverages SageMaker Autopilot under the hood and can hand off models to SageMaker Studio for experts to review. Data Wrangler is powerful for data prep but assumes practitioner skills; Personalize is a recommendation service, not a general forecasting engine; and training built-ins directly requires ML expertise and scripting. Canvas best fits the non-developer, multi-source forecasting need described.",
"incorrect_explanations": {
"A": "Training built-in algorithms from S3 needs coding and ML know-how, which the company lacks.",
"B": "Data Wrangler focuses on data preparation, not end-to-end no-code model building for non-experts.",
"C": "Amazon Personalize is for recommendation systems, not general demand forecasting across heterogeneous data."
}
},
{
"id": "aif-c01-responsible_ai-057",
"certification_id": "AIF-C01",
"domain": "RESPONSIBLE_AI",
"difficulty": "medium",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "A security camera model flags shoplifting disproportionately for one ethnic group. Which bias is most likely?",
"option_a": "Measurement bias",
"option_b": "Sampling bias",
"option_c": "Observer bias",
"option_d": "Confirmation bias",
"correct_answers": ["B"],
"explanation_detailed": "Sampling bias occurs when the training data under- or over-represents groups, contexts, or conditions, leading the model to learn skewed decision boundaries. In vision systems, if certain demographics are underrepresented or captured in different lighting, angles, or camera quality, the model may generalize poorly and yield disparate false positives. Responsible AI mitigation includes auditing dataset composition, rebalancing with targeted collection or augmentation, and evaluating using group-wise metrics (e.g., equalized odds, demographic parity). On AWS, you can use Amazon SageMaker Clarify to compute bias metrics pre- and post-training and to generate explainability artifacts. Measurement bias centers on faulty labels/sensors; observer and confirmation bias refer to human judgment and hypothesis-driven selection, respectively.",
"incorrect_explanations": {
"A": "Measurement bias concerns miscalibrated sensors or inconsistent labels, not group under-representation in the dataset.",
"C": "Observer bias arises from human annotators’ subjective judgments, not necessarily from dataset representativeness.",
"D": "Confirmation bias is selecting evidence to confirm a hypothesis. The scenario points to representation imbalance."
}
},
{
"id": "aif-c01-ai_fundamentals-058",
"certification_id": "AIF-C01",
"domain": "AI_FUNDAMENTALS",
"difficulty": "medium",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "A customer-service chatbot should improve by learning from past interactions and online resources. Which learning strategy enables continuous self-improvement?",
"option_a": "Supervised learning with a hand-curated set of good and bad replies",
"option_b": "Reinforcement learning with rewards for positive customer feedback",
"option_c": "Unsupervised learning to cluster similar customer queries",
"option_d": "Supervised learning with a continuously updated FAQ database",
"correct_answers": ["B"],
"explanation_detailed": "Reinforcement learning (RL) optimizes behavior by maximizing cumulative reward. For chatbots, you can design a reward signal from explicit ratings, resolution outcomes, or proxy signals (e.g., reduced handle time). An RL agent explores response strategies, receives feedback, and updates policies to improve future answers. On AWS, you can combine Amazon Bedrock for base LLMs, collect interaction telemetry in Amazon Kinesis/Firehose, and compute rewards with AWS Lambda or SageMaker RL stacks. Supervised fine-tuning on labeled pairs helps initial quality but doesn’t inherently optimize for long-term outcomes. Clustering organizes data but doesn’t optimize actions. Updating FAQs improves knowledge access, yet without a reward framework the model won’t systematically learn policies.",
"incorrect_explanations": {
"A": "Supervised datasets help initial accuracy but don’t optimize long-term conversational strategies via rewards.",
"C": "Clustering helps discover patterns but doesn’t adapt behavior based on feedback or outcomes.",
"D": "Feeding FAQs adds knowledge; it doesn’t define a reward mechanism to drive behavior optimization."
}
},
{
"id": "aif-c01-ai_fundamentals-059",
"certification_id": "AIF-C01",
"domain": "AI_FUNDAMENTALS",
"difficulty": "easy",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "An ML practitioner built a deep learning classifier for material types in images and wants to measure performance. Which metric helps?",
"option_a": "Confusion matrix",
"option_b": "Correlation matrix",
"option_c": "R² score",
"option_d": "Mean squared error (MSE)",
"correct_answers": ["A"],
"explanation_detailed": "A confusion matrix tabulates counts of true positives, false positives, true negatives, and false negatives per class. From it you derive accuracy, precision, recall, F1, and per-class error analysis. This is essential for multi-class vision tasks where aggregate accuracy can hide minority class failures. A correlation matrix measures feature correlations, not classification performance. R² and MSE are regression metrics and do not apply to discrete labels. On AWS, you can compute confusion matrices in Amazon SageMaker notebooks using scikit-learn, store artifacts in Amazon S3, and visualize class-wise errors to guide data collection and augmentation, threshold tuning, or class rebalancing.",
"incorrect_explanations": {
"B": "Correlation matrices reveal relationships among variables, not predictive classification outcomes.",
"C": "R² evaluates regression fit; it is not defined for categorical predictions.",
"D": "MSE is a regression loss and doesn’t directly describe classification error by class."
}
},
{
"id": "aif-c01-ai_services-060",
"certification_id": "AIF-C01",
"domain": "AI_SERVICES",
"difficulty": "medium",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "A chatbot answers natural-language questions with images. The company must ensure it never returns inappropriate images. What solution fits?",
"option_a": "Implement moderation APIs",
"option_b": "Retrain the model on a broad public dataset",
"option_c": "Perform model validation",
"option_d": "Automate user-feedback integration",
"correct_answers": ["A"],
"explanation_detailed": "Content moderation must happen at inference time to enforce policy reliably. Implement moderation APIs for both text prompts (to block unsafe requests) and image outputs (to analyze and filter generated or retrieved media). On AWS, combine Guardrails for Amazon Bedrock for prompt/output policies with Amazon Rekognition’s moderation APIs for image safety classes. You can also maintain deny-lists and add human-in-the-loop review for edge cases using Amazon A2I. Retraining may reduce risk but cannot guarantee compliance. Validation and feedback loops are valuable, yet without runtime moderation you risk policy violations slipping through.",
"incorrect_explanations": {
"B": "Retraining improves averages but cannot guarantee that no unsafe image will be produced or returned.",
"C": "Validation is pre-deployment testing; it does not enforce safety in real-time responses.",
"D": "Feedback improves future behavior but cannot reliably block unsafe content in the moment."
}
},
{
"id": "aif-c01-ai_services-061",
"certification_id": "AIF-C01",
"domain": "AI_SERVICES",
"difficulty": "medium",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "An AI practitioner uses an Amazon Bedrock base model to summarize customer-support chats and wants to store invocation logs to monitor inputs and outputs. What should they do?",
"option_a": "Configure AWS CloudTrail as the model’s log destination.",
"option_b": "Enable model invocation logging in Amazon Bedrock.",
"option_c": "Use AWS Audit Manager as the log destination for the model.",
"option_d": "Configure model invocation logging in Amazon EventBridge.",
"correct_answers": ["B"],
"explanation_detailed": "Amazon Bedrock provides native model invocation logging to capture prompts, responses, and metadata (subject to your retention and privacy policies). You can direct logs to Amazon S3 and integrate with AWS CloudTrail for control-plane auditing, but CloudTrail does not capture payloads. With Bedrock logging, teams can audit usage, debug failure cases, and build evaluation datasets. You should also apply data redaction policies, encryption with AWS KMS, and access controls via IAM. EventBridge is for event routing, not payload logging, and Audit Manager is for audit workflows and evidence collection rather than capturing model input/output content.",
"incorrect_explanations": {
"A": "CloudTrail records API calls and identities, not full model input/output payloads needed for quality monitoring.",
"C": "Audit Manager manages audit programs and evidence; it’s not used to capture inference payloads.",
"D": "EventBridge routes events; it is not a native mechanism to log Bedrock prompts and responses."
}
},
{
"id": "aif-c01-ai_services-062",
"certification_id": "AIF-C01",
"domain": "AI_SERVICES",
"difficulty": "medium",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "A company must run inference over multi-GB archived datasets and does not need immediate predictions. Which SageMaker inference option fits?",
"option_a": "Batch Transform",
"option_b": "Real-time inference",
"option_c": "Serverless inference",
"option_d": "Asynchronous inference",
"correct_answers": ["A"],
"explanation_detailed": "SageMaker Batch Transform processes large datasets asynchronously by reading from Amazon S3, running inference at scale, and writing outputs back to S3. It’s ideal when latency is not critical, instances can be right-sized for throughput, and you avoid managing request/response endpoints. Real-time and serverless endpoints target millisecond-second latencies and request/response workloads. Asynchronous inference is better for large individual payloads that still return per-request results via callback, not full dataset jobs. For multi-GB archives and non-interactive SLAs, Batch Transform minimizes operational overhead and cost.",
"incorrect_explanations": {
"B": "Real-time endpoints are optimized for low-latency per-request inference, not bulk processing of archives.",
"C": "Serverless inference suits sporadic, small payload request/response workloads, not bulk batch scoring.",
"D": "Asynchronous inference handles large single requests but is not optimized for full-dataset batch jobs."
}
},
{
"id": "aif-c01-ai_fundamentals-063",
"certification_id": "AIF-C01",
"domain": "AI_FUNDAMENTALS",
"difficulty": "easy",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "Which term describes numerical representations of real-world objects and concepts that help NLP/AI models understand text?",
"option_a": "Embeddings",
"option_b": "Tokens",
"option_c": "Models",
"option_d": "Binaries",
"correct_answers": ["A"],
"explanation_detailed": "Embeddings map discrete symbols (words, phrases, images) to dense vectors such that semantic similarity corresponds to geometric proximity. They power retrieval, clustering, classification, and semantic search. In LLM applications, text embeddings enable RAG pipelines using vector databases (e.g., OpenSearch k-NN) to find contextually relevant passages. Tokens are the atomic input/output units; models are the architectures/parameters; binaries are compiled artifacts. On AWS, you can compute embeddings with Amazon Bedrock text embedding models and store them in Amazon OpenSearch Service or Aurora PostgreSQL with pgvector to support similarity search at scale.",
"incorrect_explanations": {
"B": "Tokens are units of input/output, not semantic vector representations for similarity computations.",
"C": "A model is the learned function; embeddings are data representations the model uses.",
"D": "Binaries are compiled executables, unrelated to semantic representations."
}
},
{
"id": "aif-c01-ml_development-064",
"certification_id": "AIF-C01",
"domain": "ML_DEVELOPMENT",
"difficulty": "medium",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "A research company used an Amazon Bedrock FM for a Q&A chatbot over scientific articles. Prompt engineering failed due to complex terminology. How can they improve performance?",
"option_a": "Use few-shot prompting to define answer style.",
"option_b": "Apply domain-adaptation fine-tuning to teach scientific terminology.",
"option_c": "Change inference parameters.",
"option_d": "Clean articles to remove complex terms.",
"correct_answers": ["B"],
"explanation_detailed": "Domain adaptation fine-tunes a base FM on domain-specific corpora so the model internalizes specialized vocabulary, syntax, and discourse patterns. For scientific Q&A, fine-tuning on curated articles, glossaries, and expert-authored Q&A pairs can materially improve comprehension and factual grounding. On Amazon Bedrock, use model customization (where supported) with labeled prompt-completion pairs and evaluate via held-out domain benchmarks. Few-shot prompting and parameter tweaks can help style and determinism but rarely close knowledge gaps for highly technical language. Removing complex terms destroys needed signal. Combining fine-tuning with retrieval (Bedrock Knowledge Bases or OpenSearch) yields further gains.",
"incorrect_explanations": {
"A": "Few-shot improves formatting and task induction but cannot teach deep domain vocabulary comprehensively.",
"C": "Temperature, top-p, etc., affect diversity and determinism, not domain knowledge acquisition.",
"D": "Deleting complex terms removes essential information and undermines answer quality."
}
},
{
"id": "aif-c01-ai_fundamentals-065",
"certification_id": "AIF-C01",
"domain": "AI_FUNDAMENTALS",
"difficulty": "medium",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "To make an LLM on Amazon Bedrock produce more consistent answers to the same prompt, which inference parameter change helps most?",
"option_a": "Lower the temperature",
"option_b": "Increase the temperature",
"option_c": "Reduce maximum output tokens",
"option_d": "Increase maximum generation length",
"correct_answers": ["A"],
"explanation_detailed": "Temperature controls sampling randomness. Lower values bias the distribution toward higher-probability tokens, increasing determinism and repeatability of outputs for the same input—useful for classification, extraction, and policy-bound tasks. On Bedrock, pair low temperature with fixed seeds (where available) and constrained decoding when possible. Changing max tokens affects length, not variability; increasing length can even introduce more variation late in generation. Top-p/top-k adjustments also influence randomness, but temperature is the principal knob for global sampling sharpness. Combine with robust prompts and, if needed, guardrails for format enforcement.",
"incorrect_explanations": {
"B": "Higher temperature increases randomness and diversity—opposite of consistency.",
"C": "Limiting tokens truncates outputs but does not fundamentally reduce sampling variance.",
"D": "Longer outputs can introduce additional variability and drift without improving consistency."
}
},
{
"id": "aif-c01-ai_services-066",
"certification_id": "AIF-C01",
"domain": "AI_SERVICES",
"difficulty": "medium",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "A company builds an LLM app on Amazon Bedrock with customer data in S3. Security policy: each team can access only its own customers’ data. What should they do?",
"option_a": "Create a dedicated Amazon Bedrock service role per team scoped only to that team’s S3 data.",
"option_b": "Create a single service role with S3 access and rely on teams to pass a customer name in each request.",
"option_c": "Redact PII in S3 and open S3 access broadly for teams.",
"option_d": "Give Bedrock full S3 access and restrict via team IAM roles to folders.",
"correct_answers": ["A"],
"explanation_detailed": "Use least-privilege IAM with a distinct Amazon Bedrock service role per team. Scope each role’s S3 permissions (resource ARNs and condition keys like aws:PrincipalTag or s3:prefix) to that team’s buckets/prefixes. This ensures Bedrock invocations on behalf of a team can only access that team’s customer data. Passing a customer name is not access control. Broad S3 access violates policy even if data is redacted. Granting Bedrock full S3 access undermines isolation and complicates auditing. Combine with KMS encryption, S3 bucket policies, and CloudTrail for access auditing.",
"incorrect_explanations": {
"B": "Relying on user input is not an authorization boundary and is error-prone.",
"C": "PII redaction does not enforce tenant isolation; broad access still violates policy.",
"D": "Full S3 access to Bedrock breaks least privilege and increases blast radius."
}
},
{
"id": "aif-c01-ai_services-067",
"certification_id": "AIF-C01",
"domain": "AI_SERVICES",
"difficulty": "medium",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "A medical company deployed a disease-detection model on Amazon Bedrock. It must prevent patient PII from appearing in responses and receive notifications on policy violations. What fits?",
"option_a": "Use Amazon Macie to scan outputs and alert on sensitive data.",
"option_b": "Use AWS CloudTrail to monitor responses and alert on PII.",
"option_c": "Use Guardrails for Amazon Bedrock to filter content and CloudWatch alarms for violation notifications.",
"option_d": "Use SageMaker Model Monitor for data drift and quality alerts.",
"correct_answers": ["C"],
"explanation_detailed": "Guardrails for Amazon Bedrock can detect and redact PII or block responses that violate content policies, preventing sensitive data from being returned. Configure guardrails with PII detection, custom deny lists, and safe-response transformations. Stream logs to CloudWatch and set metric filters/alarms for violation events to notify teams. Macie discovers sensitive data at rest, not dynamically in generated outputs. CloudTrail tracks API calls, not response payloads. Model Monitor focuses on drift and data quality for SageMaker endpoints, not Bedrock content filtering. Guardrails + CloudWatch is the correct runtime control and alerting combination.",
"incorrect_explanations": {
"A": "Macie classifies data at rest in S3; it doesn’t filter or redact model outputs in real time.",
"B": "CloudTrail logs control-plane events and lacks output payload inspection for PII.",
"D": "Model Monitor detects drift/quality on SageMaker endpoints, not Bedrock content policy enforcement."
}
},
{
"id": "aif-c01-ai_services-068",
"certification_id": "AIF-C01",
"domain": "AI_SERVICES",
"difficulty": "easy",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "The company must convert many PDF resumes to plain text for automated processing. Which AWS service fits?",
"option_a": "Amazon Textract",
"option_b": "Amazon Personalize",
"option_c": "Amazon Lex",
"option_d": "Amazon Transcribe",
"correct_answers": ["A"],
"explanation_detailed": "Amazon Textract extracts text, forms, tables, and key-value pairs from scanned PDFs and images. It handles variable layouts and returns structured JSON for downstream parsing. For resumes, you can push documents from S3 to Textract via asynchronous jobs, store results back in S3, and index them in OpenSearch or a database. Transcribe converts speech to text, Lex builds chatbots, and Personalize is for recommendations. Textract is purpose-built for document OCR and structure extraction needed in resume processing pipelines.",
"incorrect_explanations": {
"B": "Personalize builds recommendation systems, unrelated to OCR and PDF text extraction.",
"C": "Lex is a conversational interface service; it does not parse documents.",
"D": "Transcribe handles audio-to-text, not image/PDF OCR."
}
},
{
"id": "aif-c01-ai_services-069",
"certification_id": "AIF-C01",
"domain": "AI_SERVICES",
"difficulty": "medium",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "An education provider builds a Q&A app with a generative model and wants the response style to adapt automatically to the user’s age group (provided to the model). What is the lowest-effort solution?",
"option_a": "Fine-tune the model on datasets spanning all supported ages.",
"option_b": "Add a role/system description in the prompt instructing the model on the target age group.",
"option_c": "Use chain-of-thought reasoning to infer the correct style.",
"option_d": "Post-summarize responses shorter for younger users.",
"correct_answers": ["B"],
"explanation_detailed": "Prompt conditioning is the lightest-weight method to adapt tone and complexity. Include role instructions such as: “You are a tutor. Target audience: {age group}. Use vocabulary and examples appropriate for this age.” Provide style exemplars (few-shot) if needed and enforce length constraints. This approach avoids training costs and is easy to iterate. Fine-tuning can help but is excessive for simple style control. Chain-of-thought affects reasoning steps, not guaranteed stylistic adaptation, and may leak internal text. Post-summarization may shorten text but won’t reliably adjust vocabulary, structure, or conceptual load to the user’s age.",
"incorrect_explanations": {
"A": "Fine-tuning is costly and slower to iterate; overkill when simple prompt control suffices.",
"C": "Reasoning traces do not inherently tailor tone or vocabulary to age groups.",
"D": "Shortening alone does not ensure age-appropriate vocabulary or pedagogy."
}
},
{
"id": "aif-c01-ml_development-070",
"certification_id": "AIF-C01",
"domain": "ML_DEVELOPMENT",
"difficulty": "medium",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "How should you assess the accuracy of a foundation model used for image classification?",
"option_a": "Compute total resource cost used by the model.",
"option_b": "Measure accuracy against a predefined benchmark dataset.",
"option_c": "Count the number of layers in the neural network.",
"option_d": "Evaluate color fidelity of processed images.",
"correct_answers": ["B"],
"explanation_detailed": "To evaluate a classifier, use a held-out benchmark dataset with ground truth labels and compute accuracy and class-wise metrics (precision, recall, F1). Benchmarks enable apples-to-apples comparisons across models and versions. For robust evaluation, stratify by sub-populations and conditions (lighting, occlusion) and analyze confusion matrices to find systematic errors. On AWS, store datasets in S3, run evaluations in SageMaker notebooks, and visualize metrics in Amazon QuickSight. Resource cost and architecture size don’t guarantee predictive quality; color fidelity is irrelevant to label accuracy unless the task is explicitly about color reproduction.",
"incorrect_explanations": {
"A": "Cost indicates efficiency, not the model’s predictive correctness on labeled images.",
"C": "Depth alone does not predict accuracy; training, data, and regularization matter.",
"D": "Color fidelity is unrelated unless color is the target label; classification accuracy needs labeled benchmarks."
}
},
{
"id": "aif-c01-responsible_ai-071",
"certification_id": "AIF-C01",
"domain": "RESPONSIBLE_AI",
"difficulty": "medium",
"tier": "FREE",
"required_selection_count": 2,
"active": true,
"question_text": "An accounting firm will deploy an LLM to automate document processing and must proceed responsibly to avoid harm. What should they do? (Choose two.)",
"option_a": "Include fairness metrics in model evaluation.",
"option_b": "Tune the model’s temperature parameter.",
"option_c": "Modify training data to mitigate biases.",
"option_d": "Avoid overfitting on training data.",
"option_e": "Apply prompt-engineering techniques.",
"correct_answers": ["A", "C"],
"explanation_detailed": "Responsible AI requires measuring and mitigating harms. Incorporate fairness metrics (e.g., demographic parity, equalized odds) and group-wise evaluations to detect disparate performance across document types, client segments, or languages. If disparities appear, adjust data sampling, reweight records, or collect targeted data to reduce bias. Temperature tuning and prompt engineering affect style and randomness but do not address structural bias. General overfitting control is good practice but is not specific to responsible-AI risk. On AWS, use SageMaker Clarify for bias analysis and explainability, Bedrock Guardrails for policy enforcement and PII controls, and CloudWatch plus Bedrock logging for auditability.",
"incorrect_explanations": {
"B": "Temperature controls randomness; it does not measure or correct systemic bias.",
"D": "Avoiding overfitting is standard ML hygiene, not a specific fairness/bias mitigation step.",
"E": "Prompting can guide style and format but won’t correct structural data bias by itself."
}
},
{
"id": "aif-c01-ml_development-072",
"certification_id": "AIF-C01",
"domain": "ML_DEVELOPMENT",
"difficulty": "easy",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "A company collected new data, computed a correlation matrix, basic statistics, and visualizations. Which ML pipeline stage is this?",
"option_a": "Data preprocessing",
"option_b": "Feature engineering",
"option_c": "Exploratory Data Analysis (EDA)",
"option_d": "Hyperparameter tuning",
"correct_answers": ["C"],
"explanation_detailed": "Exploratory Data Analysis surfaces structure, distributions, correlations, outliers, and missingness before modeling decisions. EDA informs preprocessing (imputation, scaling), feature engineering (domain-informed transforms), and model selection. On AWS, analysts typically run EDA in SageMaker Studio notebooks with Pandas/Seaborn/Matplotlib, reading data from S3, and may publish summaries to QuickSight for stakeholders. Preprocessing and feature engineering are follow-on steps guided by EDA findings; hyperparameter tuning comes after you define a modeling approach and features.",
"incorrect_explanations": {
"A": "Preprocessing applies transformations; EDA precedes and informs those choices.",
"B": "Feature engineering creates/Transforms variables after understanding data via EDA.",
"D": "Tuning occurs post-model selection; not at the data exploration stage."
}
},
{
"id": "aif-c01-ai_fundamentals-073",
"certification_id": "AIF-C01",
"domain": "AI_FUNDAMENTALS",
"difficulty": "medium",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "Some documents lost words due to a database error. The company wants a model to suggest likely words to fill the gaps. Which model type fits?",
"option_a": "Topic modeling",
"option_b": "Clustering models",
"option_c": "Prescriptive ML models",
"option_d": "BERT-based models",
"correct_answers": ["D"],
"explanation_detailed": "BERT and similar masked-language models are trained to reconstruct missing tokens given both left and right context. This bidirectional conditioning yields strong performance on fill-in-the-blank tasks. For production, you can deploy a BERT variant on SageMaker or use a Bedrock FM with masking prompts. Topic models and clustering are unsupervised and don’t predict specific missing words. Prescriptive ML refers to decision optimization, not token reconstruction. Ensure privacy by stripping PII before inference and log predictions to S3 with versioning to audit edits.",
"incorrect_explanations": {
"A": "Topic modeling uncovers latent themes, not precise token prediction for gaps.",
"B": "Clustering groups similar items; it doesn’t infer exact missing words.",
"C": "Prescriptive analytics optimize actions, not text completion."
}
},
{
"id": "aif-c01-ai_services-074",
"certification_id": "AIF-C01",
"domain": "AI_SERVICES",
"difficulty": "medium",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "A company wants automated charts of total sales for top products across retail locations for the last 12 months. Which AWS solution should they use?",
"option_a": "Amazon Q on Amazon EC2",
"option_b": "Amazon Q Developer",
"option_c": "Amazon Q in Amazon QuickSight",
"option_d": "Amazon Q in AWS Chatbot",
"correct_answers": ["C"],
"explanation_detailed": "Amazon QuickSight with Amazon Q provides natural-language-driven BI. Analysts can ask, “Show last 12 months’ sales for top products by location,” and Q builds visuals from governed semantic models. Schedule dashboards, set row-level security, and share insights. Q Developer focuses on code and software tasks; EC2 is unnecessary for managed BI; AWS Chatbot integrates chat with AWS ops, not BI visualizations. For sales analytics across stores, QuickSight + Q is the correct managed analytics solution on AWS.",
"incorrect_explanations": {
"A": "Running Q on EC2 is unnecessary; QuickSight is the managed BI service with Q built in.",
"B": "Q Developer targets developer workflows, not BI chart generation for business users.",
"D": "AWS Chatbot bridges chat tools with AWS events; it does not build BI dashboards."
}
},
{
"id": "aif-c01-ml_development-075",
"certification_id": "AIF-C01",
"domain": "ML_DEVELOPMENT",
"difficulty": "medium",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "A company uses an Amazon Bedrock LLM for intent detection in a chatbot and wants to apply few-shot learning. What additional data do they need?",
"option_a": "Chatbot replies paired with users’ correct intents",
"option_b": "User messages paired with correct chatbot replies",
"option_c": "User messages paired with correct user intents",
"option_d": "User intents paired with correct chatbot replies",
"correct_answers": ["C"],
"explanation_detailed": "Few-shot prompting teaches the model by example. For intent classification, provide user utterances labeled with the correct intent names. The LLM then generalizes from the examples to new utterances. Replies are irrelevant to training the intent labeler and may confound the task. On AWS, store labeled examples in S3, load them into your Bedrock prompt templates, and evaluate performance on a held-out labeled set. If volume grows, consider fine-tuning or a small supervised classifier served on SageMaker.",
"incorrect_explanations": {
"A": "Responses do not define the true intent; you need utterance→intent pairs.",
"B": "Pairing with replies frames a generative task, not intent classification labels.",
"D": "Intent→reply pairs are useful for response selection, not intent detection."
}
},
{
"id": "aif-c01-ai_services-076",
"certification_id": "AIF-C01",
"domain": "AI_SERVICES",
"difficulty": "medium",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "A Bedrock base model uses 10 few-shot examples in the prompt, is invoked once per day, performs well, and the company wants to reduce monthly cost. What should they do?",
"option_a": "Customize the model via fine-tuning.",
"option_b": "Reduce the number of tokens in the prompt.",
"option_c": "Increase the number of tokens in the prompt.",
"option_d": "Use Provisioned Throughput.",
"correct_answers": ["B"],
"explanation_detailed": "Bedrock pricing is token-based. Reducing prompt tokens—by compressing instructions, abbreviating examples, or replacing some with a compact schema—lowers cost without changing infra. One daily invocation doesn’t justify Provisioned Throughput. Fine-tuning may reduce prompt length but adds training cost and operational complexity. Increasing tokens raises cost. Maintain quality by selecting the most informative few-shot examples and enforcing concise formatting.",
"incorrect_explanations": {
"A": "Fine-tuning incurs training cost and overhead; unnecessary for a single daily call.",
"C": "More tokens directly increase cost without proven quality gains here.",
"D": "Provisioned Throughput is for sustained, high-volume workloads—not one call/day."
}
},
{
"id": "aif-c01-ai_fundamentals-077",
"certification_id": "AIF-C01",
"domain": "AI_FUNDAMENTALS",
"difficulty": "easy",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "An LLM generates plausible but factually incorrect marketing copy. What problem is this?",
"option_a": "Data leakage",
"option_b": "Hallucination",
"option_c": "Overfitting",
"option_d": "Underfitting",
"correct_answers": ["B"],
"explanation_detailed": "Hallucination is when a model produces confident but false statements. It arises from gaps in training data, distribution shift, or unconstrained generation. Mitigation: add retrieval-augmented generation (Amazon Bedrock Knowledge Bases or OpenSearch), lower temperature for deterministic tasks, constrain outputs with schemas, and add post-hoc verification. Overfitting/underfitting refer to training dynamics, not inference factuality; data leakage is unintended train/test contamination. Logging prompts/outputs for audits and adding guardrails improve reliability in production.",
"incorrect_explanations": {
"A": "Data leakage concerns training with test or sensitive data, not plausible but wrong outputs.",
"C": "Overfitting is a training issue; it doesn’t directly explain fabricated facts at inference.",
"D": "Underfitting is under-learning patterns, not fabricating believable details."
}
},
{
"id": "aif-c01-responsible_ai-078",
"certification_id": "AIF-C01",
"domain": "RESPONSIBLE_AI",
"difficulty": "medium",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "A practitioner fine-tuned a Bedrock model with sensitive data and wants to ensure inferences don’t expose those sensitive elements. How should they prevent exposure?",
"option_a": "Delete the model, remove sensitive data from training, and retrain.",
"option_b": "Dynamically mask sensitive data in inference responses.",
"option_c": "Encrypt sensitive data in inference responses with SageMaker.",
"option_d": "Encrypt sensitive data inside the model with AWS KMS.",
"correct_answers": ["B"],
"explanation_detailed": "Apply runtime PII detection and redaction to outputs. With Guardrails for Amazon Bedrock, configure PII policies to redact names, addresses, IDs, and custom patterns. You can also integrate Amazon Comprehend PII detection or custom regex to mask outputs before returning them to clients. This prevents accidental disclosure without retraining. Encryption protects data at rest/in transit but doesn’t stop the model from generating sensitive content. Full retraining may be ideal but is costly; runtime masking provides immediate protection and auditability.",
"incorrect_explanations": {
"A": "Retraining may help but is expensive and slow; runtime controls protect immediately.",
"C": "Encrypting responses still returns sensitive content to the caller once decrypted; it doesn’t prevent disclosure.",
"D": "KMS secures stored artifacts, not the semantic content the model chooses to emit."
}
},
{
"id": "aif-c01-ai_fundamentals-079",
"certification_id": "AIF-C01",
"domain": "AI_FUNDAMENTALS",
"difficulty": "medium",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "A company translates training manuals from English to other languages with LLMs and wants to evaluate translation accuracy. Which metric fits?",
"option_a": "BLEU (Bilingual Evaluation Understudy)",
"option_b": "RMSE",
"option_c": "ROUGE",
"option_d": "F1 score",
"correct_answers": ["A"],
"explanation_detailed": "BLEU measures n-gram overlap between a candidate translation and one or more human references, capturing precision of translated segments. It’s a standard automatic metric for machine translation evaluation. ROUGE targets summarization recall, RMSE is a regression error metric, and F1 is for classification. For robust assessment, pair BLEU with human evaluation for fluency and adequacy, and track domain terms with custom glossaries. On AWS, store references in S3, compute BLEU in SageMaker notebooks, and visualize trends in QuickSight.",
"incorrect_explanations": {
"B": "RMSE evaluates numeric predictions, not language translation quality.",
"C": "ROUGE focuses on recall overlap for summarization, not translation fidelity.",
"D": "F1 applies to classification tasks; translation needs sequence-level metrics like BLEU."
}
},
{
"id": "aif-c01-ai_services-080",
"certification_id": "AIF-C01",
"domain": "AI_SERVICES",
"difficulty": "medium",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "A large retailer receives thousands of product support queries daily and wants to use Agents for Amazon Bedrock. What key benefit helps here?",
"option_a": "Automatically generate custom FMs to predict customer needs",
"option_b": "Automate repetitive tasks and orchestrate complex workflows",
"option_c": "Automatically call multiple FMs and consolidate outputs",
"option_d": "Select the best FM by predefined metrics",
"correct_answers": ["B"],
"explanation_detailed": "Agents for Amazon Bedrock break tasks into steps, call tools/APIs, maintain memory, and orchestrate multi-step workflows—ideal for high-volume support use cases. An agent can authenticate users, look up order status via APIs, generate summaries, and escalate when needed, reducing handle time. They don’t create custom FMs or select FMs automatically, though they can route requests per your logic. For moderation and safety, pair agents with Guardrails; log traces to CloudWatch and S3 for observability.",
"incorrect_explanations": {
"A": "Agents orchestrate workflows; they do not train or auto-generate custom foundation models.",
"C": "Agents can call tools/models, but the core value is workflow orchestration, not multi-FM consolidation alone.",
"D": "Model selection is up to your routing logic; agents don’t autonomously choose the best FM by metrics."
}
},
{
"id": "aif-c01-ml_development-081",
"certification_id": "AIF-C01",
"domain": "ML_DEVELOPMENT",
"difficulty": "medium",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "What is a benefit of continual pre-training when adapting a foundation model?",
"option_a": "Decreases model architectural complexity",
"option_b": "Improves model performance over time",
"option_c": "Reduces training time requirement",
"option_d": "Optimizes model inference latency",
"correct_answers": ["B"],
"explanation_detailed": "Continual pre-training exposes the model to new corpora so it internalizes fresh terminology, styles, and facts, reducing distribution shift over time. This can improve downstream task performance, especially when paired with periodic evaluation and safety audits. It does not inherently simplify the architecture, shorten training time, or speed up inference. On AWS, you can run scheduled training on Trn or P-series instances, store datasets in S3 with versioning, track experiments in SageMaker, and validate safety with Clarify and Guardrails before promotion.",
"incorrect_explanations": {
"A": "Continual pre-training changes parameters, not architecture complexity.",
"C": "It usually adds training time since you train on more data over time.",
"D": "Inference latency depends on deployment hardware/architecture, not on continual pre-training."
}
},
{
"id": "aif-c01-ai_fundamentals-082",
"certification_id": "AIF-C01",
"domain": "AI_FUNDAMENTALS",
"difficulty": "easy",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "In generative AI, what are tokens?",
"option_a": "Basic input/output units the model operates on—words, subwords, or other linguistic units",
"option_b": "Mathematical representations of words used by models",
"option_c": "Pretrained weights adjusted for specific tasks",
"option_d": "Prompts or instructions provided to the model",
"correct_answers": ["A"],
"explanation_detailed": "Tokens are the atomic units processed by language models. Tokenization splits text into words, subwords, or characters, allowing models to handle diverse vocabularies efficiently. Costs, context windows, and rate limits are measured in tokens. Embeddings (vectors) can represent tokens semantically, but tokens themselves are the discrete symbols. Weights are model parameters. Prompts are token sequences provided as input. On AWS with Amazon Bedrock, pricing and limits are token-based; understanding tokenization helps control cost and ensure prompts fit within context windows.",
"incorrect_explanations": {
"B": "That describes embeddings; tokens are the discrete units before embedding.",
"C": "Weights are parameters, not the input/output units of text processing.",
"D": "Prompts are composed of tokens but are not tokens themselves."
}
},
{
"id": "aif-c01-ai_services-083",
"certification_id": "AIF-C01",
"domain": "AI_SERVICES",
"difficulty": "easy",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "A company wants to estimate inference costs when using Amazon Bedrock to build generative applications. Which factor most influences the cost?",
"option_a": "Number of tokens consumed",
"option_b": "Temperature value",
"option_c": "Amount of data used to train the LLM",
"option_d": "Total training time",
"correct_answers": ["A"],
"explanation_detailed": "Bedrock inference pricing is primarily based on tokens processed—both prompt and completion. Reducing prompt verbosity, pruning few-shot examples, and capping max tokens can substantially cut cost. Temperature affects randomness, not pricing. Training data size and time apply to model training, not pay-as-you-go inference with managed FMs. Monitor token usage with Bedrock logging and CloudWatch metrics, and batch requests or cache results when possible to optimize spend.",
"incorrect_explanations": {
"B": "Temperature shapes output diversity, not billing.",
"C": "Training data volume is irrelevant to on-demand inference pricing.",
"D": "Training time affects custom training cost, not Bedrock model usage fees."
}
},
{
"id": "aif-c01-ai_services-084",
"certification_id": "AIF-C01",
"domain": "AI_SERVICES",
"difficulty": "medium",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "A team uses Amazon SageMaker Studio notebooks with data in S3 and must manage data flow securely from S3 to Studio. What should they configure?",
"option_a": "Amazon Inspector to monitor SageMaker Studio",
"option_b": "Amazon Macie to monitor SageMaker Studio",
"option_c": "SageMaker in a VPC with an S3 VPC endpoint",
"option_d": "SageMaker with S3 Glacier Deep Archive",
"correct_answers": ["C"],
"explanation_detailed": "Place SageMaker Studio in a VPC and configure an S3 Gateway (or Interface) VPC Endpoint to route traffic privately to S3 without traversing the public internet. Add security groups, IAM roles with least privilege, and KMS encryption for data at rest. Inspector and Macie address vulnerability and sensitive data discovery respectively, not network path control. Glacier Deep Archive is cold storage and unrelated to Studio data access. VPC endpoints enforce secure, controlled, and auditable data movement for notebooks.",
"incorrect_explanations": {
"A": "Inspector scans for vulnerabilities; it does not control Studio↔S3 network paths.",
"B": "Macie helps discover sensitive data; it doesn’t provide private connectivity.",
"D": "Glacier Deep Archive is archival storage, not a connectivity/security configuration."
}
},
{
"id": "aif-c01-ai_fundamentals-086",
"certification_id": "AIF-C01",
"domain": "AI_FUNDAMENTALS",
"difficulty": "easy",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "What is the primary difference between AI and ML?",
"option_a": "AI is a subset of ML",
"option_b": "ML is a subset of AI",
"option_c": "They are completely unrelated fields",
"option_d": "AI and ML are the same thing",
"correct_answers": ["B"],
"explanation_detailed": "Artificial Intelligence is the broad field of building systems that perform tasks requiring human-like intelligence. Machine Learning is a subfield of AI that learns patterns from data to improve task performance without explicit rule programming. DL (deep learning) is a further subset using neural networks with many layers. On AWS, Amazon SageMaker provides tools to develop, train, and deploy ML; Bedrock exposes foundation models (an ML application) for generative AI; higher-level services like Comprehend, Rekognition, and Transcribe are AI services powered by ML under the hood.",
"incorrect_explanations": {
"A": "It is the inverse: ML is a subset of AI, not AI a subset of ML.",
"C": "They are tightly related; ML is a core approach within AI.",
"D": "AI is broader; ML is one approach, so they are not identical."
}
},
{
"id": "aif-c01-ai_fundamentals-087",
"certification_id": "AIF-C01",
"domain": "AI_FUNDAMENTALS",
"difficulty": "easy",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "Which is NOT a standard type of machine learning?",
"option_a": "Supervised learning",
"option_b": "Unsupervised learning",
"option_c": "Reinforcement learning",
"option_d": "Diagnostic learning",
"correct_answers": ["D"],
"explanation_detailed": "The canonical categories are supervised, unsupervised, and reinforcement learning. Supervised uses labeled examples; unsupervised finds structure in unlabeled data; reinforcement learns via rewards in an environment. “Diagnostic learning” is not a standard ML category, though models can assist diagnosis. On AWS, SageMaker supports all three through built-ins, frameworks, and RL toolkits.",
"incorrect_explanations": {
"A": "Supervised learning is a core ML paradigm using labeled data.",
"B": "Unsupervised learning is standard for clustering, dimensionality reduction, and discovery.",
"C": "Reinforcement learning is a major paradigm for sequential decision-making."
}
},
{
"id": "aif-c01-ai_fundamentals-088",
"certification_id": "AIF-C01",
"domain": "AI_FUNDAMENTALS",
"difficulty": "easy",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "Which data type best suits training a computer vision model?",
"option_a": "Tabular data",
"option_b": "Time-series data",
"option_c": "Image data",
"option_d": "Text data",
"correct_answers": ["C"],
"explanation_detailed": "Computer vision models learn from images or video frames. They require pixel arrays with labels for tasks like classification, detection, or segmentation. Time-series and tabular data power forecasting or structured ML tasks, and text suits NLP. On AWS, use SageMaker built-ins (e.g., image classification) or frameworks (PyTorch, TensorFlow), store images in S3, and optimize pipelines with SageMaker Processing and Training jobs.",
"incorrect_explanations": {
"A": "Tabular data is for structured ML, not visual perception tasks.",
"B": "Time-series is suited to forecasting and signal analysis, not image vision directly.",
"D": "Text is for NLP tasks; computer vision consumes images/videos."
}
},
{
"id": "aif-c01-ai_services-089",
"certification_id": "AIF-C01",
"domain": "AI_SERVICES",
"difficulty": "easy",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "Which AWS service is best for NLP tasks like sentiment, entity extraction, and key phrases?",
"option_a": "Amazon SageMaker",
"option_b": "Amazon Comprehend",
"option_c": "Amazon Polly",
"option_d": "Amazon Transcribe",
"correct_answers": ["B"],
"explanation_detailed": "Amazon Comprehend is a managed NLP service for sentiment analysis, entity recognition, key phrases, and topic modeling out of the box. It supports custom classification and entity models. SageMaker is a general ML platform; Polly converts text to speech; Transcribe converts speech to text. Comprehend speeds time to value for common NLP tasks without building custom models.",
"incorrect_explanations": {
"A": "SageMaker can build NLP models but requires ML engineering; Comprehend is turnkey.",
"C": "Polly is TTS, not text understanding.",
"D": "Transcribe is speech-to-text, not NLP analysis."
}
},
{
"id": "aif-c01-ml_development-090",
"certification_id": "AIF-C01",
"domain": "ML_DEVELOPMENT",
"difficulty": "easy",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "What is the primary goal of Exploratory Data Analysis (EDA) in ML development?",
"option_a": "Train the model",
"option_b": "Deploy the model",
"option_c": "Understand the data’s characteristics",
"option_d": "Monitor the model in production",
"correct_answers": ["C"],
"explanation_detailed": "EDA helps you understand distributions, correlations, missingness, outliers, and potential data leakage before modeling. Insights from EDA inform preprocessing, feature engineering, and model choice. On AWS, run EDA in SageMaker Studio with Pandas/Matplotlib, storing datasets in S3, and communicate findings via QuickSight dashboards.",
"incorrect_explanations": {
"A": "Model training happens after you understand and prepare data via EDA.",
"B": "Deployment occurs after training and evaluation, not during EDA.",
"D": "Monitoring is a production concern, not an analysis-stage task."
}
},
{
"id": "aif-c01-ml_development-091",
"certification_id": "AIF-C01",
"domain": "ML_DEVELOPMENT",
"difficulty": "easy",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "Which is NOT a typical step in an ML pipeline?",
"option_a": "Data collection",
"option_b": "Feature engineering",
"option_c": "Model training",
"option_d": "Customer acquisition",
"correct_answers": ["D"],
"explanation_detailed": "ML pipelines focus on data operations, modeling, evaluation, and deployment. Customer acquisition is a business process, not a technical pipeline step. On AWS, pipelines may use S3 for data, SageMaker Processing/Training, Model Registry for versions, and CI/CD via SageMaker Pipelines or CodePipeline.",
"incorrect_explanations": {
"A": "Collecting data is fundamental to ML pipelines.",
"B": "Feature engineering shapes inputs and is a core step.",
"C": "Training is the central modeling stage."
}
},
{
"id": "aif-c01-ai_fundamentals-092",
"certification_id": "AIF-C01",
"domain": "AI_FUNDAMENTALS",
"difficulty": "easy",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "In model performance metrics, what does AUC stand for?",
"option_a": "Average User Cost",
"option_b": "Area Under the Curve",
"option_c": "Automated Universal Calculation",
"option_d": "Augmented Use Case",
"correct_answers": ["B"],
"explanation_detailed": "AUC—Area Under the ROC Curve—measures a classifier’s ability to rank positives above negatives across thresholds. It summarizes trade-offs between true-positive and false-positive rates. AUC is threshold-independent and useful for imbalanced datasets. On AWS, compute AUC in SageMaker notebooks (scikit-learn) and track it in Model Registry alongside precision/recall and calibration plots.",
"incorrect_explanations": {
"A": "Not a standard ML metric acronym.",
"C": "Not an ML performance term.",
"D": "Not related to classification performance."
}
},
{
"id": "aif-c01-ai_fundamentals-093",
"certification_id": "AIF-C01",
"domain": "AI_FUNDAMENTALS",
"difficulty": "easy",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "Which learning type is most appropriate when you have a large labeled dataset?",
"option_a": "Unsupervised learning",
"option_b": "Reinforcement learning",
"option_c": "Supervised learning",
"option_d": "Semi-supervised learning",
"correct_answers": ["C"],
"explanation_detailed": "Supervised learning maps inputs to outputs using labeled examples and performs best when abundant labeled data exists. It underpins classification and regression tasks. Unsupervised learning discovers structure without labels; reinforcement learning optimizes actions via rewards; semi-supervised uses limited labels plus unlabeled data. On AWS, train supervised models with SageMaker built-ins (XGBoost) or frameworks.",
"incorrect_explanations": {
"A": "Unsupervised lacks labels and won’t exploit your labeled dataset.",
"B": "RL is for sequential decisions and rewards, not fixed input-label pairs.",
"D": "Semi-supervised is for scarce labels; you already have many."
}
},
{
"id": "aif-c01-ai_fundamentals-094",
"certification_id": "AIF-C01",
"domain": "AI_FUNDAMENTALS",
"difficulty": "easy",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "What is the main advantage of using pre-trained models?",
"option_a": "They always outperform custom models",
"option_b": "They require fewer compute resources to train",
"option_c": "They are always more accurate",
"option_d": "They can be used immediately without additional training",
"correct_answers": ["D"],
"explanation_detailed": "Pre-trained models already encode linguistic or visual knowledge learned from large corpora, enabling zero-shot and few-shot use and rapid prototyping. You can often deploy them directly or lightly adapt them via prompt engineering or fine-tuning. They don’t guarantee higher accuracy than tailored models, but they reduce time-to-value. On AWS, use Amazon Bedrock for managed access to pre-trained FMs and SageMaker JumpStart for pre-built models and notebooks.",
"incorrect_explanations": {
"A": "They don’t always outperform domain-specific custom models.",
"B": "They still require compute if you fine-tune; the advantage is starting from a trained baseline.",
"C": "Accuracy depends on domain and data; not guaranteed."
}
},
{
"id": "aif-c01-ai_services-095",
"certification_id": "AIF-C01",
"domain": "AI_SERVICES",
"difficulty": "easy",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "Which AWS service best automates finding strong hyperparameters for a model?",
"option_a": "Amazon SageMaker Autopilot",
"option_b": "Amazon Comprehend",
"option_c": "Amazon Polly",
"option_d": "Amazon Transcribe",
"correct_answers": ["A"],
"explanation_detailed": "SageMaker Autopilot automates feature preprocessing, algorithm selection, and hyperparameter tuning to produce strong baselines with minimal code. It leverages Bayesian or grid/random search under the hood (and you can also use SageMaker Hyperparameter Tuning Jobs directly). Comprehend is NLP, Polly is TTS, Transcribe is STT. Autopilot reduces manual iteration and logs candidates for inspection and deployment via Model Registry.",
"incorrect_explanations": {
"B": "Comprehend performs NLP analysis; it doesn’t tune arbitrary models.",
"C": "Polly synthesizes speech, unrelated to tuning ML models.",
"D": "Transcribe converts audio to text, not model optimization."
}
},
{
"id": "aif-c01-ml_development-096",
"certification_id": "AIF-C01",
"domain": "ML_DEVELOPMENT",
"difficulty": "easy",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "What does MLOps stand for?",
"option_a": "Machine Learning Operations",
"option_b": "Multiple Learning Optimizations",
"option_c": "Model Learning Objectives",
"option_d": "Managed Learning Outputs",
"correct_answers": ["A"],
"explanation_detailed": "MLOps is the practice of reliably building, deploying, monitoring, and governing ML systems in production. It covers CI/CD for models, data/version management, automated testing, bias/safety checks, and observability. On AWS, use SageMaker Pipelines, Model Registry, Clarify, Model Monitor, and CI/CD via CodePipeline/CodeBuild to implement MLOps at scale.",
"incorrect_explanations": {
"B": "Not a standard expansion; MLOps refers to operational practices, not optimizations only.",
"C": "Objectives are part of ML design, not the definition of MLOps.",
"D": "Outputs management is a subset; MLOps is broader lifecycle operations."
}
},
{
"id": "aif-c01-ml_development-097",
"certification_id": "AIF-C01",
"domain": "ML_DEVELOPMENT",
"difficulty": "easy",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "Which is NOT a typical business metric to evaluate ML systems?",
"option_a": "Cost per user",
"option_b": "Development costs",
"option_c": "Customer feedback",
"option_d": "F1 score",
"correct_answers": ["D"],
"explanation_detailed": "F1 is a technical metric reflecting precision-recall balance. Business metrics gauge commercial impact, costs, and user sentiment. Aligning technical metrics with business KPIs is crucial for value delivery. On AWS, tie CloudWatch model metrics to QuickSight dashboards that include cost and customer outcomes.",
"incorrect_explanations": {
"A": "Unit economics like cost per user are core business KPIs.",
"B": "Development costs affect ROI and are tracked by product teams.",
"C": "User feedback measures satisfaction and value, key to success."
}
},
{
"id": "aif-c01-ai_fundamentals-098",
"certification_id": "AIF-C01",
"domain": "AI_FUNDAMENTALS",
"difficulty": "easy",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "Which learning paradigm is best when an agent must learn from interactions with an environment?",
"option_a": "Supervised learning",
"option_b": "Unsupervised learning",
"option_c": "Reinforcement learning",
"option_d": "Transfer learning",
"correct_answers": ["C"],
"explanation_detailed": "Reinforcement learning learns policies that maximize cumulative reward via trial and error. It suits control, recommendation with long-term value, and dialogue management. Supervised learning requires labeled input/output pairs; unsupervised finds structure; transfer adapts knowledge across tasks. On AWS, you can implement RL in SageMaker or leverage simulation on AWS RoboMaker for robotics scenarios.",
"incorrect_explanations": {
"A": "Supervised learning uses static labels, not interactive feedback loops.",
"B": "Unsupervised learning discovers patterns without rewards or actions.",
"D": "Transfer learning reuses knowledge but is orthogonal to interaction-driven learning."
}
},
{
"id": "aif-c01-ai_services-099",
"certification_id": "AIF-C01",
"domain": "AI_SERVICES",
"difficulty": "easy",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "Which AWS service converts text to lifelike speech?",
"option_a": "Amazon Comprehend",
"option_b": "Amazon Translate",
"option_c": "Amazon Transcribe",
"option_d": "Amazon Polly",
"correct_answers": ["D"],
"explanation_detailed": "Amazon Polly is a neural text-to-speech service providing natural voices and SSML control over prosody, pronunciation, and emphasis. It supports multiple languages and can stream audio for low-latency applications. Comprehend performs NLP analysis, Translate handles language translation, and Transcribe converts speech to text. Polly is the correct TTS solution on AWS.",
"incorrect_explanations": {
"A": "Comprehend analyzes text but does not synthesize speech.",
"B": "Translate converts text between languages, not to audio.",
"C": "Transcribe is speech-to-text, the inverse of TTS."
}
},
{
"id": "aif-c01-ml_development-100",
"certification_id": "AIF-C01",
"domain": "ML_DEVELOPMENT",
"difficulty": "easy",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "What is the primary objective of feature engineering in ML development?",
"option_a": "Collect more data",
"option_b": "Create or transform features to improve model performance",
"option_c": "Evaluate model performance",
"option_d": "Deploy the model to production",
"correct_answers": ["B"],
"explanation_detailed": "Feature engineering extracts domain signal from raw data—creating, transforming, and selecting variables that improve model capacity to generalize. Examples include scaling, encoding categoricals, aggregations, domain-specific ratios, and time-window features. On AWS, use SageMaker Processing or Feature Store to build, document, and share features consistently across training and inference. Good features reduce complexity requirements and model brittleness.",
"incorrect_explanations": {
"A": "More data can help but doesn’t replace engineering informative features.",
"C": "Evaluation measures performance; feature engineering changes inputs to affect it.",
"D": "Deployment is an operational step, not feature creation."
}
},

{
"id": "aif-c01-ai_fundamentals-101",
"certification_id": "AIF-C01",
"domain": "AI_FUNDAMENTALS",
"difficulty": "easy",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "Which of the following is an example of unsupervised learning?",
"option_a": "Spam detection",
"option_b": "Image classification",
"option_c": "Clustering of customer segments",
"option_d": "House price prediction",
"correct_answers": ["C"],
"explanation_detailed": "Unsupervised learning seeks patterns without human labels. The classic example is clustering, which organizes samples into groups (clusters) with high internal similarity and low external similarity. This is useful for customer segmentation when you don't have predefined classes, topic discovery, and detection of latent structures. In an AWS context, you can experiment with clustering using algorithms available in Amazon SageMaker (e.g., K-Means), running managed training, automatic tuning, and batch inference. This approach differs from supervised learning, which uses labeled data to train a classifier or regressor. Clustering helps reveal hidden segments for campaigns, pricing, and personalization.",
"incorrect_explanations": {
"A": "Spam detection is typically a supervised problem: you train a classifier with labeled examples of 'spam' and 'not spam' messages. The model learns boundaries from these labels.",
"B": "Image classification uses class labels (e.g., 'cat', 'dog'). Without labels, the model cannot learn the discriminative boundaries typical of supervised classification.",
"D": "House price prediction is supervised regression: the model learns from labeled feature-target pairs (house features → price) to estimate continuous values."
}
},
{
"id": "aif-c01-ml_development-102",
"certification_id": "AIF-C01",
"domain": "ML_DEVELOPMENT",
"difficulty": "medium",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "What is the main difference between batch inference and real-time inference?",
"option_a": "Batch inference is always more accurate",
"option_b": "Real-time inference can only be done on small datasets",
"option_c": "Batch inference processes multiple inputs at once, while real-time inference processes individual inputs as they arrive",
"option_d": "Real-time inference is always faster than batch inference",
"correct_answers": ["C"],
"explanation_detailed": "Batch inference aggregates many inputs and processes them periodically, optimizing cost and throughput when latency is not critical (e.g., nightly scoring of millions of records). In real-time, each request is processed with low latency to power online experiences, such as recommendations or chatbots. On AWS, Amazon SageMaker offers Batch Transform for batches and real-time Endpoints (including Serverless Inference) for low latency. Managed AI services, like Amazon Comprehend or Rekognition, also expose synchronous (real-time) APIs and, in some scenarios, asynchronous mechanisms for processing large collections. The choice depends on latency SLA, volume, cost, and operational needs.",
"incorrect_explanations": {
"A": "Accuracy is not a direct function of the inference mode. The same model can be applied in batch or real-time; the central difference is in latency and aggregate processing.",
"B": "Real-time is not limited to 'small' data; it processes unitary requests with low latency. The practical limit comes from SLA, endpoint scalability, and cost.",
"D": "Real-time aims for low latency per request, but 'always faster' is false. In massive scenarios, batches can complete the total set faster and at a lower cost."
}
},
{
"id": "aif-c01-ai_services-103",
"certification_id": "AIF-C01",
"domain": "AI_SERVICES",
"difficulty": "easy",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "Which AWS service is best suited for managing the entire machine learning lifecycle?",
"option_a": "Amazon Comprehend",
"option_b": "Amazon SageMaker",
"option_c": "Amazon Polly",
"option_d": "Amazon Translate",
"correct_answers": ["B"],
"explanation_detailed": "Amazon SageMaker is the complete ML platform from AWS for preparing data, training, tuning, deploying, and monitoring models at scale. It provides managed notebooks, MLOps pipelines (SageMaker Pipelines), experiments, automatic tuning (HPO), as well as endpoints for real-time, asynchronous, and batch inference. Features like SageMaker Model Monitor detect data/model drift, and SageMaker JumpStart provides pre-trained models and solutions. Comparatively, Amazon Comprehend, Polly, and Translate are high-level services for NLP, speech synthesis, and translation, respectively, consumed via API, without managing the entire lifecycle. SageMaker centralizes end-to-end ML, promoting governance and repeatability.",
"incorrect_explanations": {
"A": "Amazon Comprehend solves NLP tasks (topics, entities, sentiment) via API. It is not a full-lifecycle platform for creating and operating arbitrary models.",
"C": "Amazon Polly performs Text-to-Speech. It does not cover data preparation, experiments, custom training, deployment, and monitoring of general models.",
"D": "Amazon Translate focuses on automatic translation. Although powerful, it is not a comprehensive ML management solution for any domain and MLOps pipeline."
}
},
{
"id": "aif-c01-ml_development-104",
"certification_id": "AIF-C01",
"domain": "ML_DEVELOPMENT",
"difficulty": "medium",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "What is the primary goal of model monitoring in production?",
"option_a": "To train new models",
"option_b": "To collect more data",
"option_c": "To detect issues like model drift or data drift",
"option_d": "To perform feature engineering",
"correct_answers": ["C"],
"explanation_detailed": "After deployment, the production environment changes: the distribution of input data and the relationship between features and labels can evolve. Monitoring detects data drift, concept/model drift, metric degradation, and operational issues. On AWS, SageMaker Model Monitor collects traffic samples, compares statistics with baselines, and triggers alerts (e.g., via CloudWatch) when there are violations. This telemetry guides actions such as scheduled retraining, feature re-engineering, and prompt review for LLMs. Without monitoring, silent errors grow, eroding business KPIs and user trust, especially in recommendation and decision systems.",
"incorrect_explanations": {
"A": "Training new models can be a consequence, not the primary goal. The target is to detect anomalies and degradation to decide if retraining makes sense.",
"B": "Collecting data helps, but the focus is on analyzing quality/distribution and metrics in production. Collection without analysis does not guarantee detection of drift or regressions.",
"D": "Feature engineering improves representations but occurs before or during retraining. Monitoring evaluates post-deployment behavior and triggers corrections when necessary."
}
},
{
"id": "aif-c01-ai_fundamentals-105",
"certification_id": "AIF-C01",
"domain": "AI_FUNDAMENTALS",
"difficulty": "easy",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "Which of the following is NOT a typical use case for AI/ML?",
"option_a": "Fraud detection",
"option_b": "Recommendation systems",
"option_c": "Manual data entry",
"option_d": "Speech recognition",
"correct_answers": ["C"],
"explanation_detailed": "AI/ML automates repetitive tasks, makes probabilistic decisions, and extracts patterns from data at scale. Fraud detection uses supervised models and anomaly detection; recommendations employ collaborative filtering and models like Amazon Personalize; speech recognition utilizes acoustic networks, available via Amazon Transcribe. 'Manual data entry' is precisely the opposite: manual processes prone to errors and low scale that tend to be reduced by automation. On AWS, managed services accelerate adoption without requiring research teams, and when necessary, SageMaker allows creating custom models and pipelines to incorporate MLOps and governance.",
"incorrect_explanations": {
"A": "Fraud detection is a consolidated application of AI/ML with classification and anomalies. Banks and e-commerce have used it for years to reduce losses and review alerts.",
"B": "Recommendation systems are at the core of many digital products. AWS offers Amazon Personalize to accelerate this capability without heavy infrastructure.",
"D": "Speech recognition is widely served by Amazon Transcribe, allowing transcription of audio/video and feeding into sentiment analysis, searches, and reports."
}
},
{
"id": "aif-c01-ai_fundamentals-106",
"certification_id": "AIF-C01",
"domain": "AI_FUNDAMENTALS",
"difficulty": "easy",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "What is a token in the context of generative AI?",
"option_a": "A security feature",
"option_b": "A unit of text processed by the model",
"option_c": "A type of neural network",
"option_d": "A model evaluation metric",
"correct_answers": ["B"],
"explanation_detailed": "Tokens are atomic units that language models consume and produce. Depending on the tokenization, a token can be a word, subword, or character. The cost and latency in LLM services are often proportional to the number of input and output tokens. In Amazon Bedrock, most model providers price per 1K tokens processed, directly impacting the bill. Understanding tokens helps design concise prompts, apply chunking for long documents, and predict context limits. Tools like token counters and embeddings guide the sizing of context windows and inference costs in generation and RAG applications.",
"incorrect_explanations": {
"A": "'Token' here does not refer to security credentials or JWTs. It is a textual unit used internally by the model to represent inputs/outputs.",
"C": "A token is not an architecture. Networks like Transformers process sequences of tokens, but a token is the input, not the type of network itself.",
"D": "It is not an evaluation metric. Metrics like BLEU, ROUGE, or BERTScore evaluate quality; tokens measure the amount of text and costs."
}
},
{
"id": "aif-c01-ai_fundamentals-107",
"certification_id": "AIF-C01",
"domain": "AI_FUNDAMENTALS",
"difficulty": "easy",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "Which of the following is NOT a typical use case for generative AI models?",
"option_a": "Image generation",
"option_b": "Summarization",
"option_c": "Data encryption",
"option_d": "Code generation",
"correct_answers": ["C"],
"explanation_detailed": "Generative models create new content from learned patterns: text, images, audio, and code. Summarization condenses texts while maintaining meaning; image generation produces original visuals; code generation assists development. Encryption, on the other hand, is a security domain with mathematical algorithms (AES, RSA) and protocols. Although AI can assist in detecting vulnerabilities or generating examples, 'encrypting' data is not a typical function of a generative model. On AWS, generative capabilities appear in Amazon Bedrock (LLMs and diffusion), while security and encryption controls are provided by services like KMS, not by LLMs.",
"incorrect_explanations": {
"A": "Image generation is a central use of diffusion models, available from Amazon Bedrock providers and vision frameworks.",
"B": "Summarization is a classic use case for LLMs, including via managed APIs and examples in Bedrock or SageMaker JumpStart.",
"D": "Code generation is widely supported by code-centric LLMs. The output does not replace human review, but it is a genuine use case."
}
},
{
"id": "aif-c01-ai_fundamentals-108",
"certification_id": "AIF-C01",
"domain": "AI_FUNDAMENTALS",
"difficulty": "easy",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "What is the main advantage of the adaptability of generative AI?",
"option_a": "It can only work with structured data",
"option_b": "It can handle a wide range of tasks and domains",
"option_c": "It always produces perfect results",
"option_d": "It eliminates the need for human supervision",
"correct_answers": ["B"],
"explanation_detailed": "LLMs and multimodal models are pre-trained on diverse data and learn reusable representations. This foundation allows for adaptation through prompt engineering, few-shot, and fine-tuning to multiple tasks: summarization, extraction, code generation, classification, image generation, and more. In Amazon Bedrock, you choose suitable FMs and can perform managed fine-tuning or grounding via RAG. This versatility reduces time to value, without rewriting pipelines from scratch. However, adaptability does not imply perfection nor does it replace governance: human supervision, evaluations, and monitoring remain essential for accuracy, security, and compliance. The strength lies in generalizing and composing capabilities under different business contexts.",
"incorrect_explanations": {
"A": "Generative models work well with unstructured data (text, image, audio). They are not limited to tabular structures.",
"C": "'Perfect' results are not guaranteed. There is stochastic variability, dependence on the prompt, and a risk of hallucinations.",
"D": "Human supervision remains relevant for validation, security, and ethics, including reviewing outputs and critical decisions."
}
},
{
"id": "aif-c01-responsible_ai-109",
"certification_id": "AIF-C01",
"domain": "RESPONSIBLE_AI",
"difficulty": "medium",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "What is a hallucination in the context of generative AI?",
"option_a": "A visual output produced by the model",
"option_b": "A type of model architecture",
"option_c": "An incorrect or fabricated output presented as fact",
"option_d": "A model training method",
"correct_answers": ["C"],
"explanation_detailed": "A hallucination occurs when a model generates content that is plausible but false, incomplete, or not based on a source. In critical contexts, this undermines trust and can cause harm. Mitigation strategies include grounding with RAG (retrieving evidence from bases like Amazon OpenSearch Service, Amazon Kendra, or S3), temperature/top-p controls, post-processing validations, and human evaluation. In Amazon Bedrock, you can combine FMs with Guardrails for security policies and with Agents to orchestrate calls to tools and sources. Monitoring accuracy and traceability, recording citations, and limiting 'hard fact' tasks to flows with verification reduce the operational risk of hallucinations.",
"incorrect_explanations": {
"A": "A 'visual output' does not characterize a hallucination on its own. A hallucination is about factual incorrectness, regardless of modality (text, image).",
"B": "It is not a model architecture. Architectures like Transformers or diffusion can hallucinate if used without grounding/validation.",
"D": "It is not a training method. A technique like RLHF can reduce hallucinations, but hallucination is an unwanted behavior in inference."
}
},
{
"id": "aif-c01-ai_services-110",
"certification_id": "AIF-C01",
"domain": "AI_SERVICES",
"difficulty": "easy",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "Which AWS service is specifically designed for developing generative AI applications?",
"option_a": "Amazon EC2",
"option_b": "Amazon S3",
"option_c": "Amazon Bedrock",
"option_d": "Amazon RDS",
"correct_answers": ["C"],
"explanation_detailed": "Amazon Bedrock offers API access to various foundation models (FMs) for text, image, and multimodal, as well as capabilities like fine-tuning, guardrails, and orchestration with Agents. It abstracts the infrastructure, simplifying the integration of LLMs into products. You can prototype in the console, use SDKs, and combine with RAG (Kendra/OpenSearch) for responses based on proprietary data. While EC2, S3, and RDS are compute, storage, and database services respectively, Bedrock focuses directly on the generative app building cycle, with per-token billing and tools for security, auditing, and enterprise integration.",
"incorrect_explanations": {
"A": "Amazon EC2 provides compute instances. Useful for hosting models, but it is not the managed service for FMs and generative AI orchestrations.",
"B": "Amazon S3 stores objects. It can hold training data, prompts, and results, but it does not provide generative models via API.",
"D": "Amazon RDS manages relational databases. It does not directly deliver FMs or content generation/orchestration features."
}
},
{
"id": "aif-c01-ai_fundamentals-111",
"certification_id": "AIF-C01",
"domain": "AI_FUNDAMENTALS",
"difficulty": "easy",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "What is a foundation model in generative AI?",
"option_a": "A model that can only generate text",
"option_b": "A large, pre-trained model that can be adapted for various tasks",
"option_c": "A model designed specifically for image generation",
"option_d": "A model that does not require training data",
"correct_answers": ["B"],
"explanation_detailed": "Foundation models (FMs) are large-scale networks pre-trained on extensive and diverse data to learn general representations. They serve as a basis for many tasks: QA, summarization, extraction, image/code generation, and more. Adaptation occurs through prompt engineering, few-shot, fine-tuning, or instruction. In Amazon Bedrock, you choose FMs from different providers and adjust them to your use case, saving the time and cost of training from scratch. These models are not limited to text or images; the term encompasses unimodal and multimodal variants. Although extensible, they still require training data in the pre-training stage and curation when customizing.",
"incorrect_explanations": {
"A": "FMs are not limited to text; there are models for image, multimodality, and code. The key is the broad and reusable pre-training.",
"C": "Image models (e.g., diffusion) are a possible subset. 'Foundation' does not necessarily imply a focus on images.",
"D": "Every model requires data at some stage. Massive pre-training is central to the concept of an FM."
}
},
{
"id": "aif-c01-ai_fundamentals-112",
"certification_id": "AIF-C01",
"domain": "AI_FUNDAMENTALS",
"difficulty": "easy",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "Which of the following is NOT a step in the foundation model lifecycle?",
"option_a": "Data selection",
"option_b": "Pre-training",
"option_c": "Deployment",
"option_d": "Marketing",
"correct_answers": ["D"],
"explanation_detailed": "The technical lifecycle of FMs involves collecting/curating data, pre-training models on a large scale, adjusting for specific tasks (instruction/fine-tuning), evaluating with appropriate metrics, and deploying/monitoring in production. Governance, security, and cost are cross-cutting considerations. Marketing is a go-to-market activity and not part of the model's technical lifecycle. On AWS, SageMaker and Bedrock offer tools for various phases: JumpStart and datasets, MLOps pipelines, managed endpoints, guardrails, and monitoring. The discipline ensures reproducibility, quality, and compliance, reducing risks of drift and degrading less in dynamic scenarios.",
"incorrect_explanations": {
"A": "Data selection/curation is essential for quality and coverage. Noisy data degrades pre-training and subsequent adaptations.",
"B": "Pre-training learns general representations on massive corpora and is central to the FM concept.",
"C": "Deployment ensures the model serves traffic with SLA, security, and observability, a critical part of the operational cycle."
}
},
{
"id": "aif-c01-ai_services-113",
"certification_id": "AIF-C01",
"domain": "AI_SERVICES",
"difficulty": "easy",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "What is the main advantage of using AWS generative AI services to build applications?",
"option_a": "They are always free",
"option_b": "They provide a lower barrier to entry",
"option_c": "They guarantee 100% accuracy",
"option_d": "They eliminate the need for any coding",
"correct_answers": ["B"],
"explanation_detailed": "Managed services like Amazon Bedrock, Amazon Q, and integrations with Kendra/OpenSearch remove much of the complexity of model infrastructure and orchestration. You access FMs via API, can prototype quickly, adjust by instruction, apply guardrails, and combine with RAG. This approach reduces time to value, MLOps effort, and initial costs, allowing focus on the business case. Still, accuracy is not guaranteed and coding may be necessary for integrations, evaluation, and monitoring. The key benefit is accelerating delivery with less operational burden, while maintaining good security and governance practices supported by the AWS ecosystem.",
"incorrect_explanations": {
"A": "They are not 'always free'. Most adopt billing per token, request, or compute resources used.",
"C": "No service guarantees 100% accuracy. Evaluation, limits, and validations remain mandatory.",
"D": "Although low-code/no-code options exist, integrations and business logic require some level of coding."
}
},
{
"id": "aif-c01-ai_fundamentals-114",
"certification_id": "AIF-C01",
"domain": "AI_FUNDAMENTALS",
"difficulty": "medium",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "What is prompt engineering in the context of generative AI?",
"option_a": "A method of hardware optimization",
"option_b": "A technique for designing the physical structure of AI models",
"option_c": "The process of creating effective input prompts to guide model outputs",
"option_d": "A way to reduce power consumption in AI systems",
"correct_answers": ["C"],
"explanation_detailed": "Prompt engineering consists of structuring instructions, context, and examples to guide LLMs to produce responses aligned with the task. It includes techniques such as clear delimitation, roles, constraints, few-shot, negative prompts, and automatic validations. In AWS applications, you can iterate on prompts in Amazon Bedrock, use Agents to decompose tasks, and combine RAG (Kendra/OpenSearch) for grounding, reducing hallucinations. The practice involves measuring quality, cost (tokens), and latency, as well as versioning templates and using parameters like temperature and top-p. Good prompts make the application more predictable and help meet business and compliance requirements.",
"incorrect_explanations": {
"A": "It does not optimize hardware. It deals with the textual/structural design of the input to guide the model's behavior.",
"B": "It does not design the 'physics' of the model. It is an application layer on top of already trained models.",
"D": "It can indirectly influence cost/latency, but it is not an energy efficiency technique."
}
},
{
"id": "aif-c01-responsible_ai-115",
"certification_id": "AIF-C01",
"domain": "RESPONSIBLE_AI",
"difficulty": "medium",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "Which of the following is a potential disadvantage of generative AI solutions?",
"option_a": "Adaptability",
"option_b": "Responsiveness",
"option_c": "Inaccuracy",
"option_d": "Simplicity",
"correct_answers": ["C"],
"explanation_detailed": "Generative AI can produce incorrect, biased, or unsubstantiated outputs, affecting security, reputation, and KPIs. Mitigation requires policies, human evaluation, grounding (RAG), fact-checking, and guardrails. On AWS, combining Bedrock with Amazon Kendra/OpenSearch and content rules, in addition to continuous monitoring (CloudWatch, logs, metrics), helps reduce risks. Evaluating quality with appropriate metrics and A/B tests ensures that changes in prompts/models do not degrade business results. Adaptability and simplicity are benefits, but they must come with governance, cost-per-token controls, and human review strategies for critical domains like healthcare, finance, and legal.",
"incorrect_explanations": {
"A": "Adaptability is an advantage: models adjust to multiple contexts via prompt, few-shot, and tuning.",
"B": "Responsiveness refers to quick and natural interaction, generally beneficial in assistants and chat.",
"D": "Simplicity in integration lowers barriers. The problem is not being simple, but managing quality and security risks."
}
},
{
"id": "aif-c01-ai_fundamentals-116",
"certification_id": "AIF-C01",
"domain": "AI_FUNDAMENTALS",
"difficulty": "easy",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "What is a multimodal model in generative AI?",
"option_a": "A model that can only process text data",
"option_b": "A model that can work with multiple types of data (e.g., text, images, audio)",
"option_c": "A model that requires multiple GPUs to run",
"option_d": "A model that can only generate images",
"correct_answers": ["B"],
"explanation_detailed": "Multimodal models consume and generate different modalities, such as text, image, audio, and video. They create shared representations between modalities, enabling tasks like describing images, answering questions about figures, or generating illustrations from text. In Amazon Bedrock, there are providers with multimodal capabilities exposed via API, which you can combine with storage in S3 and vector search (OpenSearch) for grounding. This integration expands the range of applications, from technical support with screenshots to media analysis. Although flexible, they require the same governance as text-only LLMs: evaluation, monitoring, and control of costs per token and per content.",
"incorrect_explanations": {
"A": "Text-only describes unimodal models. Multimodality goes beyond just one form of data.",
"C": "GPU requirement depends on size and SLA, it is not a definition of multimodality.",
"D": "Generating only images is unimodal. Multimodal combines multiple input/output modalities."
}
},
{
"id": "aif-c01-ai_services-117",
"certification_id": "AIF-C01",
"domain": "AI_SERVICES",
"difficulty": "easy",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "Which AWS service provides a playground for experimenting with generative AI models?",
"option_a": "Amazon SageMaker",
"option_b": "Amazon Comprehend",
"option_c": "PartyRock",
"option_d": "Amazon Polly",
"correct_answers": ["C"],
"explanation_detailed": "PartyRock is a playground based on Amazon Bedrock that allows you to quickly build generative apps, experimenting with prompts, UI components, and data sources without managing infrastructure. It facilitates learning and prototyping for cases like chat, summarization, extraction, and image generation. In parallel, the Bedrock console and SDKs allow for direct testing of FMs and integration with services like Kendra for RAG. Unlike Comprehend or Polly, which solve specific NLP and TTS tasks via API, PartyRock accelerates ideation and validation of use cases, reducing the initial barrier for technical and non-technical teams to explore generative solutions.",
"incorrect_explanations": {
"A": "SageMaker is the complete ML platform. Useful for lifecycles and tuning, but it's not a 'playground' focused on ready-made generative apps.",
"B": "Comprehend provides ready-made NLP (entities, sentiment). It is not a prototyping environment for generative AI apps.",
"D": "Polly does Text-to-Speech. It does not offer broad experimentation with LLMs/generative flows."
}
},
{
"id": "aif-c01-ml_development-118",
"certification_id": "AIF-C01",
"domain": "ML_DEVELOPMENT",
"difficulty": "medium",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "What is a key consideration when selecting an appropriate generative AI model for a business problem?",
"option_a": "The model's popularity on social media",
"option_b": "The model's performance requirements",
"option_c": "The model's development date",
"option_d": "The model's country of origin",
"correct_answers": ["B"],
"explanation_detailed": "Model selection should align task requirements with metrics and constraints: expected quality, latency, cost per token, maximum context, security, and support for languages/modalities. On AWS, comparing Bedrock providers by context size, instruction capabilities, tools (agents, function calling), fine-tuning options, and guardrails is essential. For long tasks, larger windows and predictable costs are important; for secure chat, controls and grounding with Kendra/OpenSearch are decisive. Popularity does not guarantee suitability. A systematic evaluation with benchmarks and A/B tests reduces the risk of choosing models unsuited to the desired SLA and ROI.",
"incorrect_explanations": {
"A": "Popularity does not ensure quality/fit for your use case. Business metrics and requirements should guide the choice.",
"C": "Release date does not imply better performance for your task. Empirical evaluation matters more.",
"D": "Origin does not determine technical suitability. Focus on metrics, usage policies, and applicable compliance."
}
},
{
"id": "aif-c01-ml_development-119",
"certification_id": "AIF-C01",
"domain": "ML_DEVELOPMENT",
"difficulty": "medium",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "Which of the following is NOT a typical business metric for evaluating generative AI applications?",
"option_a": "Conversion rate",
"option_b": "Average revenue per user",
"option_c": "Customer lifetime value",
"option_d": "Model parameter count",
"correct_answers": ["D"],
"explanation_detailed": "Business metrics measure impact on results: conversion, retention, average ticket, LTV, NPS, and time saved. In generative applications, in addition to technical metrics (latency, cost per 1K tokens), it is vital to capture KPIs that prove ROI. Parameter count is a technical characteristic of the model and does not translate to business value. On AWS, combine application telemetry (CloudWatch, OpenSearch dashboards) and experimentation (A/B via CloudWatch Evidently or frameworks) to track goals. Mapping metrics to hypotheses and executing continuous improvement cycles avoids 'model for model's sake' optimizations without evidence of real gain.",
"incorrect_explanations": {
"A": "Conversion rate reflects direct impact on acquisition/sales objectives. It is a central business metric.",
"B": "Average revenue per user indicates monetization per active base, useful for evaluating growth and upsell.",
"C": "LTV measures the total projected value of the customer, important for prioritizing investments and CAC."
}
},
{
"id": "aif-c01-responsible_ai-120",
"certification_id": "AIF-C01",
"domain": "RESPONSIBLE_AI",
"difficulty": "medium",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "What is a key benefit of the AWS infrastructure for generative AI applications?",
"option_a": "It eliminates the need for any security measures",
"option_b": "It provides free and unlimited computational resources",
"option_c": "It ensures compliance with relevant regulations",
"option_d": "It guarantees that AI models will never make mistakes",
"correct_answers": ["C"],
"explanation_detailed": "AWS provides rigorous security and compliance controls (e.g., certifications and encryption features) that make it easier to meet sectoral regulatory requirements when hosting data and running inferences. For generative AI, this enables integrating FMs with governance, auditing, and data isolation (VPC endpoints, KMS). Services like Bedrock, SageMaker, and Kendra operate within this ecosystem, allowing for access policies, logging, and monitoring. This does not imply model infallibility, but creates a reliable foundation for building solutions in regulated domains. Additionally, elastic resources reduce operational effort without promising 'unlimited' free resources or the absence of security practices.",
"incorrect_explanations": {
"A": "Security measures are still necessary: identity, encryption, human review, and content controls.",
"B": "The cloud is elastic, but charged according to use. There are no 'free and unlimited' resources.",
"D": "Models can make mistakes. Compliance does not eliminate the need for continuous evaluation and monitoring."
}
},
{
"id": "aif-c01-ml_development-121",
"certification_id": "AIF-C01",
"domain": "ML_DEVELOPMENT",
"difficulty": "medium",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "What is 'chunking' in the context of generative AI?",
"option_a": "A data compression method",
"option_b": "A technique for splitting large inputs into smaller, manageable pieces",
"option_c": "A type of model architecture",
"option_d": "A way to increase model accuracy",
"correct_answers": ["B"],
"explanation_detailed": "Chunking divides long documents into smaller segments (chunks) to fit within the context limit of LLMs and improve retrieval in RAG. Each chunk receives metadata (title, source, date) and, optionally, an overlap to maintain coherence between sections. On AWS, you can create chunks in pipelines on Amazon SageMaker, store embeddings per chunk in Amazon OpenSearch Service, or query with Amazon Kendra. During generation (Amazon Bedrock), the system retrieves the most relevant chunks (semantic similarity) and injects them into the prompt. This reduces hallucinations, improves factual accuracy, and controls cost per token, as only relevant parts of the content are sent to the model instead of the entire document.",
"incorrect_explanations": {
"A": "Compression reduces binary size; chunking organizes text into semantic blocks to bypass context limits and enable efficient retrieval.",
"C": "Model architecture describes structures like Transformers or diffusion. Chunking is a preprocessing and retrieval strategy, not an architecture.",
"D": "It can indirectly contribute to better accuracy in RAG, but the central goal is to enable context and retrieval, not to 'increase accuracy' per se."
}
},
{
"id": "aif-c01-ai_fundamentals-122",
"certification_id": "AIF-C01",
"domain": "AI_FUNDAMENTALS",
"difficulty": "easy",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "Which of the following is a key advantage of the simplicity of generative AI?",
"option_a": "It always produces perfect results",
"option_b": "It requires no human input",
"option_c": "It can be easier to implement and use compared to traditional methods",
"option_d": "It eliminates the need for data preprocessing",
"correct_answers": ["C"],
"explanation_detailed": "Simplicity comes from access to pre-trained foundation models and high-level interfaces. Instead of building pipelines from scratch, you call APIs (e.g., Amazon Bedrock) and get capabilities like summarization and extraction. This reduces time to value and MLOps effort. Services like Amazon Q, Amazon Kendra, and integrations with Amazon SageMaker JumpStart accelerate adoption without requiring deep training knowledge. Despite being simple to start, best practices remain: prompt engineering, evaluation, monitoring, and cost-per-token control. Simplicity reduces initial friction but does not guarantee perfection or eliminate the need for human curation and validation.",
"incorrect_explanations": {
"A": "Generative models can make mistakes or hallucinate. Simplicity of use does not imply perfection of results.",
"B": "Human interaction remains essential for defining requirements, validating outputs, and applying governance.",
"D": "Preprocessing (cleaning, chunking, metadata) is still useful, especially in flows with RAG and automatic evaluations."
}
},
{
"id": "aif-c01-ai_fundamentals-123",
"certification_id": "AIF-C01",
"domain": "AI_FUNDAMENTALS",
"difficulty": "easy",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "What is a diffusion model in generative AI?",
"option_a": "A model that only works with textual data",
"option_b": "A type of generative model often used for image generation",
"option_c": "A model that does not require training data",
"option_d": "A model designed specifically for natural language processing",
"correct_answers": ["B"],
"explanation_detailed": "Diffusion models learn to reverse a process of progressive noise until they reconstruct a coherent image. During training, the model observes data with increasing noise; during generation, it removes noise step-by-step from pure noise until it creates the image. This approach produces high-quality results and is the basis of many modern visual synthesis systems. On AWS, you can consume image generators via Amazon Bedrock (foundation models from partner providers) or train/infer custom ones on Amazon SageMaker. Integrations with Amazon S3 for data and monitoring with CloudWatch complete a production pipeline.",
"incorrect_explanations": {
"A": "Diffusion is mostly applied to images, although there is multimodal research. It is not 'text-only'.",
"C": "Every model learns from data. Diffusion requires image datasets to train the reverse process.",
"D": "Although diffusion exists for audio and video, it is not a model designed 'only' for NLP."
}
},
{
"id": "aif-c01-ai_services-124",
"certification_id": "AIF-C01",
"domain": "AI_SERVICES",
"difficulty": "easy",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "Which AWS service is designed to provide conversational AI capabilities?",
"option_a": "Amazon Bedrock",
"option_b": "Amazon SageMaker",
"option_c": "Amazon Q",
"option_d": "Amazon S3",
"correct_answers": ["C"],
"explanation_detailed": "Amazon Q is a generative AI assistant for work, with secure chat, content generation, and actions in business applications. It can be connected to internal sources (e.g., through connectors) to respond based on your data, apply policies, and log audits. It can use foundation models from Amazon Bedrock and integrates with the AWS ecosystem for security, identity, and monitoring. While Bedrock provides the FMs and SageMaker allows for building custom models and pipelines, Q delivers the ready-to-use corporate conversational experience, with governance, access control, and productivity features.",
"incorrect_explanations": {
"A": "Bedrock exposes FMs and orchestration, but it is not by itself the ready-made corporate conversational experience.",
"B": "SageMaker is an end-to-end ML platform, not a ready-made corporate chat product.",
"D": "S3 is object storage; it does not provide a conversational interface or logic."
}
},
{
"id": "aif-c01-ai_services-125",
"certification_id": "AIF-C01",
"domain": "AI_SERVICES",
"difficulty": "medium",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "What is a key consideration in the cost trade-offs of AWS generative AI services?",
"option_a": "The color scheme of the user interface",
"option_b": "The number of employees in the company",
"option_c": "Token-based pricing",
"option_d": "The physical location of the data center",
"correct_answers": ["C"],
"explanation_detailed": "Many providers on Amazon Bedrock charge per 1,000 input and output tokens. Costs grow with long prompts, extensive context, and verbose responses. Reduction strategies include efficient chunking, concise prompts, focused RAG, result caching, and parameterization (temperature/top-p) for shorter outputs when possible. In high-volume flows, consider optimized endpoints and usage monitoring via CloudWatch and Cost Explorer. For your own models on Amazon SageMaker, costs include instances, storage, and traffic. In all cases, instrument telemetry per resource, trace cost per call, and measure business impact, avoiding optimizing only technical metrics.",
"incorrect_explanations": {
"A": "UI colors do not influence billing per token, latency, or throughput.",
"B": "Number of employees does not directly determine cost; the volume of calls and tokens does.",
"D": "Region can marginally affect price, but the main variable is the volume of tokens and model type."
}
},
{
"id": "aif-c01-ai_fundamentals-126",
"certification_id": "AIF-C01",
"domain": "AI_FUNDAMENTALS",
"difficulty": "medium",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "What is the primary purpose of embeddings in generative AI?",
"option_a": "To compress data for storage",
"option_b": "To represent data in a high-dimensional space",
"option_c": "To encrypt sensitive information",
"option_d": "To generate random numbers",
"correct_answers": ["B"],
"explanation_detailed": "Embeddings transform items (words, sentences, images) into high-dimensional vectors where proximity reflects semantic similarity. They are the basis for semantic search, deduplication, classification, and RAG. On AWS, you can generate embeddings with models available on Amazon Bedrock or Amazon SageMaker, and store them in engines with vector search, such as Amazon OpenSearch Service, or index them with Amazon Kendra. During inference, the system queries the vectors closest to the question's content and injects the retrieved context into the prompt, reducing hallucinations and improving factual accuracy. Embeddings are not encryption or compression; they are representations for calculating similarity.",
"incorrect_explanations": {
"A": "Although they can be compact, the purpose is semantic, not data compression like ZIP.",
"C": "They do not provide confidentiality like encryption. Vectors can even leak information if mismanaged.",
"D": "Random number generation is not related to semantic representations."
}
},
{
"id": "aif-c01-ai_fundamentals-127",
"certification_id": "AIF-C01",
"domain": "AI_FUNDAMENTALS",
"difficulty": "easy",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "Which of the following is NOT a typical use case for generative AI in customer service?",
"option_a": "Chatbots",
"option_b": "Automated email responses",
"option_c": "Physical robot assistants",
"option_d": "FAQ generation",
"correct_answers": ["C"],
"explanation_detailed": "In customer service, LLMs provide contextualized answers, summarize histories, generate emails, and update FAQs based on corporate knowledge. Chatbots (e.g., Amazon Lex integrating with FMs via Amazon Bedrock) handle common requests and escalate to humans when necessary. Amazon Kendra and embeddings enable RAG on manuals and policies. 'Physical robot assistants' involve robotics and hardware, outside the usual scope of generative AI applied to digital channels. The priority is to reduce resolution time, maintain an appropriate tone, and track sources, with guardrails and auditing.",
"incorrect_explanations": {
"A": "Chatbots are a central use case, especially combining NLU (Lex) and LLMs for rich responses.",
"B": "Generating email with historical context is a common application to speed up the agent.",
"D": "LLMs generate and update FAQs from internal bases, reducing manual effort."
}
},
{
"id": "aif-c01-ai_services-128",
"certification_id": "AIF-C01",
"domain": "AI_SERVICES",
"difficulty": "easy",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "What is a key advantage of using AWS generative AI services to build applications in terms of development speed?",
"option_a": "They automatically write all the code for you",
"option_b": "They provide a faster time-to-market",
"option_c": "They eliminate the need for testing",
"option_d": "They guarantee instant deployment",
"correct_answers": ["B"],
"explanation_detailed": "With Amazon Bedrock, PartyRock, and integrations with Amazon Kendra, you can prototype quickly without managing model infrastructure. Ready-made foundation models reduce training work, and SDKs simplify integration. Amazon SageMaker JumpStart provides examples, notebooks, and solutions to accelerate POCs and MVPs. The result is a shorter time from ideation to pilot, with reproducible pipelines later in SageMaker for production. It is still necessary to test, monitor, and govern the cycle, but the time-to-market drops substantially.",
"incorrect_explanations": {
"A": "Code generation can help, but integrations and validations still require development.",
"C": "Testing remains essential for quality, security, and business metrics.",
"D": "Deployment requires configuring endpoints, security, and observability; it is not 'instant'."
}
},
{
"id": "aif-c01-ai_fundamentals-129",
"certification_id": "AIF-C01",
"domain": "AI_FUNDAMENTALS",
"difficulty": "easy",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "What is non-determinism in the context of generative AI?",
"option_a": "A type of model architecture",
"option_b": "A data preprocessing method",
"option_c": "The property of producing different outputs for the same input",
"option_d": "A technique to improve model accuracy",
"correct_answers": ["C"],
"explanation_detailed": "LLMs often sample the next token from probability distributions; parameters like temperature and top-p control this variability. Thus, the same input can generate different outputs between runs. For reproducibility, use a fixed seed and low temperature; for creativity, increase temperature/top-p. In production environments (Amazon Bedrock or endpoints on Amazon SageMaker), choose profiles according to the use case: 'hard fact' flows demand less randomness; creative content allows for more variation. Logs and prompt versioning help audit and compare executions.",
"incorrect_explanations": {
"A": "Architecture (like a Transformer) does not define the stochastic behavior on its own; sampling does.",
"B": "Preprocessing prepares data; it does not explain different outputs for the same input.",
"D": "Non-determinism is not an accuracy technique; it is a characteristic of stochastic generation."
}
},
{
"id": "aif-c01-ai_services-130",
"certification_id": "AIF-C01",
"domain": "AI_SERVICES",
"difficulty": "easy",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "Which AWS service is designed to help developers get started quickly with pre-trained models for generative AI?",
"option_a": "Amazon EC2",
"option_b": "Amazon SageMaker JumpStart",
"option_c": "Amazon RDS",
"option_d": "Amazon CloudFront",
"correct_answers": ["B"],
"explanation_detailed": "Amazon SageMaker JumpStart offers catalogs of pre-trained models, example notebooks, and reference solutions to accelerate POCs. You can launch training/deployment instances in a few clicks, compare models, and adapt them to your use case. For generative AI, JumpStart includes language, vision, and diffusion models, with ready-made pipelines for fine-tuning and inference. It is complementary to Amazon Bedrock, which provides API access to managed FMs; JumpStart facilitates experimenting and operationalizing within the SageMaker ecosystem.",
"incorrect_explanations": {
"A": "EC2 provides raw compute. Useful, but it does not offer ready-made ML catalogs and templates.",
"C": "RDS is a relational database. It does not accelerate the exploration of AI models.",
"D": "CloudFront is a CDN for content distribution, not a hub for ML models."
}
},
{
"id": "aif-c01-ml_development-131",
"certification_id": "AIF-C01",
"domain": "ML_DEVELOPMENT",
"difficulty": "medium",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "What is Retrieval-Augmented Generation (RAG)?",
"option_a": "A technique for generating new data",
"option_b": "A method for combining retrieved information with model generation",
"option_c": "A type of model architecture",
"option_d": "A data compression algorithm",
"correct_answers": ["B"],
"explanation_detailed": "RAG searches for evidence in external sources (documents, wikis, databases) and injects the relevant context into the prompt before generation. This grounds the response, reduces hallucinations, and allows updating 'knowledge' without retraining the model. On AWS, use Amazon Kendra or Amazon OpenSearch Service for retrieval, store embeddings in the index, and call the LLM via Amazon Bedrock. Record sources, implement guardrails, and measure quality with automatic and human evaluations. RAG separates 'content memory' from the model's parameters, favoring governance and cost.",
"incorrect_explanations": {
"A": "RAG is not limited to creating new data; it integrates search + generation based on evidence.",
"C": "It is a system pattern, not a specific architecture like a Transformer.",
"D": "It does not involve compression; it deals with semantic search and conditioning the prompt."
}
},
{
"id": "aif-c01-ai_services-132",
"certification_id": "AIF-C01",
"domain": "AI_SERVICES",
"difficulty": "medium",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "Which AWS service is suitable for storing embeddings in a vector database?",
"option_a": "Amazon S3",
"option_b": "Amazon RDS",
"option_c": "Amazon OpenSearch Service",
"option_d": "Amazon EC2",
"correct_answers": ["C"],
"explanation_detailed": "Amazon OpenSearch Service supports k-NN (k-nearest neighbors) and vector indices, allowing for the storage and retrieval of embeddings with similarity queries. This is ideal for semantic search and RAG. The typical pipeline generates embeddings (SageMaker or Bedrock), indexes vectors and metadata in OpenSearch, and, upon query, returns the most similar ones to compose the prompt sent to the LLM. S3 stores objects; RDS is relational; EC2 is general compute. For proximity search in high dimensions, OpenSearch's vector indices provide scalability, metadata filtering, and integration with the AWS ecosystem.",
"incorrect_explanations": {
"A": "S3 stores blobs; it does not offer native vector search by similarity.",
"B": "RDS is relational; it can store vectors but does not natively provide efficient k-NN search.",
"D": "EC2 hosts software, but it is not a managed vector database service."
}
},
{
"id": "aif-c01-ai_fundamentals-133",
"certification_id": "AIF-C01",
"domain": "AI_FUNDAMENTALS",
"difficulty": "medium",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "What is the primary purpose of adjusting the temperature parameter in inference?",
"option_a": "To control the physical temperature of the server",
"option_b": "To adjust the creativity or randomness of the model's output",
"option_c": "To increase the model's processing speed",
"option_d": "To reduce power consumption",
"correct_answers": ["B"],
"explanation_detailed": "Temperature controls the 'flatness' of the probability distribution over the next tokens. High values make the distribution more uniform (more creativity/variation); low values make it sharper (more predictable responses). In production flows on Amazon Bedrock or SageMaker endpoints, set a low temperature for factual and consistent responses; increase it for brainstorming and ideation. Combine with top-p and length limits to balance cost per token. Log parameters per call to reproduce behaviors when necessary.",
"incorrect_explanations": {
"A": "It has no relation to thermal hardware. It is a sampling hyperparameter in generation.",
"C": "Temperature does not speed up the model; it influences the diversity of the generated language.",
"D": "Energy consumption is more related to load and hardware than to the sampling temperature."
}
},
{
"id": "aif-c01-ai_fundamentals-134",
"certification_id": "AIF-C01",
"domain": "AI_FUNDAMENTALS",
"difficulty": "medium",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "What is a chain-of-thought prompt?",
"option_a": "A physical chain used in AI hardware",
"option_b": "A prompt that encourages the model to show its reasoning process",
"option_c": "A method of linking multiple AI models",
"option_d": "A technique for encrypting prompts",
"correct_answers": ["B"],
"explanation_detailed": "Chain-of-thought instructs the model to break down the problem into intermediate steps before the final answer, improving reasoning tasks (calculation, logic, planning). In enterprise environments, prefer 'structured reasoning' with checks and rules to avoid leaking sensitive reasoning. On AWS, you can couple validations and function calling (Agents for Amazon Bedrock) to execute verified steps and record evidence. Evaluate the impact on cost (more tokens) and latency. For auditing, record prompts, outputs, and sources (Kendra/OpenSearch) and define guardrails to remove inappropriate content.",
"incorrect_explanations": {
"A": "It is not a hardware component; it is a prompt engineering technique.",
"C": "It does not link multiple models per se; it deals with how the model organizes the response.",
"D": "It does not provide confidentiality/encryption; it only guides the form of the generated reasoning."
}
},
{
"id": "aif-c01-ml_development-135",
"certification_id": "AIF-C01",
"domain": "ML_DEVELOPMENT",
"difficulty": "medium",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "Which of the following is NOT a typical method for fine-tuning a foundation model?",
"option_a": "Instruction tuning",
"option_b": "Transfer learning",
"option_c": "Physical tuning",
"option_d": "Continued pre-training",
"correct_answers": ["C"],
"explanation_detailed": "FMs are adapted by instruction tuning (training on instruction-response pairs), classic fine-tuning (transfer learning), or continued pre-training in specific domains. These techniques refine weights for new tasks, styles, and vocabulary. On AWS, Amazon SageMaker offers fine-tuning pipelines and Amazon Bedrock provides managed tuning for certain providers. 'Physical tuning' does not exist: adjustments occur in the model's parameter space. The choice depends on available data, cost, governance, and the need to retain the FM's general knowledge.",
"incorrect_explanations": {
"A": "Instruction tuning is a common practice to teach the model to follow commands with quality.",
"B": "Transfer learning is the basis for efficient adaptation, leveraging pre-training.",
"D": "Continued pre-training extends the model's base with additional domain data."
}
},
{
"id": "aif-c01-ml_development-136",
"certification_id": "AIF-C01",
"domain": "ML_DEVELOPMENT",
"difficulty": "medium",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "What is the ROUGE metric used for in the evaluation of foundation models?",
"option_a": "To measure the redness of the model's output",
"option_b": "To evaluate the quality of generated summaries",
"option_c": "To calculate the model's energy efficiency",
"option_d": "To determine the model's processing speed",
"correct_answers": ["B"],
"explanation_detailed": "ROUGE compares the overlap of n-grams, subsequences, and pairs between the generated summary and one or more reference summaries. It is useful for evaluating abstractive and extractive summarization. In AWS pipelines, you can run automatic evaluation in Amazon SageMaker jobs, store metrics in Amazon S3, and visualize in CloudWatch or dashboards. ROUGE does not measure deep understanding, so combine it with human metrics (adequacy, factuality) and, where applicable, RAG with citability (Kendra/OpenSearch) to reduce hallucinations.",
"incorrect_explanations": {
"A": "The name 'ROUGE' is an acronym; it does not measure color. It evaluates textual overlaps.",
"C": "Energy efficiency requires hardware/usage metrics; ROUGE evaluates summarization quality.",
"D": "Speed is latency/throughput, not captured by ROUGE."
}
},
{
"id": "aif-c01-ai_services-137",
"certification_id": "AIF-C01",
"domain": "AI_SERVICES",
"difficulty": "medium",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "What is the primary purpose of using Agents for Amazon Bedrock?",
"option_a": "To hire human agents for AI tasks",
"option_b": "To handle multi-step tasks in AI applications",
"option_c": "To physically maintain AI hardware",
"option_d": "To reduce the cost of AI services",
"correct_answers": ["B"],
"explanation_detailed": "Agents for Amazon Bedrock orchestrate multi-step flows: interpreting instructions, calling tools/integrations, querying bases (Kendra/OpenSearch), and executing actions before responding. They encapsulate structured reasoning and function calling, reducing custom logic on the client side. In corporate use, resources, policies, and validations are defined to maintain traceability and security. The focus is to decompose complex tasks (e.g., opening a ticket, seeking context, summarizing, logging) with consistency, not to reduce costs directly—although efficient automation can impact expenses.",
"incorrect_explanations": {
"A": "It does not involve hiring people; they are model-based orchestrators.",
"C": "Hardware is managed by AWS; agents operate at the application/orchestration layer.",
"D": "They can optimize operations, but the main goal is coordination of steps and tools."
}
},
{
"id": "aif-c01-ml_development-138",
"certification_id": "AIF-C01",
"domain": "ML_DEVELOPMENT",
"difficulty": "medium",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "Which of the following is a key consideration when selecting a pre-trained model?",
"option_a": "The model's popularity on social media",
"option_b": "The physical size of the server hosting the model",
"option_c": "The model's input/output length capabilities",
"option_d": "The color scheme of the model's documentation",
"correct_answers": ["C"],
"explanation_detailed": "The context window (input/output tokens) determines if the model can handle your prompts and documents. For RAG, larger windows reduce the need for truncation but increase cost. Also, evaluate languages, tools (function calling), security, performance, and latency. On AWS, compare FMs in Amazon Bedrock and alternatives in SageMaker JumpStart. Validate with tests in your domain and monitor business metrics in production.",
"incorrect_explanations": {
"A": "Popularity does not ensure suitability; evaluate metrics and use case requirements.",
"B": "Server size is an operational detail; the context window is a direct feasibility factor.",
"D": "Documentation colors do not affect the model's capability or quality."
}
},
{
"id": "aif-c01-responsible_ai-139",
"certification_id": "AIF-C01",
"domain": "RESPONSIBLE_AI",
"difficulty": "medium",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "What is prompt hijacking in the context of prompt engineering?",
"option_a": "A method of optimizing prompts",
"option_b": "A technique for stealing prompts from competitors",
"option_c": "An attack where the model is tricked into ignoring the intended prompt",
"option_d": "A way to speed up prompt processing",
"correct_answers": ["C"],
"explanation_detailed": "Prompt hijacking occurs when malicious inputs inject instructions that cause the model to ignore the original guidelines (e.g., in content retrieved via RAG). Mitigate by using strict instruction delimitation, content filters, source normalization, 'context wrapping', and post-generation validations. On AWS, combine Amazon Bedrock's Guardrails, source verification (Kendra/OpenSearch), and monitoring with CloudWatch/CloudTrail. Maintain blocklists, sanitization, and continuous adversarial testing.",
"incorrect_explanations": {
"A": "It's not mere optimization; it's an attack vector that alters expected behavior.",
"B": "It's not about 'stealing' others' prompts, but about manipulating your model via inputs.",
"D": "It does not speed up processing; it compromises the integrity and security of the responses."
}
},
{
"id": "aif-c01-ml_development-140",
"certification_id": "AIF-C01",
"domain": "ML_DEVELOPMENT",
"difficulty": "medium",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "What is the primary purpose of instruction tuning in foundation models?",
"option_a": "To teach the model to follow specific instructions",
"option_b": "To reduce the size of the model",
"option_c": "To increase the model's processing speed",
"option_d": "To change the model's programming language",
"correct_answers": ["A"],
"explanation_detailed": "Instruction tuning trains the FM on curated instruction-response pairs so it learns to obey requests in natural language with expected formats. This improves adherence to policies, style, and output structure. On AWS, use pipelines in Amazon SageMaker for fine-tuning or managed options in Amazon Bedrock when available. The technique does not reduce size or change language; it refines behavior for specific tasks and response patterns.",
"incorrect_explanations": {
"B": "Compression/distillation can reduce size, but that is not the goal of instruction tuning.",
"C": "Speed depends on hardware, optimizations, and size; instruction tuning focuses on behavior.",
"D": "It doesn't change the programming language; it standardizes how the model follows instructions."
}
},
{
"id": "aif-c01-ml_development-141",
"certification_id": "AIF-C01",
"domain": "ML_DEVELOPMENT",
"difficulty": "medium",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "What is BERTScore used for in the evaluation of foundation models?",
"option_a": "To measure the model's energy efficiency",
"option_b": "To evaluate the quality of generated text",
"option_c": "To calculate the model's processing speed",
"option_d": "To determine the model's market value",
"correct_answers": ["B"],
"explanation_detailed": "BERTScore measures semantic similarity between generated and reference text using contextualized embeddings (e.g., BERT). Instead of just counting n-grams (like ROUGE), it captures more subtle semantic relationships. On AWS, you can run evaluations in Amazon SageMaker jobs, save metrics to Amazon S3, and visualize in CloudWatch. Combine BERTScore with human evaluation and fact-checking (RAG + Kendra/OpenSearch) for a robust view of quality.",
"incorrect_explanations": {
"A": "Energy efficiency requires hardware/consumption metrics, not semantic similarity.",
"C": "Speed is latency/throughput. BERTScore evaluates textual quality.",
"D": "Market value is not a technical metric of generation quality."
}
},
{
"id": "aif-c01-ml_development-142",
"certification_id": "AIF-C01",
"domain": "ML_DEVELOPMENT",
"difficulty": "easy",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "What is a key benefit of using in-context learning for foundation model personalization?",
"option_a": "It requires no additional training data",
"option_b": "It always produces perfect results",
"option_c": "It reduces the model's size",
"option_d": "It eliminates the need for prompts",
"correct_answers": ["A"],
"explanation_detailed": "In-context learning shows a few examples directly in the prompt to guide the LLM's behavior, without retraining. It is useful when there is little data and a need for an immediate response. In Amazon Bedrock, you structure the prompt with instructions, output format, and 2–5 examples, optionally combining RAG (Kendra/OpenSearch) for grounding. Monitor cost per token and latency, as examples increase the context. If the task requires strong consistency, evolve to fine-tuning on Amazon SageMaker.",
"incorrect_explanations": {
"B": "No technique guarantees perfection; evaluate and monitor.",
"C": "It does not change the model's size; it only guides behavior via examples in the prompt.",
"D": "It depends on well-structured prompts; it does not eliminate them."
}
},
{
"id": "aif-c01-ml_development-143",
"certification_id": "AIF-C01",
"domain": "ML_DEVELOPMENT",
"difficulty": "medium",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "What is a potential risk of using zero-shot learning in prompt engineering?",
"option_a": "The model may perform poorly on tasks it was not explicitly trained for",
"option_b": "The model will refuse to generate any output",
"option_c": "The model will only work with numerical data",
"option_d": "The model will consume excessive energy",
"correct_answers": ["A"],
"explanation_detailed": "Zero-shot asks the model to perform a task without examples in the prompt. In specific domains or strict formats, this can degrade quality, leading to errors and hallucinations. To mitigate, add examples (few-shot), use RAG with Kendra or OpenSearch for grounding, and validate with automatic checks. In production on Amazon Bedrock, monitor accuracy metrics and rework prompts/flows when performance drops.",
"incorrect_explanations": {
"B": "Models usually respond; the problem is quality, not a total absence of output.",
"C": "LLMs work with text, not just numerical data. The limitation is semantic and instructional.",
"D": "Consumption depends on load/infra; zero-shot does not imply excessive energy expenditure."
}
},
{
"id": "aif-c01-ml_development-144",
"certification_id": "AIF-C01",
"domain": "ML_DEVELOPMENT",
"difficulty": "medium",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "What is the primary purpose of reinforcement learning from human feedback (RLHF) in training foundation models?",
"option_a": "To reduce the model's energy consumption",
"option_b": "To improve the model's performance based on human evaluations",
"option_c": "To increase the model's size",
"option_d": "To translate the model into different languages",
"correct_answers": ["B"],
"explanation_detailed": "RLHF uses human preferences to train a reward model and then optimize the FM via reinforcement for more aligned responses. This improves usefulness, safety, and style. On AWS, you can train components on Amazon SageMaker, manage labeled data in S3, and evaluate results with automated pipelines. It still requires curation and governance to avoid biasing the model towards inappropriate patterns.",
"incorrect_explanations": {
"A": "The goal is not energy efficiency; it is alignment with human preferences.",
"C": "RLHF does not increase model size by itself; it adjusts behavior.",
"D": "Translation involves models/services like Amazon Translate, not RLHF."
}
},
{
"id": "aif-c01-ml_development-145",
"certification_id": "AIF-C01",
"domain": "ML_DEVELOPMENT",
"difficulty": "easy",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "Which of the following is NOT a typical consideration when preparing data for fine-tuning a foundation model?",
"option_a": "Data curation",
"option_b": "Data size",
"option_c": "Data labeling",
"option_d": "Data color coding",
"correct_answers": ["D"],
"explanation_detailed": "Data preparation involves selection, cleaning, balancing, deduplication, anonymization, and consistent labeling. Size affects generalization and cost; curation defines quality and case coverage. On AWS, use AWS Glue/SageMaker Processing for transformation and S3 as a data lake. 'Color coding' is not relevant for text/code and rarely for images, where what matters are correct annotations and formats. Well-prepared data results in more stable and useful fine-tuning.",
"incorrect_explanations": {
"A": "Curation decides what goes into training and ensures representativeness and quality.",
"B": "Size impacts performance and cost; little data tends to overfitting.",
"C": "Consistent labeling is vital for instruction/fine-tuning with high-quality pairs."
}
},
{
"id": "aif-c01-ml_development-146",
"certification_id": "AIF-C01",
"domain": "ML_DEVELOPMENT",
"difficulty": "medium",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "What is prompt templating in the context of prompt engineering?",
"option_a": "A method of physically printing prompts",
"option_b": "A technique for creating reusable prompt structures",
"option_c": "A way to encrypt prompts",
"option_d": "A process of translating prompts into different languages",
"correct_answers": ["B"],
"explanation_detailed": "Prompt templating defines standardized templates with clear sections: objective, context, input data, output format, examples, and constraints. This improves consistency, facilitates versioning and A/B measurement. On AWS, store templates in S3, manage versions via CodeCommit/CodePipeline, and test on SageMaker or Bedrock. Standardizing prompts reduces variation, accelerates new use cases, and supports governance and change auditing.",
"incorrect_explanations": {
"A": "It does not deal with printing; it is the logical design of instructions for LLMs.",
"C": "Security is obtained with KMS/access control; prompt templating does not encrypt.",
"D": "Templates can be translated, but the technique itself is not a translation process."
}
},
{
"id": "aif-c01-ml_development-147",
"certification_id": "AIF-C01",
"domain": "ML_DEVELOPMENT",
"difficulty": "easy",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "What is the main advantage of using few-shot learning in prompt engineering?",
"option_a": "It requires no examples in the prompt",
"option_b": "It allows the model to learn from a small number of examples",
"option_c": "It always produces perfect results",
"option_d": "It reduces the model's energy consumption",
"correct_answers": ["B"],
"explanation_detailed": "Few-shot includes a few representative examples in the prompt to guide style, format, and reasoning without retraining the model. This improves adherence to specific patterns and reduces the ambiguities of zero-shot. In Bedrock, combine few-shot with RAG for grounded responses, and control costs by limiting the number of examples and tokens. Record prompt versions and evaluate with quality metrics to ensure stability.",
"incorrect_explanations": {
"A": "Few-shot depends on examples; zero-shot is the case without examples.",
"C": "There is no guarantee of perfection; validation and monitoring are still necessary.",
"D": "Consumption is linked to tokens/hardware; few-shot increases tokens in the prompt."
}
},
{
"id": "aif-c01-ai_services-148",
"certification_id": "AIF-C01",
"domain": "AI_SERVICES",
"difficulty": "medium",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "Which AWS service is suitable for storing embeddings in a relational database?",
"option_a": "Amazon DynamoDB",
"option_b": "Amazon S3",
"option_c": "Amazon Aurora",
"option_d": "Amazon EC2",
"correct_answers": ["C"],
"explanation_detailed": "Although vector engines (like OpenSearch) are ideal, it is possible to persist embeddings in a relational database for transactional integrations. Amazon Aurora (MySQL/PostgreSQL compatible) offers scalability and support for numeric types and extensions that facilitate storing vectors and metadata, as well as queries combining relational filters with pre-selected vector IDs. Use Aurora to maintain consistency with transactional systems and OpenSearch when you need k-NN search at a large scale and low latency. Generate embeddings in SageMaker/Bedrock and manage data in S3 as a source layer.",
"incorrect_explanations": {
"A": "DynamoDB is NoSQL key-value/document; it can store vectors but does not offer relational SQL.",
"B": "S3 stores objects; it does not offer native relational queries.",
"D": "EC2 is compute; it is not a managed database service."
}
},
{
"id": "aif-c01-ai_fundamentals-149",
"certification_id": "AIF-C01",
"domain": "AI_FUNDAMENTALS",
"difficulty": "medium",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "What is a key consideration when evaluating whether a foundation model effectively meets business objectives?",
"option_a": "The model's popularity on social media",
"option_b": "The physical size of the server hosting the model",
"option_c": "The model's impact on user engagement",
"option_d": "The color scheme of the model's user interface",
"correct_answers": ["C"],
"explanation_detailed": "Evaluate business outcomes: engagement, conversion, CSAT, LTV, and time savings. Technical metrics (latency, cost per 1,000 tokens) matter, but they need to connect to KPIs. On AWS, collect telemetry in CloudWatch, experiment with Evidently, store logs in S3, and visualize in dashboards (OpenSearch/Kibana). Run A/B tests with prompts/models to validate real impact and avoid optimizations that do not move business metrics.",
"incorrect_explanations": {
"A": "Popularity does not guarantee effectiveness for your context and goals.",
"B": "Physical dimensions do not reflect the value delivered to the user.",
"D": "UI color scheme does not measure business success."
}
},
{
"id": "aif-c01-ai_fundamentals-150",
"certification_id": "AIF-C01",
"domain": "AI_FUNDAMENTALS",
"difficulty": "easy",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "What is the primary purpose of negative prompts in prompt engineering?",
"option_a": "To make the model generate negative emotions",
"option_b": "To tell the model what to avoid in its output",
"option_c": "To reduce the model's energy consumption",
"option_d": "To decrease the model's processing speed",
"correct_answers": ["B"],
"explanation_detailed": "Negative prompts specify what should not appear in the response (e.g., 'do not include PII', 'do not invent citations', 'no HTML'). In production flows on Amazon Bedrock, combine negative prompts with post-processing validations and guardrails to enforce policies. This reduces hallucinations and unwanted content, improves compliance, and makes the output more predictable. The technique does not directly alter energy consumption or latency; it is a content control.",
"incorrect_explanations": {
"A": "It does not define 'negative emotions'; it defines content and format constraints.",
"C": "Energy depends on load and hardware; negative prompts do not directly impact this.",
"D": "Speed is a function of model/infra; negatives are instructions with no direct effect on latency."
}
},
{
"id": "aif-c01-ml_development-151",
"certification_id": "AIF-C01",
"domain": "ML_DEVELOPMENT",
"difficulty": "medium",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "What is continued pre-training in the context of foundation models?",
"option_a": "A method of constantly retraining the model on new data",
"option_b": "A technique for training models 24/7",
"option_c": "A way to train models using continuous mathematics",
"option_d": "A process of training models on a continuous physical surface",
"correct_answers": ["A"],
"explanation_detailed": "Continued pre-training (continual/pretraining refresh) updates an already pre-trained foundation model with additional domain data to expand coverage and reduce obsolescence, while preserving general knowledge. The focus is to adapt vocabulary, style, and new facts without 'forgetting' what it already knows (mitigating catastrophic forgetting with techniques like regularization, data mixing, and checkpoints). On AWS, store corpus in Amazon S3, orchestrate training jobs in Amazon SageMaker, track metrics in Amazon CloudWatch, and manage versions with Model Registry. Use automatic and human evaluations to verify real gains, and data policies (IAM, KMS) for security. Do not confuse with instruction fine-tuning; here the goal is to extend the base pre-training with additional broad texts.",
"incorrect_explanations": {
"B": "Training 24/7 describes a schedule, not the technique of updating the model's knowledge with additional data.",
"C": "'Continuous mathematics' does not characterize the process; it's about expanding the corpus and refining parameters.",
"D": "It does not involve physical surfaces; it is a statistical update of the weights with new data."
}
},
{
"id": "aif-c01-responsible_ai-152",
"certification_id": "AIF-C01",
"domain": "RESPONSIBLE_AI",
"difficulty": "medium",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "What is prompt poisoning in the context of prompt engineering risks?",
"option_a": "A method of optimizing prompts",
"option_b": "A technique for improving prompt quality",
"option_c": "An attack where malicious content is inserted into training data or prompts",
"option_d": "A way to speed up prompt processing",
"correct_answers": ["C"],
"explanation_detailed": "Prompt poisoning occurs when an attacker injects malicious instructions or content into the material that the model consumes (e.g., documents in RAG or tuning data), inducing incorrect responses, data leakage, or non-compliance with policies. Mitigate with source validation and content sanitization, PII and topic filters, isolation between system instructions and retrieved context, post-generation checks, and auditing. In Amazon Bedrock, use Guardrails to block content classes, PII policies, and forbidden terms; with Amazon Kendra/OpenSearch, control relevance, metadata, and provenance. Log events in CloudTrail/CloudWatch, and apply IAM/KMS to protect pipelines and repositories.",
"incorrect_explanations": {
"A": "Optimization seeks to improve responses; poisoning aims at hostile manipulation of behavior.",
"B": "Prompt improvement does not inject malicious vectors; poisoning is adversarial.",
"D": "It does not speed up processing; it introduces a security and integrity risk."
}
},
{
"id": "aif-c01-ml_development-153",
"certification_id": "AIF-C01",
"domain": "ML_DEVELOPMENT",
"difficulty": "medium",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "What is the BLEU score used for in the evaluation of foundation models?",
"option_a": "To measure the model's energy efficiency",
"option_b": "To evaluate the quality of machine translations",
"option_c": "To calculate the model's processing speed",
"option_d": "To determine the model's market value",
"correct_answers": ["B"],
"explanation_detailed": "BLEU evaluates machine translation by comparing n-grams of the output with human reference translations and penalizing very short sentences (brevity penalty). It is objective and cheap, but limited: it does not capture synonyms or global fluency well. Use BLEU along with complementary metrics (COMET, BERTScore) and human review for production decisions. On AWS, run evaluations in jobs on Amazon SageMaker, store results in S3, and create dashboards in CloudWatch or OpenSearch. For practical translation workflows, Amazon Translate provides a managed service; for custom models, SageMaker JumpStart and Bedrock allow experimenting with alternatives that can then be evaluated with BLEU.",
"incorrect_explanations": {
"A": "Energy efficiency requires hardware/usage telemetry; BLEU measures textual overlap.",
"C": "Speed/latency is not measured by BLEU; it is a system performance metric.",
"D": "Market value is not a technical metric of translation quality."
}
},
{
"id": "aif-c01-ml_development-154",
"certification_id": "AIF-C01",
"domain": "ML_DEVELOPMENT",
"difficulty": "medium",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "What is a key benefit of using transfer learning for foundation model personalization?",
"option_a": "It requires no additional training",
"option_b": "It allows the model to leverage knowledge from one domain to another",
"option_c": "It always produces perfect results",
"option_d": "It reduces the model's size to zero",
"correct_answers": ["B"],
"explanation_detailed": "Transfer learning reuses representations learned during pre-training to accelerate adaptation to new tasks/domains with less data and cost. The model maintains general knowledge and adjusts final layers or specific parameters (e.g., LoRA) for style/terminology. On AWS, use Amazon SageMaker for controlled fine-tuning and reproducible experiments; JumpStart provides ready-made models and notebooks. Validate gains with business-specific metrics and evaluate bias/robustness with SageMaker Clarify. Control versions and access policies (Model Registry, IAM/KMS) and monitor post-deployment drift with Model Monitor.",
"incorrect_explanations": {
"A": "Even with transfer, there is training/adaptation, albeit less than from scratch.",
"C": "There is no guarantee of perfection; it depends on data, evaluation, and governance.",
"D": "It does not eliminate the model; it reduces training effort, not the size to zero."
}
},
{
"id": "aif-c01-ai_fundamentals-155",
"certification_id": "AIF-C01",
"domain": "AI_FUNDAMENTALS",
"difficulty": "medium",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "What is the primary purpose of the model's latent space in the context of prompt engineering?",
"option_a": "To physically store the model",
"option_b": "To represent the model's internal understanding and knowledge",
"option_c": "To increase the model's processing speed",
"option_d": "To reduce the model's energy consumption",
"correct_answers": ["B"],
"explanation_detailed": "The latent space is the internal representation where meanings, relationships, and patterns are encoded in vectors/activations. Effective prompts 'navigate' this space by conditioning the generation to desired regions (style, tone, format), without direct access to the vectors. In image models (diffusion), the latent space guides content/aesthetics; in LLMs, it structures the next token based on history. On AWS, you manipulate this behavior via Amazon Bedrock (inference parameters, few-shot) and integrate retrieval (Kendra/OpenSearch) for factual anchoring. Metrics and A/B tests in SageMaker/CloudWatch confirm if the prompt guidance truly moves the output to the desired region of the latent space.",
"incorrect_explanations": {
"A": "It is not a physical medium; it is a mathematical representation in the activations and weights.",
"C": "Speed depends on hardware and optimizations; the latent space represents semantics.",
"D": "Energy consumption is not a function of the latent space; it is of load/infrastructure."
}
},
{
"id": "aif-c01-responsible_ai-156",
"certification_id": "AIF-C01",
"domain": "RESPONSIBLE_AI",
"difficulty": "easy",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "Which of the following is NOT a characteristic of responsible AI?",
"option_a": "Fairness",
"option_b": "Robustness",
"option_c": "Profitability",
"option_d": "Inclusivity",
"correct_answers": ["C"],
"explanation_detailed": "Responsible AI is anchored in fairness, transparency, security, privacy, robustness, governance, and inclusion. 'Profitability' is a business objective, not an ethical principle. On AWS, combine Guardrails for Amazon Bedrock for content/PII filters, SageMaker Clarify to detect biases, Model Cards for documentation, and A2I for human review. Implement auditing (CloudTrail), access control (IAM/KMS), and continuous monitoring (Model Monitor). Have data policies and impact assessments. Focus on distributional effects, not just aggregate metrics.",
"incorrect_explanations": {
"A": "Fairness is central: equitable treatment across groups and contexts.",
"B": "Robustness ensures performance under variations and attacks.",
"D": "Inclusion mitigates systemic exclusions and errors in minority groups."
}
},
{
"id": "aif-c01-responsible_ai-157",
"certification_id": "AIF-C01",
"domain": "RESPONSIBLE_AI",
"difficulty": "medium",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "What is the primary purpose of Guardrails for Amazon Bedrock?",
"option_a": "To physically protect the AI hardware",
"option_b": "To identify and enforce responsible AI features",
"option_c": "To increase model performance",
"option_d": "To reduce energy consumption",
"correct_answers": ["B"],
"explanation_detailed": "Guardrails for Amazon Bedrock applies responsible use policies: PII filters, blocking of sensitive categories, toxicity limits, protection against prompt injection, and response compliance. It operates at the application layer, independent of the foundation model, with logs for auditing and iterative adjustments. Combine with IAM/KMS for security, CloudWatch for monitoring, and post-processing validations. In RAG flows, involve Kendra/OpenSearch with provenance and sanitation. Guardrails is not a performance or hardware tool; it is a content and security policy layer to keep outputs within business-defined standards.",
"incorrect_explanations": {
"A": "It does not deal with datacenter/hardware; it focuses on content and policy.",
"C": "It can prevent bad responses, but it is not a model acceleration mechanism.",
"D": "Energy consumption depends on load/infra; guardrails do not directly reduce it."
}
},
{
"id": "aif-c01-responsible_ai-158",
"certification_id": "AIF-C01",
"domain": "RESPONSIBLE_AI",
"difficulty": "medium",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "Which of the following is a key consideration in responsible model selection?",
"option_a": "The model's popularity",
"option_b": "The model's environmental impact",
"option_c": "The model's country of origin",
"option_d": "The model's color scheme",
"correct_answers": ["B"],
"explanation_detailed": "Responsible selection weighs performance, security, bias, privacy, IP risks, and environmental footprint (energy/CO₂). On AWS, choose models in Amazon Bedrock or SageMaker JumpStart considering context window, tool support, quality, cost per token, and regulatory requirements. Evaluate consumption and scalability by region (data residency), and adopt best practices from the Sustainability pillar of the Well-Architected Framework. Document justifications in Model Cards and run fairness tests with Clarify. 'Popularity' and aesthetics do not replace technical and ethical due diligence.",
"incorrect_explanations": {
"A": "Popularity does not indicate suitability for your case, risks, and costs.",
"C": "Geographic origin does not determine quality/ethics by itself; evaluate evidence.",
"D": "Colors have no relation to responsibility or performance."
}
},
{
"id": "aif-c01-responsible_ai-159",
"certification_id": "AIF-C01",
"domain": "RESPONSIBLE_AI",
"difficulty": "medium",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "What is a potential legal risk of working with generative AI?",
"option_a": "Physical injury to users",
"option_b": "Intellectual property infringement claims",
"option_c": "Increased electricity bills",
"option_d": "Reduced internet speed",
"correct_answers": ["B"],
"explanation_detailed": "Generated content can resemble protected works, use trademarks, or incorporate sensitive materials. Mitigate with data policies, content filters (Guardrails), similarity checks, authorized sources, and provenance in RAG (Kendra/OpenSearch). Document uses, licenses, and limitations in Model Cards. On AWS, restrict access with IAM/KMS, audit with CloudTrail, and monitor flows in CloudWatch. Establish human review (A2I) when there is high risk and define clear terms of use for internal clients. Also address privacy/PII and confidentiality of prompts and outputs.",
"incorrect_explanations": {
"A": "Generative AI operates digitally; the primary risk is legal/compliance, not physical.",
"C": "Energy cost is operational, not a direct legal risk.",
"D": "Bandwidth does not define legal IP exposure."
}
},
{
"id": "aif-c01-responsible_ai-160",
"certification_id": "AIF-C01",
"domain": "RESPONSIBLE_AI",
"difficulty": "easy",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "Which of the following is NOT a characteristic of important datasets for responsible AI?",
"option_a": "Inclusivity",
"option_b": "Diversity",
"option_c": "Size",
"option_d": "Balanced representation",
"correct_answers": ["C"],
"explanation_detailed": "Size helps, but does not guarantee fairness. Responsible datasets prioritize diversity of sources, balanced representation between groups, and documentation of provenance and consent. On AWS, manage data in S3/Lake Formation, catalog in Glue Data Catalog, apply IAM/KMS, and run bias assessments with SageMaker Clarify by subgroups. Record decisions and limitations in Model Cards and maintain update processes to correct gaps. Validate label quality and coverage, not just volume.",
"incorrect_explanations": {
"A": "Inclusivity reduces coverage gaps and systematic errors.",
"B": "Diversity mitigates sampling bias and improves generalization.",
"D": "Balancing prevents the model from favoring dominant classes/groups."
}
},
{
"id": "aif-c01-ai_fundamentals-161",
"certification_id": "AIF-C01",
"domain": "AI_FUNDAMENTALS",
"difficulty": "easy",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "What is overfitting in the context of AI models?",
"option_a": "When a model performs very well on training data but poorly on new data",
"option_b": "When a model is too large to fit into memory",
"option_c": "When a model generates outputs that are too long",
"option_d": "When a model consumes too much power",
"correct_answers": ["A"],
"explanation_detailed": "Overfitting occurs when the model learns noise and peculiarities from the training data, failing to generalize. Symptoms: large gap between training and validation metrics, unstable predictions. Mitigations: regularization, early stopping, data augmentation, cross-validation, and more representative data. On AWS, use SageMaker Experiments to track metrics, Debugger to detect training issues, and Model Monitor to observe drift after deployment. Document limitations in Model Cards and evaluate by subgroups with Clarify to understand where generalization is weak.",
"incorrect_explanations": {
"B": "Memory limitation is an infrastructure constraint, not the definition of overfitting.",
"C": "Output length is an inference choice; it does not characterize overfitting.",
"D": "Power consumption does not define the model's generalization."
}
},
{
"id": "aif-c01-ai_services-162",
"certification_id": "AIF-C01",
"domain": "AI_SERVICES",
"difficulty": "medium",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "Which AWS service is designed to help detect and monitor bias in machine learning models?",
"option_a": "Amazon EC2",
"option_b": "Amazon S3",
"option_c": "Amazon SageMaker Clarify",
"option_d": "Amazon RDS",
"correct_answers": ["C"],
"explanation_detailed": "SageMaker Clarify detects bias in data and models, generates reports by attributes (e.g., gender, region), produces explanations (SHAP), and supports monitoring in production. Integrate Clarify into ML pipelines in SageMaker to evaluate before deployment and use Model Monitor to continue checking after deployment. Combine with Model Cards to record context, metrics, and limitations, and with A2I when the decision requires human review. EC2, S3, and RDS do not provide this bias analysis natively.",
"incorrect_explanations": {
"A": "EC2 provides compute, not a native bias/explainability tool.",
"B": "S3 is object storage; it does not evaluate bias.",
"D": "RDS is a relational database; it does not measure model bias."
}
},
{
"id": "aif-c01-responsible_ai-163",
"certification_id": "AIF-C01",
"domain": "RESPONSIBLE_AI",
"difficulty": "easy",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "What is the main difference between transparent and non-transparent AI models?",
"option_a": "Transparent models are always more accurate",
"option_b": "Transparent models allow for understanding of their decision-making process",
"option_c": "Transparent models are always smaller in size",
"option_d": "Transparent models consume less power",
"correct_answers": ["B"],
"explanation_detailed": "Transparency means explaining how inputs influence outputs, which attributes matter, and under what conditions the model fails. Techniques: intrinsically interpretable models (linear, trees), local explanations (LIME/SHAP), reports (Model Cards), and data documentation. On AWS, Clarify generates explanations and bias detection; Model Cards record usage context; CloudTrail/Config audit changes. Transparency does not imply higher accuracy or lower cost; it is an attribute of governance and trust.",
"incorrect_explanations": {
"A": "Transparency does not guarantee superior accuracy; it may even trade-off with complexity.",
"C": "Size does not define interpretability; it depends on the structure and explanation techniques.",
"D": "Energy consumption is an infrastructure issue, not one of interpretability."
}
},
{
"id": "aif-c01-responsible_ai-164",
"certification_id": "AIF-C01",
"domain": "RESPONSIBLE_AI",
"difficulty": "medium",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "Which tool can be used to document model information for transparency?",
"option_a": "Amazon SageMaker Model Cards",
"option_b": "Amazon EC2",
"option_c": "Amazon S3",
"option_d": "Amazon RDS",
"correct_answers": ["A"],
"explanation_detailed": "SageMaker Model Cards consolidate purpose, training data, metrics, bias/robustness evaluations, risks, and usage recommendations, promoting transparency and governance. Integrate with Clarify to attach fairness/explainability results and with Model Registry for versioning and audit trail. Store artifacts in S3 and monitor in production with Model Monitor. EC2, S3, and RDS are useful for infrastructure but do not replace standardized documentation of the model lifecycle.",
"incorrect_explanations": {
"B": "EC2 provides compute; it is not a model documentation tool.",
"C": "S3 stores artifacts but does not structure model cards.",
"D": "RDS is a database; it does not provide model cards."
}
},
{
"id": "aif-c01-responsible_ai-165",
"certification_id": "AIF-C01",
"domain": "RESPONSIBLE_AI",
"difficulty": "medium",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "What is a potential trade-off between model security and transparency?",
"option_a": "More secure models are always less transparent",
"option_b": "Transparent models are always less secure",
"option_c": "Greater transparency can reveal vulnerabilities",
"option_d": "There are no trade-offs between security and transparency",
"correct_answers": ["C"],
"explanation_detailed": "Detailed explanations and extensive documentation can expose limits, sensitive features, or protocols, which facilitates attacks (e.g., model extraction, evasion, prompt injection). Mitigate by providing useful explanations to the user without leaking exploitable signals, aggregating information, and controlling access. On AWS, use IAM/KMS to segregate environments, CloudTrail for auditing, Guardrails for content policies, and continuous adversarial testing. Transparency and security are compatible but require deliberate design.",
"incorrect_explanations": {
"A": "It is not a general rule; good designs achieve security and transparency together.",
"B": "Transparency does not imply inevitable insecurity; it depends on scope and controls.",
"D": "There are practical trade-offs; denying this ignores operational risk."
}
},
{
"id": "aif-c01-responsible_ai-166",
"certification_id": "AIF-C01",
"domain": "RESPONSIBLE_AI",
"difficulty": "easy",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "What is human-centered design in the context of explainable AI?",
"option_a": "Designing AI systems that look like humans",
"option_b": "Creating AI systems that prioritize human needs and understanding",
"option_c": "Using humans instead of AI for all tasks",
"option_d": "Designing AI systems that can only be used by humans",
"correct_answers": ["B"],
"explanation_detailed": "Human-centered design adapts interfaces, explanations, and controls for safe and auditable decisions. Explain in clear language, show sources (RAG), allow for contestation and feedback, and provide calibrated confidence (scores, thresholds). On AWS, combine Bedrock + Kendra/OpenSearch for citability, Clarify for importing explanations, A2I for human review, and Model Cards to guide use. The goal is to make AI useful and understandable, not anthropomorphic.",
"incorrect_explanations": {
"A": "Human appearance is irrelevant to explainability.",
"C": "It does not eliminate AI; it balances automation and human intervention.",
"D": "Usage is not exclusive; it is about accessibility and understanding."
}
},
{
"id": "aif-c01-responsible_ai-167",
"certification_id": "AIF-C01",
"domain": "RESPONSIBLE_AI",
"difficulty": "easy",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "Which of the following is NOT a typical effect of bias in AI systems?",
"option_a": "Unfair treatment of certain demographic groups",
"option_b": "Improved overall accuracy",
"option_c": "Potential legal issues",
"option_d": "Loss of user trust",
"correct_answers": ["B"],
"explanation_detailed": "Bias tends to reduce accuracy in subgroups, generate unfairness, create legal risk, and erode trust. Behaviors include unbalanced false positives/negatives and inconsistent responses across populations. On AWS, use Clarify to measure bias, Model Monitor to monitor distribution drift, and A2I for human review in critical decisions. Mitigate with diverse data, resampling, reweighting, and fairness constraints, recording results in Model Cards.",
"incorrect_explanations": {
"A": "Unequal treatment is a typical consequence when data is biased.",
"C": "Fairness failures expose to litigation and regulatory scrutiny.",
"D": "Users lose trust when they perceive unfair/inconsistent results."
}
},
{
"id": "aif-c01-responsible_ai-168",
"certification_id": "AIF-C01",
"domain": "RESPONSIBLE_AI",
"difficulty": "medium",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "What is the primary purpose of subgroup analysis in responsible AI?",
"option_a": "To divide the development team into subgroups",
"option_b": "To analyze model performance across different demographic groups",
"option_c": "To reduce the model's size",
"option_d": "To increase the model's processing speed",
"correct_answers": ["B"],
"explanation_detailed": "Subgroup analysis measures performance by groups (e.g., age, region, language) to detect asymmetries and unfairness. Compare metrics (precision/recall, MAE) by group, apply statistical tests, and define minimum quality targets. On AWS, use Clarify for reports by attribute, and Model Monitor to observe deviations over time. Record limitations in Model Cards and implement corrective actions (data recollection, reweighting, new features).",
"incorrect_explanations": {
"A": "It is not about organizational charts; it is a statistical evaluation by population.",
"C": "Model size does not by itself solve imbalances by groups.",
"D": "Speed does not evidence fairness; focus on metrics per subgroup."
}
},
{
"id": "aif-c01-responsible_ai-169",
"certification_id": "AIF-C01",
"domain": "RESPONSIBLE_AI",
"difficulty": "easy",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "Which of the following is a key consideration for dataset diversity in responsible AI?",
"option_a": "Using data from only one source",
"option_b": "Ensuring representation of various demographic groups",
"option_c": "Using the largest dataset available, regardless of content",
"option_d": "Using only the most recent data",
"correct_answers": ["B"],
"explanation_detailed": "Data diversity reduces sampling bias and improves generalization. Seek varied origins, scenarios, and languages/dialects, with documentation of provenance, consent, and retention policies. On AWS, catalog with Glue Data Catalog, control access with Lake Formation/IAM, and use Clarify to measure coverage by group. Prioritize quality and representativeness, not just volume or recency.",
"incorrect_explanations": {
"A": "A single source increases the risk of bias and low coverage.",
"C": "Larger volume without curation does not ensure fairness.",
"D": "Recency helps, but does not replace diversity and coverage."
}
},
{
"id": "aif-c01-responsible_ai-170",
"certification_id": "AIF-C01",
"domain": "RESPONSIBLE_AI",
"difficulty": "easy",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "What is truthfulness in the context of responsible AI?",
"option_a": "The speed at which the AI system operates",
"option_b": "The truthfulness and accuracy of the AI system's outputs",
"option_c": "The size of the AI model",
"option_d": "The cost of operating the AI system",
"correct_answers": ["B"],
"explanation_detailed": "Truthfulness requires correct, verifiable, and consistent responses. In RAG solutions, include citable sources (Kendra/OpenSearch) in the prompt to increase factual fidelity. Apply guardrails against prohibited information and human validation processes (A2I) when necessary. In production, track accuracy, correction rates, and error reports in CloudWatch, and maintain Model Cards describing usage limits and reliability.",
"incorrect_explanations": {
"A": "Speed is latency; it does not measure factual truth.",
"C": "Size does not imply greater truthfulness.",
"D": "Cost is operational; truthfulness is informational quality."
}
},
{
"id": "aif-c01-responsible_ai-171",
"certification_id": "AIF-C01",
"domain": "RESPONSIBLE_AI",
"difficulty": "medium",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "Which of the following is NOT a typical method for improving model interpretability?",
"option_a": "Using simpler models",
"option_b": "Providing feature importance ratings",
"option_c": "Increasing the model's size",
"option_d": "Generating human-readable explanations",
"correct_answers": ["C"],
"explanation_detailed": "Interpretability improves with simpler structures (trees, regressions), local explanations (LIME/SHAP), and clear reports (Model Cards). Increasing size tends to reduce transparency. On AWS, Clarify generates importances/SHAP and Model Cards document purpose, data, and limits. In high-risk applications, combine human review (A2I) with explanations and rules for auditable decisions.",
"incorrect_explanations": {
"A": "Simple models facilitate understanding and auditing.",
"B": "Importances help see decision drivers.",
"D": "Readable explanations connect the technical to the end-user."
}
},
{
"id": "aif-c01-ai_services-172",
"certification_id": "AIF-C01",
"domain": "AI_SERVICES",
"difficulty": "easy",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "What is the primary purpose of Amazon Augmented AI (A2I) in responsible AI?",
"option_a": "To replace human workers with AI",
"option_b": "To facilitate human review of AI predictions",
"option_c": "To increase the size of the AI model",
"option_d": "To reduce the energy consumption of AI systems",
"correct_answers": ["B"],
"explanation_detailed": "Amazon A2I inserts humans into the loop to review AI predictions in low-confidence scenarios, audit samplings, or exceptions. It integrates with SageMaker endpoints, Model Monitor, and labeling flows. Define routing policies (thresholds), collect feedback, and use it to correct data, refine prompts, or adjust models. In regulated applications, A2I helps demonstrate governance and effective human control.",
"incorrect_explanations": {
"A": "The goal is not to replace humans; it is to supervise and validate the AI.",
"C": "Model size is not affected by A2I.",
"D": "Energy is not the focus; it is quality and compliance."
}
},
{
"id": "aif-c01-responsible_ai-173",
"certification_id": "AIF-C01",
"domain": "RESPONSIBLE_AI",
"difficulty": "medium",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "Which of the following is a key consideration when evaluating the fairness of an AI system?",
"option_a": "The system's processing speed",
"option_b": "The system's power consumption",
"option_c": "The system's impact on different demographic groups",
"option_d": "The system's popularity among users",
"correct_answers": ["C"],
"explanation_detailed": "Fairness requires measuring performance by subgroups, investigating disparities, understanding causes (data/algorithm), and applying mitigation. On AWS, use Clarify for reports by attribute, Model Monitor for continuous surveillance, and Model Cards to document limits. Connect technical metrics to real consequences (costly errors for specific groups) and adopt human review policies when uncertainty is high.",
"incorrect_explanations": {
"A": "Latency does not measure equitable treatment.",
"B": "Energy is sustainability, not fairness.",
"D": "Popularity does not reveal bias or unequal impact."
}
},
{
"id": "aif-c01-ai_fundamentals-174",
"certification_id": "AIF-C01",
"domain": "AI_FUNDAMENTALS",
"difficulty": "easy",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "What is underfitting in the context of AI models?",
"option_a": "When a model is too small to fit into memory",
"option_b": "When a model performs poorly on both training data and new data",
"option_c": "When a model generates outputs that are too short",
"option_d": "When a model consumes little power",
"correct_answers": ["B"],
"explanation_detailed": "Underfitting occurs when the model is too simple to capture patterns, yielding high error on both training and validation. Mitigate with more expressive models, better features, and more epochs. On AWS, use SageMaker Experiments to compare architectures/hyperparameters and Debugger to identify bottlenecks. Monitor metrics in CloudWatch and record results and limitations in Model Cards.",
"incorrect_explanations": {
"A": "Insufficient memory does not define underfitting; it is an operational limitation.",
"C": "Output length does not indicate a lack of modeling capacity.",
"D": "Low energy consumption does not characterize underfitting."
}
},
{
"id": "aif-c01-responsible_ai-175",
"certification_id": "AIF-C01",
"domain": "RESPONSIBLE_AI",
"difficulty": "medium",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "Which of the following is NOT a typical benefit of using open-source models for transparency?",
"option_a": "Ability to inspect the model's code",
"option_b": "Community-driven improvements",
"option_c": "Guaranteed perfect performance",
"option_d": "Potential for independent audits",
"correct_answers": ["C"],
"explanation_detailed": "Open-source models allow for inspection, external audits, reproducibility, and customization, which aids transparency and trust. They do not guarantee perfect accuracy. On AWS, host training on SageMaker/EC2, control access via IAM, log experiments and metrics, and create Model Cards. Evaluate licenses and IP risks and apply Clarify for bias measurements and explanations, regardless of whether they are open or proprietary.",
"incorrect_explanations": {
"A": "Open source enables detailed inspection and reviews.",
"B": "The community can discover bugs and improve performance.",
"D": "Available code facilitates independent audits and governance."
}
},
{
"id": "aif-c01-ml_development-176",
"certification_id": "AIF-C01",
"domain": "ML_DEVELOPMENT",
"difficulty": "medium",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "What is the primary purpose of analyzing label quality in responsible AI?",
"option_a": "To improve the visual appearance of labels",
"option_b": "To ensure the accuracy and consistency of data labels",
"option_c": "To reduce the number of labels used",
"option_d": "To increase the model's processing speed",
"correct_answers": ["B"],
"explanation_detailed": "Poor quality labels generate noise and bias. Evaluate inter-annotator agreement, distributions by class, and ambiguous examples. On AWS, use SageMaker Ground Truth for labeling with quality control, A2I for human review, and Clarify to check the impact of labels on metrics by subgroup. Monitor label drift after deployment with Model Monitor and update datasets as new patterns emerge.",
"incorrect_explanations": {
"A": "Aesthetics are irrelevant; accuracy/consistency matters.",
"C": "Fewer labels does not solve the problem; quality and coverage are critical.",
"D": "Speed comes from infrastructure; label quality affects accuracy."
}
},
{
"id": "aif-c01-responsible_ai-177",
"certification_id": "AIF-C01",
"domain": "RESPONSIBLE_AI",
"difficulty": "easy",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "Which of the following is a potential consequence of using biased datasets in AI training?",
"option_a": "Improved model performance for all groups",
"option_b": "Unfair or discriminatory outcomes for certain groups",
"option_c": "Reduced energy consumption",
"option_d": "Faster model training times",
"correct_answers": ["B"],
"explanation_detailed": "Biased data leads the model to replicate asymmetries, producing unfair decisions for underrepresented groups. On AWS, use Clarify to measure bias and simulate impacts, A2I for human review of critical cases, and Model Cards to record risks. Update datasets with diverse sources and perform reweighting/resampling. Continuously monitor metrics by subgroup in production with Model Monitor.",
"incorrect_explanations": {
"A": "Performance may worsen for entire segments, not improve for all.",
"C": "Energy is not a direct consequence of data bias.",
"D": "Training time is not determined by the fairness of the data."
}
},
{
"id": "aif-c01-responsible_ai-178",
"certification_id": "AIF-C01",
"domain": "RESPONSIBLE_AI",
"difficulty": "medium",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "What is the primary purpose of responsible practices in model selection?",
"option_a": "To always choose the largest available model",
"option_b": "To select models based solely on performance metrics",
"option_c": "To balance performance with ethical considerations and sustainability",
"option_d": "To choose the most expensive model",
"correct_answers": ["C"],
"explanation_detailed": "Responsible selection weighs accuracy, cost, privacy, bias, robustness, and environmental footprint. On AWS, compare FMs from Bedrock and options in SageMaker JumpStart by evaluating context window, tools, security, cost per token, and legal requirements. Document decisions in Model Cards and test for fairness with Clarify. Operationalize governance with CloudTrail/Config and periodic reviews.",
"incorrect_explanations": {
"A": "Bigger is not always better; cost/latency can make it unfeasible.",
"B": "Performance alone ignores risks and compliance.",
"D": "Price is not a proxy for suitability or responsibility."
}
},
{
"id": "aif-c01-responsible_ai-179",
"certification_id": "AIF-C01",
"domain": "RESPONSIBLE_AI",
"difficulty": "medium",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "Which of the following is NOT a typical characteristic of a curated data source for responsible AI?",
"option_a": "Verified accuracy",
"option_b": "Known provenance",
"option_c": "Largest possible size",
"option_d": "Ethical collection methods",
"correct_answers": ["C"],
"explanation_detailed": "Curation prioritizes quality, consent, licenses, traceability, and representativeness. Size by itself does not guarantee fairness or factuality. On AWS, store in S3 with Lake Formation policies, catalog in Glue, protect with KMS/IAM, and document in data dictionaries. Audit flows with CloudTrail and evaluate bias/impacts with Clarify. Prefer less high-quality data over more ungoverned data.",
"incorrect_explanations": {
"A": "Accuracy verification is a requirement for reliability.",
"B": "Clear provenance enables auditing and compliance.",
"D": "Ethical collection reduces legal and reputational risks."
}
},
{
"id": "aif-c01-responsible_ai-180",
"certification_id": "AIF-C01",
"domain": "RESPONSIBLE_AI",
"difficulty": "medium",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "What is the primary purpose of human audits in responsible AI systems?",
"option_a": "To replace AI systems with human workers",
"option_b": "To verify and validate the outputs and processes of the AI system",
"option_c": "To increase the processing speed of the AI system",
"option_d": "To reduce the energy consumption of the AI system",
"correct_answers": ["B"],
"explanation_detailed": "Human audits review processes, data, models, and outputs to verify compliance, ethics, and real-world performance. They include case sampling, replicating results, checking logs, and validating policies. On AWS, extract trails with CloudTrail, metrics in CloudWatch, reports from Clarify, and documentation from Model Cards; collect reviews via A2I. Independent audits increase trust and meet regulatory requirements.",
"incorrect_explanations": {
"A": "The goal is to oversee and validate, not to replace all automation.",
"C": "Speed is not the target; it is quality and compliance.",
"D": "Energy is not the focus of an AI audit per se."
}
},
{
"id": "aif-c01-responsible_ai-181",
"certification_id": "AIF-C01",
"domain": "RESPONSIBLE_AI",
"difficulty": "easy",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "Which AWS service is primarily used to manage access and permissions for AI systems?",
"option_a": "Amazon S3",
"option_b": "AWS IAM",
"option_c": "Amazon EC2",
"option_d": "Amazon RDS",
"correct_answers": ["B"],
"explanation_detailed": "AWS Identity and Access Management (IAM) defines users, roles, policies, and granular permissions for resources (S3, SageMaker, Bedrock, KMS). It is the foundation of security 'in' the cloud. Combine with KMS for keys, CloudTrail for auditing, and SCPs/Organizations for control at scale. Control access to training data, endpoints, and model artifacts. S3/EC2/RDS are data/compute services, not the central authorization mechanism.",
"incorrect_explanations": {
"A": "S3 stores objects; permissions are governed via IAM/bucket policies.",
"C": "EC2 is compute; it does not manage identities/permissions.",
"D": "RDS is a relational database; access is mediated by IAM/credentials."
}
},
{
"id": "aif-c01-responsible_ai-182",
"certification_id": "AIF-C01",
"domain": "RESPONSIBLE_AI",
"difficulty": "medium",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "What is the primary purpose of Amazon Macie in AI security?",
"option_a": "To generate AI models",
"option_b": "To discover and protect sensitive data",
"option_c": "To increase model performance",
"option_d": "To reduce energy consumption",
"correct_answers": ["B"],
"explanation_detailed": "Amazon Macie automatically identifies PII and sensitive data in S3 buckets using ML and pattern matching, flagging exposures and policy changes. Integrate with CloudWatch/CloudTrail for alerts, apply KMS/IAM/Lake Formation for remediation, and log remedies. In AI pipelines, Macie helps ensure that data used in training/inference is adequately protected.",
"incorrect_explanations": {
"A": "Macie does not train models for you; it focuses on data discovery/protection.",
"C": "Model performance is not within Macie's scope.",
"D": "Energy is not the target; it is data security."
}
},
{
"id": "aif-c01-responsible_ai-183",
"certification_id": "AIF-C01",
"domain": "RESPONSIBLE_AI",
"difficulty": "easy",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "What does the AWS Shared Responsibility Model refer to?",
"option_a": "Sharing AI models between customers",
"option_b": "Division of security responsibilities between AWS and the customer",
"option_c": "Sharing costs between AWS and the customer",
"option_d": "Division of AI tasks between humans and machines",
"correct_answers": ["B"],
"explanation_detailed": "AWS protects the global infrastructure ('of' the cloud); customers protect data, configurations, identities, and applications ('in' the cloud). For AI, customers define data policies, keys (KMS), access (IAM), guardrails, and auditing (CloudTrail). AWS offers secure services, but correct configuration and operation are the customer's responsibility. Align responsibilities in documentation and Model Cards when models expose specific risks.",
"incorrect_explanations": {
"A": "It does not deal with exchanging models between customers.",
"C": "Costing does not define security and governance.",
"D": "Human vs. machine is product design, not the responsibility model."
}
},
{
"id": "aif-c01-responsible_ai-184",
"certification_id": "AIF-C01",
"domain": "RESPONSIBLE_AI",
"difficulty": "easy",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "Which of the following is NOT a typical method for securing AI systems?",
"option_a": "Encryption",
"option_b": "Access control",
"option_c": "Public data sharing",
"option_d": "Vulnerability management",
"correct_answers": ["C"],
"explanation_detailed": "Protection includes encryption (KMS), access control (IAM/Lake Formation), vulnerability scanning (Inspector), monitoring (GuardDuty/CloudTrail), and network segmentation (VPC/WAF). Sharing data publicly amplifies risk. In AI, apply guardrails in Bedrock, sanitization in RAG, and human review (A2I) for critical decisions. Maintain an audit trail and documented incident responses.",
"incorrect_explanations": {
"A": "Encryption is a foundation for confidentiality and compliance.",
"B": "Least privilege access reduces attack surfaces.",
"D": "Unpatched vulnerabilities expose data and pipelines."
}
},
{
"id": "aif-c01-responsible_ai-185",
"certification_id": "AIF-C01",
"domain": "RESPONSIBLE_AI",
"difficulty": "medium",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "What is data lineage in the context of AI security?",
"option_a": "A method of data encryption",
"option_b": "Tracking the origin and transformations of data",
"option_c": "A type of AI model architecture",
"option_d": "A way to increase data processing speed",
"correct_answers": ["B"],
"explanation_detailed": "Data lineage maps where data came from, what transformations it underwent, and who accessed/altered it. It is essential for auditing, reproducibility, and incident investigation. On AWS, catalog in Glue Data Catalog, govern with Lake Formation, log jobs in Glue/Athena, and use CloudTrail/CloudWatch for access and execution trails. For AI, link datasets/versions in SageMaker Model Registry and document in Model Cards.",
"incorrect_explanations": {
"A": "Encryption protects; lineage explains trajectory and use.",
"C": "Model architecture is not data tracking.",
"D": "Speed is a performance issue, not lineage."
}
},
{
"id": "aif-c01-responsible_ai-186",
"certification_id": "AIF-C01",
"domain": "RESPONSIBLE_AI",
"difficulty": "medium",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "Which AWS service is used to detect security threats in AI systems?",
"option_a": "Amazon Macie",
"option_b": "Amazon S3",
"option_c": "Amazon EC2",
"option_d": "Amazon RDS",
"correct_answers": ["A"],
"explanation_detailed": "Macie finds exposed PII and sensitive data in S3 and alerts on risky configurations, which helps prevent leaks and abuse in AI pipelines. Combine with CloudWatch/CloudTrail for alarms and with Lake Formation/IAM/KMS for remediation. For threat detection in accounts and traffic, it is complemented by GuardDuty; for vulnerabilities in workloads, use Inspector. The focus here is on protecting sensitive data used by AI.",
"incorrect_explanations": {
"B": "S3 stores; it does not detect threats by itself.",
"C": "EC2 is compute, not a threat detection service.",
"D": "RDS is a relational database; it does not perform security scans."
}
},
{
"id": "aif-c01-responsible_ai-187",
"certification_id": "AIF-C01",
"domain": "RESPONSIBLE_AI",
"difficulty": "medium",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "What is prompt injection in the context of AI security?",
"option_a": "A method of optimizing prompts",
"option_b": "A security vulnerability where malicious input manipulates the AI's behavior",
"option_c": "A technique to speed up AI processing",
"option_d": "A way to reduce the AI's energy consumption",
"correct_answers": ["B"],
"explanation_detailed": "Prompt injection tricks the model into ignoring original instructions and following malicious commands present in the input or in retrieved documents (RAG). Mitigate with clear role delimitation, content sanitation, allow/deny lists, post-generation validations, and Guardrails for Amazon Bedrock to filter prohibited instructions/PII. Monitor usage and incidents via CloudTrail/CloudWatch and restrict to trusted sources with Kendra/OpenSearch with metadata.",
"incorrect_explanations": {
"A": "Optimizing a prompt aims for quality; injection is an attack.",
"C": "It does not speed up; it alters behavior for malicious purposes.",
"D": "Energy is not directly affected; it is an integrity risk."
}
},
{
"id": "aif-c01-responsible_ai-188",
"certification_id": "AIF-C01",
"domain": "RESPONSIBLE_AI",
"difficulty": "easy",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "Which of the following is NOT a typical regulatory compliance standard for AI systems?",
"option_a": "ISO",
"option_b": "SOC",
"option_c": "HTML",
"option_d": "Algorithm liability laws",
"correct_answers": ["C"],
"explanation_detailed": "Compliance involves standards like ISO/IEC (e.g., 27001), SOC reports, and algorithmic liability legislation. HTML is a markup language, not a compliance standard. On AWS, access reports in AWS Artifact, log audits with CloudTrail, monitor configurations with Config, and collect evidence with Audit Manager. Maintain Model Cards, policies, and trails to demonstrate continuous compliance.",
"incorrect_explanations": {
"A": "ISO covers security/management practices; it is relevant.",
"B": "SOC evaluates organizational controls; it is an accepted standard.",
"D": "Algorithmic regulations are increasingly required."
}
},
{
"id": "aif-c01-responsible_ai-189",
"certification_id": "AIF-C01",
"domain": "RESPONSIBLE_AI",
"difficulty": "easy",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "Which AWS service is used for continuous monitoring and evaluation of resources?",
"option_a": "Amazon EC2",
"option_b": "AWS Config",
"option_c": "Amazon S3",
"option_d": "Amazon RDS",
"correct_answers": ["B"],
"explanation_detailed": "AWS Config tracks resource configurations and compares them to rules/policies, generating a history and notifications of deviations. It is useful for governance of AI pipelines (S3, SageMaker, KMS, IAM), ensuring environments remain within defined standards. Integrate with CloudWatch Events/Rules for automations and with CloudTrail for complete audits.",
"incorrect_explanations": {
"A": "EC2 runs workloads; compliance monitoring is done by Config.",
"C": "S3 stores; it does not evaluate resource compliance.",
"D": "RDS is a database; it does not audit global configurations."
}
},
{
"id": "aif-c01-responsible_ai-190",
"certification_id": "AIF-C01",
"domain": "RESPONSIBLE_AI",
"difficulty": "easy",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "What is the primary purpose of AWS Artifact in AI governance?",
"option_a": "To generate AI models",
"option_b": "To provide access to AWS compliance reports",
"option_c": "To increase model performance",
"option_d": "To reduce energy consumption",
"correct_answers": ["B"],
"explanation_detailed": "AWS Artifact provides compliance reports (ISO, SOC, PCI, etc.) and agreements to support audits and due diligence. Combine with Audit Manager to collect evidence of internal controls, CloudTrail for API trails, and Config for resource posture. Use these artifacts to demonstrate governance of AI solutions that rely on AWS infrastructure.",
"incorrect_explanations": {
"A": "Artifact does not train models; it is a compliance repository.",
"C": "Model performance is not Artifact's objective.",
"D": "Energy is not the focus; it is compliance."
}
},
{
"id": "aif-c01-responsible_ai-191",
"certification_id": "AIF-C01",
"domain": "RESPONSIBLE_AI",
"difficulty": "easy",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "Which of the following is NOT normally part of a data governance strategy?",
"option_a": "Data lifecycle management",
"option_b": "Data retention policies",
"option_c": "Public data sharing policies",
"option_d": "Data monitoring",
"correct_answers": ["C"],
"explanation_detailed": "Data governance defines roles, access policies, retention, quality, and monitoring, prioritizing security and compliance. Indiscriminate public sharing contradicts protection principles. On AWS, implement Lake Formation/IAM/KMS, catalogs in Glue, and auditing via CloudTrail/Config. Document processes and maintain evidence with Audit Manager.",
"incorrect_explanations": {
"A": "Lifecycle (creation, use, retention, deletion) is central.",
"B": "Retention meets laws and minimizes risk.",
"D": "Monitoring detects deviations and incidents."
}
},
{
"id": "aif-c01-responsible_ai-192",
"certification_id": "AIF-C01",
"domain": "RESPONSIBLE_AI",
"difficulty": "easy",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "What is the primary purpose of AWS CloudTrail in AI governance?",
"option_a": "To generate AI models",
"option_b": "To record API calls and account activity",
"option_c": "To increase model performance",
"option_d": "To reduce energy consumption",
"correct_answers": ["B"],
"explanation_detailed": "CloudTrail records who did what, when, and from where, creating an audit trail for resources (S3, SageMaker, Bedrock, IAM). It is essential for investigations, compliance, and change controls. Integrate with CloudWatch Logs/Events for alerts and automated responses. For governance, combine with Config, Audit Manager, and Model Cards for a complete view of the model lifecycle.",
"incorrect_explanations": {
"A": "It does not train models; it provides API auditing.",
"C": "Performance is not CloudTrail's focus.",
"D": "Energy consumption is not monitored by CloudTrail."
}
},
{
"id": "aif-c01-responsible_ai-193",
"certification_id": "AIF-C01",
"domain": "RESPONSIBLE_AI",
"difficulty": "medium",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "Which of the following is a key consideration in secure data engineering for AI?",
"option_a": "Maximizing data collection without considering quality",
"option_b": "Implementing privacy-enhancing technologies",
"option_c": "Making all data publicly accessible",
"option_d": "Using only unencrypted data storage",
"correct_answers": ["B"],
"explanation_detailed": "Secure engineering includes encryption (KMS), access control (IAM/Lake Formation), masking/anonymization, data minimization, and environment segregation. On AWS, catalog in Glue, store in S3 with policies, audit with CloudTrail, and monitor posture with Config. Privacy-enhancing technologies reduce risk and exposure bias without blocking legitimate use cases.",
"incorrect_explanations": {
"A": "Collecting without criteria increases risk and cost, without ensuring quality.",
"C": "Public access is the opposite of data security.",
"D": "Storing without encryption violates best practices."
}
},
{
"id": "aif-c01-responsible_ai-194",
"certification_id": "AIF-C01",
"domain": "RESPONSIBLE_AI",
"difficulty": "medium",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "What is the primary purpose of the Generative AI Security Scoping Matrix?",
"option_a": "To generate AI models",
"option_b": "To provide a framework for assessing AI security risks",
"option_c": "To increase model performance",
"option_d": "To reduce energy consumption",
"correct_answers": ["B"],
"explanation_detailed": "A generative AI security scoping matrix maps risks by layers (data, model, orchestration, interface) and by responsibilities (team, provider, customer), guiding controls: guardrails, PII, provenance, abuse detection, and auditing. On AWS, apply Bedrock Guardrails, IAM/KMS, CloudTrail/Config, Kendra/OpenSearch with metadata, and A2I for human review. The goal is to systematically evaluate and prioritize mitigation.",
"incorrect_explanations": {
"A": "It does not create models; it structures risk assessment.",
"C": "Performance is not the primary focus; it is security.",
"D": "Energy is not the target of the security matrix."
}
},
{
"id": "aif-c01-responsible_ai-195",
"certification_id": "AIF-C01",
"domain": "RESPONSIBLE_AI",
"difficulty": "easy",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "Which AWS service is used for automated security assessments?",
"option_a": "Amazon EC2",
"option_b": "Amazon Inspector",
"option_c": "Amazon S3",
"option_d": "Amazon RDS",
"correct_answers": ["B"],
"explanation_detailed": "Amazon Inspector continuously assesses workloads (EC2, ECR, Lambda) for known vulnerabilities and network exposure. In AI environments, it helps maintain clean images and secure hosts for data/training pipelines. Combine with GuardDuty for threat detection, Config for posture, and CloudTrail for auditing. Remediate findings with automations and change control.",
"incorrect_explanations": {
"A": "EC2 is compute; Inspector is what assesses vulnerabilities.",
"C": "S3 stores data; it does not perform CVE checks.",
"D": "RDS does not perform security scans of workloads."
}
},
{
"id": "aif-c01-responsible_ai-196",
"certification_id": "AIF-C01",
"domain": "RESPONSIBLE_AI",
"difficulty": "easy",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "What is data residency in the context of AI governance?",
"option_a": "The physical location where data is stored",
"option_b": "The duration for which data is kept",
"option_c": "The speed at which data is processed",
"option_d": "The format in which data is stored",
"correct_answers": ["A"],
"explanation_detailed": "Data residency specifies the region/country where data is stored and under which laws it falls. On AWS, choose regions, use S3/Lake Formation policies and replication restrictions, and document requirements in Model Cards and policies. For AI, confirm that training/inference data complies with location and transfer rules.",
"incorrect_explanations": {
"B": "This is retention, not residency.",
"C": "Speed is performance; residency is legal location.",
"D": "Format describes schema/serialization, not location."
}
},
{
"id": "aif-c01-responsible_ai-197",
"certification_id": "AIF-C01",
"domain": "RESPONSIBLE_AI",
"difficulty": "easy",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "Which of the following is NOT a typical consideration in the security of AI applications?",
"option_a": "Threat detection",
"option_b": "Vulnerability management",
"option_c": "Maximizing public data sharing",
"option_d": "Infrastructure protection",
"correct_answers": ["C"],
"explanation_detailed": "Security of AI applications requires monitoring threats (GuardDuty), managing vulnerabilities (Inspector), protecting infrastructure (VPC, WAF, IAM/KMS), and controlling data (Macie, Lake Formation). Sharing data publicly goes against confidentiality and compliance. Add guardrails in Bedrock and audit trails (CloudTrail).",
"incorrect_explanations": {
"A": "Threat detection is an essential pillar.",
"B": "Unpatched vulnerabilities expose the environment.",
"D": "Protected infrastructure reduces the attack surface."
}
},
{
"id": "aif-c01-responsible_ai-198",
"certification_id": "AIF-C01",
"domain": "RESPONSIBLE_AI",
"difficulty": "easy",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "What is the primary purpose of AWS Trusted Advisor in AI governance?",
"option_a": "To generate AI models",
"option_b": "To provide real-time guidance to improve the AWS environment",
"option_c": "To increase model performance",
"option_d": "To reduce energy consumption",
"correct_answers": ["B"],
"explanation_detailed": "Trusted Advisor provides recommendations for cost, performance, security, fault tolerance, and service limits. In AI environments, it helps maintain healthy accounts: excessive permissions, underutilized resources, backups, and quotas. Combine with Config/CloudTrail for continuous governance and with Guardrails/Clarify for AI-specific responsibility layers.",
"incorrect_explanations": {
"A": "It does not train models; it evaluates account best practices.",
"C": "Model performance is not the target; it is environment posture.",
"D": "Energy is not a direct focus of Trusted Advisor."
}
},
{
"id": "aif-c01-responsible_ai-199",
"certification_id": "AIF-C01",
"domain": "RESPONSIBLE_AI",
"difficulty": "easy",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "Which of the following is a key aspect of data integrity in AI systems?",
"option_a": "Ensuring data remains unchanged and uncorrupted",
"option_b": "Making all data publicly accessible",
"option_c": "Using only the largest available datasets",
"option_d": "Storing all data in a single location",
"correct_answers": ["A"],
"explanation_detailed": "Integrity ensures that data is not improperly altered. Use versioning (S3), checksums, access logs (CloudTrail), encryption (KMS), and controlled change policies (Lake Formation/IAM). For AI, link datasets and models in the Model Registry and document transformations in Glue to track consistency between data and results.",
"incorrect_explanations": {
"B": "Public access compromises confidentiality and integrity.",
"C": "Size does not ensure data integrity.",
"D": "Concentrating in one location creates risk; integrity requires control, not blind centralization."
}
},
{
"id": "aif-c01-responsible_ai-200",
"certification_id": "AIF-C01",
"domain": "RESPONSIBLE_AI",
"difficulty": "easy",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "What is the primary purpose of encryption at rest in AI security?",
"option_a": "To protect data while it is being transmitted",
"option_b": "To protect stored data",
"option_c": "To increase data processing speed",
"option_d": "To reduce energy consumption",
"correct_answers": ["B"],
"explanation_detailed": "Encryption at rest protects stored data (S3, EBS, RDS) using keys managed by AWS KMS. In AI pipelines, ensure that datasets, training artifacts, and models are encrypted. Combine with encryption in transit (TLS), IAM for least privilege access, and auditing via CloudTrail. This reduces the risk of exposure in case of media loss or unauthorized access.",
"incorrect_explanations": {
"A": "In transit is TLS; at rest is storage.",
"C": "Encryption does not speed up processing; it protects confidentiality.",
"D": "Energy is not the target of encryption; it is a security control."
}
},
{
"id": "aif-c01-responsible_ai-201",
"certification_id": "AIF-C01",
"domain": "RESPONSIBLE_AI",
"difficulty": "medium",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "Which of the following is NOT normally part of the governance protocols for AI systems?",
"option_a": "Regular policy reviews",
"option_b": "Staff training requirements",
"option_c": "Maximizing model complexity",
"option_d": "Transparency standards",
"correct_answers": ["C"],
"explanation_detailed": "Governance defines policies, forms teams, creates transparency standards (Model Cards), runs periodic audits, and measures risks. Model complexity is not a governance goal. On AWS, use Audit Manager, Artifact, CloudTrail, Config, and logs from Clarify/Model Monitor for evidence. Periodically review guardrails and human review flows.",
"incorrect_explanations": {
"A": "Reviews keep policies aligned with real risks.",
"B": "Training enables safe and ethical operation.",
"D": "Transparency is a pillar for trust and auditing."
}
},
{
"id": "aif-c01-responsible_ai-202",
"certification_id": "AIF-C01",
"domain": "RESPONSIBLE_AI",
"difficulty": "medium",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "What is the primary purpose of AWS Audit Manager in AI governance?",
"option_a": "To generate AI models",
"option_b": "To continuously audit AWS usage for compliance",
"option_c": "To increase model performance",
"option_d": "To reduce energy consumption",
"correct_answers": ["B"],
"explanation_detailed": "AWS Audit Manager automatically collects evidence from multiple services (CloudTrail, Config, etc.) to assess compliance against frameworks. In AI, this supports audits of data, access, training, and inference. Combine with Artifact for official reports, with Model Cards for model documentation, and with Clarify/Model Monitor for attached metrics.",
"incorrect_explanations": {
"A": "Audit Manager does not train models; it collects evidence.",
"C": "Model performance is not the service's scope.",
"D": "Energy is not the focus; it is compliance."
}
},
{
"id": "aif-c01-responsible_ai-203",
"certification_id": "AIF-C01",
"domain": "RESPONSIBLE_AI",
"difficulty": "medium",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "Which of the following is a key consideration in protecting AI infrastructure?",
"option_a": "Maximizing public access to AI systems",
"option_b": "Implementing network security measures",
"option_c": "Using only the largest available models",
"option_d": "Storing all data in a single location",
"correct_answers": ["B"],
"explanation_detailed": "Network security includes segmentation (VPC), access control lists (NACLs), Security Groups, WAF, and DDoS protection. In AI pipelines, isolate SageMaker/Bedrock endpoints behind API and authentication layers, log traffic (VPC Flow Logs), and audit with CloudTrail. Combine with IAM/KMS and continuous monitoring to reduce attack surfaces.",
"incorrect_explanations": {
"A": "Public access increases the risk of exploitation.",
"C": "Model size does not protect infrastructure.",
"D": "Centralizing everything increases risk; prefer controls and redundancy."
}
},
{
"id": "aif-c01-responsible_ai-204",
"certification_id": "AIF-C01",
"domain": "RESPONSIBLE_AI",
"difficulty": "easy",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "What is the primary purpose of the data catalog in AI governance?",
"option_a": "To make all data publicly accessible",
"option_b": "To organize and inventory data assets",
"option_c": "To increase data processing speed",
"option_d": "To reduce data storage costs",
"correct_answers": ["B"],
"explanation_detailed": "A catalog (e.g., AWS Glue Data Catalog) describes datasets, schemas, locations, owners, and sensitivity classifications. It facilitates discovery, access control (Lake Formation/IAM), and auditing (CloudTrail). For AI, link datasets to models in the Model Registry and document in Model Cards for end-to-end traceability.",
"incorrect_explanations": {
"A": "A catalog does not imply public access; it aids governance.",
"C": "Speed depends on architecture and compute, not the catalog.",
"D": "Costs may improve indirectly, but it is not the central objective."
}
},
{
"id": "aif-c01-responsible_ai-205",
"certification_id": "AIF-C01",
"domain": "RESPONSIBLE_AI",
"difficulty": "easy",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "Which of the following is NOT typically a component of a data lifecycle management strategy?",
"option_a": "Data creation",
"option_b": "Data retention",
"option_c": "Data deletion",
"option_d": "Public data sharing",
"correct_answers": ["D"],
"explanation_detailed": "Lifecycle management covers creation, ingestion, classification, storage, use, retention, and secure deletion. Public sharing is not a standard component. On AWS, combine S3 Lifecycle, Lake Formation/IAM, KMS, Glue Catalog, and audits with CloudTrail/Config. Document processes for audits and compliance.",
"incorrect_explanations": {
"A": "Creation/ingestion starts the cycle and needs governance.",
"B": "Retention complies with laws and policies.",
"C": "Controlled deletion prevents improper retention and risks."
}
},
{
"id": "aif-c01-ml_development-206",
"certification_id": "AIF-C01",
"domain": "ML_DEVELOPMENT",
"difficulty": "medium",
"tier": "FREE",
"required_selection_count": 2,
"active": true,
"question_text": "A company has deployed a fraud detection model and wants to ensure continued accuracy and human review when necessary. Which AWS services or features should be used? (Select TWO.)",
"option_a": "Amazon SageMaker Model Monitor",
"option_b": "Amazon A2I (Amazon Augmented AI)",
"option_c": "Amazon Bedrock",
"option_d": "Amazon SageMaker Ground Truth",
"correct_answers": ["A", "B"],
"explanation_detailed": "SageMaker Model Monitor detects data and quality drifts in production endpoints, generating metrics and alerts when distributions change or when performance degrades. This allows triggering retraining or reviews. Amazon A2I inserts human review based on criteria (low confidence, periodic sampling, risk cases) to validate predictions, collect feedback, and create error sets for correction. Bedrock is useful for generative AI, but it is not the specific monitoring service for this scenario. Ground Truth is for data labeling; relevant for creating datasets, but it does not monitor production endpoints nor orchestrate real-time prediction reviews.",
"incorrect_explanations": {
"C": "Bedrock orchestrates FMs; it does not monitor the metrics of an existing fraud model.",
"D": "Ground Truth labels data; it does not monitor endpoint drift or trigger human review in production."
}
}
]
