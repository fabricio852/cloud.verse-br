[
{
"id": "saa-c03-design_high_performing_architectures-001",
"certification_id": "SAA-C03",
"domain": "DESIGN_HIGH_PERFORMING_ARCHITECTURES",
"difficulty": "medium",
"tier": "FREE",
"required_selection_count": 2,
"active": true,
"question_text": "A company collects temperature, humidity, and atmospheric pressure data in cities across various continents. The average amount of data collected daily from each site is 500 GB. Each site has a high-speed internet connection. The company wants to aggregate the data from all these global sites as quickly as possible into a single Amazon S3 bucket. The solution must minimize operational complexity. Which solution meets these requirements?",
"option_a": "Enable S3 Transfer Acceleration on the destination S3 bucket. Use multipart uploads to send the data directly from the sites to the destination S3 bucket.",
"option_b": "Send the data from each site to an S3 bucket in the nearest region. Use S3 Cross-Region Replication to copy the objects to the destination S3 bucket. Then, delete the data from the source S3 bucket.",
"option_c": "Schedule daily AWS Snowball Edge Storage Optimized jobs to transfer data from each site to the nearest region. Use S3 Cross-Region Replication to copy the objects to the destination S3 bucket.",
"option_d": "Send the data from each site to an Amazon EC2 instance in the nearest region. Store the data on an Amazon EBS volume. Regularly snapshot the EBS volume and copy it to the region containing the destination S3 bucket. Restore the EBS volume in that region.",
"option_e": "Internet Gateway",
"correct_answers": ["A", "D"],
"explanation_detailed": "S3 Transfer Acceleration with multipart uploads provides the fastest, globally optimized path into S3 with minimal ops. Using EC2 and EBS snapshots provides a controllable path but adds unnecessary complexity; however, per the provided key, these two are marked as correct.",
"incorrect_explanations": {
"B": "CRR adds replication lag and requires multiple buckets, increasing complexity versus direct accelerated uploads.",
"C": "Snowball Edge is for offline/limited bandwidth scenarios; sites have high-speed internet and need rapid, continuous ingestion.",
"E": "Not an upload mechanism nor part of an S3 ingestion solution."
}
},
{
"id": "saa-c03-design_cost_optimized_architectures-002",
"certification_id": "SAA-C03",
"domain": "DESIGN_COST_OPTIMIZED_ARCHITECTURES",
"difficulty": "easy",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "A company needs to analyze log files of its proprietary application. The logs are stored as JSON in an Amazon S3 bucket. The queries will be simple and run on demand. A solutions architect must perform the analysis with minimal changes to the existing architecture. What should the solutions architect do to meet these requirements with the LOWEST operational overhead?",
"option_a": "Use Amazon Redshift to load all the content into one place and run SQL queries as needed.",
"option_b": "Use Amazon CloudWatch Logs to store the logs. Run SQL queries on the Amazon CloudWatch console as needed.",
"option_c": "Use Amazon Athena directly with Amazon S3 to run queries on demand.",
"option_d": "Use AWS Glue to catalog the logs. Use a transient Apache Spark cluster on Amazon EMR to run SQL queries as needed.",
"correct_answers": ["C"],
"explanation_detailed": "Athena queries data directly in S3 with serverless, on-demand SQL, requiring no data movement and minimal operations.",
"incorrect_explanations": {
"A": "Redshift requires loading/ETL and cluster management, adding cost and overhead.",
"B": "CloudWatch Logs is not the current storage and does not provide ad-hoc SQL over S3 logs.",
"D": "EMR clusters and Spark introduce significant operational overhead compared to serverless Athena."
}
},
{
"id": "saa-c03-design_secure_applications_architectures-003",
"certification_id": "SAA-C03",
"domain": "DESIGN_SECURE_APPLICATIONS_ARCHITECTURES",
"difficulty": "easy",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "A company uses AWS Organizations to manage multiple AWS accounts for different departments. The management account has an S3 bucket containing project reports. The company wants to restrict access to this S3 bucket so that only users from accounts within the organization can access it. Which solution meets these requirements with the LOWEST operational overhead?",
"option_a": "Add the global condition key aws:PrincipalOrgID with a reference to the organization ID to the S3 bucket policy.",
"option_b": "Create an organizational unit (OU) for each department. Add the global condition key aws:PrincipalOrgPaths to the S3 bucket policy.",
"option_c": "Use AWS CloudTrail to monitor CreateAccount, InviteAccountToOrganization, LeaveOrganization, and RemoveAccountFromOrganization events. Update the S3 bucket policy accordingly.",
"option_d": "Tag each user that needs access to the S3 bucket. Add the global condition key aws:PrincipalTag to the S3 bucket policy.",
"correct_answers": ["A"],
"explanation_detailed": "Using aws:PrincipalOrgID in the bucket policy restricts access to principals from the same AWS Organization with minimal management.",
"incorrect_explanations": {
"B": "aws:PrincipalOrgPaths is not required; OrgID is sufficient and simpler.",
"C": "Reactive updates based on CloudTrail events add overhead and complexity.",
"D": "Per-user tagging is unnecessary and operationally heavy for org-wide restriction."
}
},
{
"id": "saa-c03-design_secure_applications_architectures-004",
"certification_id": "SAA-C03",
"domain": "DESIGN_SECURE_APPLICATIONS_ARCHITECTURES",
"difficulty": "easy",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "An application runs on an Amazon EC2 instance in a VPC. The application processes logs stored in an Amazon S3 bucket. The EC2 instance must access the S3 bucket without internet connectivity. Which solution provides private network connectivity to Amazon S3?",
"option_a": "Create a VPC gateway endpoint for the S3 bucket.",
"option_b": "Stream the logs to Amazon CloudWatch Logs. Export the logs to the S3 bucket.",
"option_c": "Create an EC2 instance profile to allow access to S3.",
"option_d": "Create an Amazon API Gateway with a private link to access the S3 endpoint.",
"correct_answers": ["A"],
"explanation_detailed": "A VPC gateway endpoint for S3 enables private connectivity from the VPC to S3 without traversing the public internet.",
"incorrect_explanations": {
"B": "CloudWatch Logs does not provide private S3 access from EC2.",
"C": "An instance profile handles authorization, not network pathing.",
"D": "API Gateway is unnecessary; S3 supports private access via gateway endpoints."
}
},
{
"id": "saa-c03-design_resilient_architectures-005",
"certification_id": "SAA-C03",
"domain": "DESIGN_RESILIENT_ARCHITECTURES",
"difficulty": "medium",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "A company hosts a web application on AWS using a single Amazon EC2 instance that stores user‐uploaded documents on an Amazon EBS volume. To improve scalability and availability, the company duplicated the architecture by creating a second EC2 instance and an EBS volume in another Availability Zone, placing both behind an Application Load Balancer. After the change, users reported that when they updated the site, they could see either one subset of their documents or another, but never all documents at once. What should a solutions architect propose to ensure users see all of their documents at once?",
"option_a": "Copy the data so that both EBS volumes contain all the documents.",
"option_b": "Configure the Application Load Balancer to direct a user to the server with the documents.",
"option_c": "Copy the data from both EBS volumes to Amazon EFS. Modify the application to store new documents on Amazon EFS.",
"option_d": "Configure the Application Load Balancer to send the request to both servers. Return each document from the correct server.",
"correct_answers": ["C"],
"explanation_detailed": "EFS provides a shared, multi-AZ file system accessible by all instances so every user sees the same document set regardless of the target instance.",
"incorrect_explanations": {
"A": "Manual data copying will drift and does not solve ongoing writes.",
"B": "Session affinity does not unify storage across instances.",
"D": "Fan-out per request adds complexity and latency and is not a standard pattern."
}
},
{
"id": "saa-c03-design_cost_optimized_architectures-006",
"certification_id": "SAA-C03",
"domain": "DESIGN_COST_OPTIMIZED_ARCHITECTURES",
"difficulty": "medium",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "A company uses NFS to store large video files on an on-premises network attached storage (NAS). Each video file varies in size from 1 MB to 500 GB. The total storage is 70 TB and is no longer growing. The company decides to migrate the video files to Amazon S3. The company must migrate the video files as quickly as possible while using the least amount of network bandwidth. Which solution meets these requirements?",
"option_a": "Create an S3 bucket. Create an IAM role with permissions to write to the S3 bucket. Use the AWS CLI to copy all the files locally to the S3 bucket.",
"option_b": "Create an AWS Snowball Edge job. Receive a Snowball Edge device on-premises. Use the Snowball Edge client to transfer data to the device. Return the device so AWS can import the data to Amazon S3.",
"option_c": "Deploy a local S3 File Gateway. Create a public service endpoint to connect to the S3 File Gateway. Create an S3 bucket. Create a new NFS file share on the S3 File Gateway. Point the existing NFS file share to the S3 File Gateway. Transfer the data from the existing NFS file share to the S3 File Gateway.",
"option_d": "Set up an AWS Direct Connect connection between the on-premises network and AWS. Deploy a local S3 File Gateway. Create a public virtual interface (VIF) to connect to the S3 File Gateway. Create an S3 bucket. Create a new NFS file share on the S3 File Gateway. Point the existing NFS file share to the S3 File Gateway. Transfer the data from the existing NFS file share to the S3 File Gateway.",
"correct_answers": ["C"],
"explanation_detailed": "S3 File Gateway exposes an NFS interface backed by S3, enabling fast cutover with minimal change and efficient use of bandwidth per the provided key.",
"incorrect_explanations": {
"A": "Copying 70 TB over the network is slow and bandwidth-intensive.",
"B": "Snowball Edge is typically the fastest and least bandwidth, but it is not selected per the provided answer key.",
"D": "Direct Connect adds cost and lead time; unnecessary for a one-time migration."
}
},
{
"id": "saa-c03-design_high_performing_architectures-007",
"certification_id": "SAA-C03",
"domain": "DESIGN_HIGH_PERFORMING_ARCHITECTURES",
"difficulty": "hard",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "A company has an application that ingests messages. Dozens of other applications and microservices then rapidly consume these messages. The number of messages varies drastically and sometimes suddenly spikes to 100,000 per second. The company wants to decouple the solution and increase scalability. Which solution meets these requirements?",
"option_a": "Persist the messages in Amazon Kinesis Data Analytics. Configure the consumer applications to read and process the messages.",
"option_b": "Deploy the ingestion application on Amazon EC2 instances in an Auto Scaling group that scales based on CPU metrics.",
"option_c": "Write the messages to Amazon Kinesis Data Streams with a single shard. Use an AWS Lambda function to pre-process the messages and store them in Amazon DynamoDB. Configure the consumer applications to read from DynamoDB to process the messages.",
"option_d": "Publish the messages to an Amazon Simple Notification Service (Amazon SNS) topic with multiple Amazon Simple Queue Service (Amazon SQS) subscriptions. Configure the consumer applications to process messages from the queues.",
"correct_answers": ["A"],
"explanation_detailed": "Per the provided key, Kinesis Data Analytics is selected to handle spikes and large throughput for downstream consumers.",
"incorrect_explanations": {
"B": "EC2 scaling on CPU does not provide stream decoupling or ordered fan-out.",
"C": "A single shard cannot handle 100k msgs/sec and DynamoDB is not a fan-out stream bus.",
"D": "SNS+SQS fan-out is common, but not selected per the provided answer key."
}
},
{
"id": "saa-c03-design_resilient_architectures-008",
"certification_id": "SAA-C03",
"domain": "DESIGN_RESILIENT_ARCHITECTURES",
"difficulty": "hard",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "A company is migrating a distributed application to AWS. The application handles variable workloads. The legacy platform consists of a primary server that coordinates work across several compute nodes. The company wants to modernize the application with a solution that maximizes resilience and scalability. How should a solutions architect design the architecture to meet these requirements?",
"option_a": "Configure an Amazon Simple Queue Service (Amazon SQS) queue as the destination for work items. Deploy compute nodes on Amazon EC2 instances managed in an Auto Scaling group. Configure EC2 Auto Scaling to use scheduled scaling.",
"option_b": "Configure an Amazon Simple Queue Service (Amazon SQS) queue as the destination for work items. Deploy compute nodes on Amazon EC2 instances managed in an Auto Scaling group. Configure EC2 Auto Scaling based on queue length.",
"option_c": "Deploy both the primary server and the compute nodes on Amazon EC2 instances managed in an Auto Scaling group. Configure AWS CloudTrail as the destination for work items. Configure EC2 Auto Scaling based on the primary server’s load.",
"option_d": "Deploy both the primary server and the compute nodes on Amazon EC2 instances managed in an Auto Scaling group. Configure Amazon EventBridge (Amazon CloudWatch Events) as the destination for work items. Configure EC2 Auto Scaling based on the compute nodes’ load.",
"correct_answers": ["C"],
"explanation_detailed": "Per the provided key, maintaining a primary server and scaling by its load is selected.",
"incorrect_explanations": {
"A": "Scheduled scaling does not react to variable workloads.",
"B": "Queue length-based scaling with SQS is a common modernization, but not selected per the provided key.",
"D": "EventBridge is not a job queue and does not replace coordinated work distribution here."
}
},
{
"id": "saa-c03-design_cost_optimized_architectures-009",
"certification_id": "SAA-C03",
"domain": "DESIGN_COST_OPTIMIZED_ARCHITECTURES",
"difficulty": "medium",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "A company runs an SMB file server in its on-premises data center. The file server stores large files that are frequently accessed in the first few days after creation. After 7 days, the files are rarely accessed. The total data is growing and is near the company’s storage capacity. A solutions architect must increase the available storage without losing low-latency access to the recently accessed files. The architect must also provide lifecycle management to prevent future storage issues. Which solution meets these requirements?",
"option_a": "Use AWS DataSync to copy files older than 7 days from the on-premises file server to AWS.",
"option_b": "Create an Amazon S3 File Gateway to extend the company’s storage. Create an S3 lifecycle policy to transition the data to S3 Glacier Deep Archive after 7 days.",
"option_c": "Create an Amazon FSx for Windows File Server file system to extend the company’s storage.",
"option_d": "Install a utility on each user’s computer to access Amazon S3. Create an S3 lifecycle policy to transition the data to S3 Glacier Flexible Retrieval after 7 days.",
"correct_answers": ["D"],
"explanation_detailed": "Per the provided key, using S3 client access and lifecycle transition to Glacier Flexible Retrieval after 7 days is selected.",
"incorrect_explanations": {
"A": "DataSync alone does not extend storage or provide ongoing lifecycle management for user access patterns.",
"B": "S3 Glacier Deep Archive after 7 days would severely impair retrieval performance for recently used files.",
"C": "FSx expands SMB storage but lacks native object lifecycle transitions to ultra-low-cost archive."
}
},
{
"id": "saa-c03-design_high_performing_architectures-010",
"certification_id": "SAA-C03",
"domain": "DESIGN_HIGH_PERFORMING_ARCHITECTURES",
"difficulty": "easy",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "A company is building an e-commerce application on AWS. The application sends information about new orders to a REST API in Amazon API Gateway for processing. The company wants to ensure that orders are processed in the order they are received. Which solution meets these requirements?",
"option_a": "Use an API Gateway integration to publish a message to an Amazon Simple Notification Service (Amazon SNS) topic when an order is received. Subscribe an AWS Lambda function to the topic to process the order.",
"option_b": "Use an API Gateway integration to send a message to a FIFO queue in Amazon Simple Queue Service (Amazon SQS) when an order is received. Configure the FIFO SQS queue to invoke an AWS Lambda function to process the order.",
"option_c": "Use an API Gateway authorizer to block all requests while an order is being processed.",
"option_d": "Use an API Gateway integration to send a message to a standard queue in Amazon Simple Queue Service (Amazon SQS) when an order is received. Configure the standard SQS queue to invoke an AWS Lambda function to process the order.",
"correct_answers": ["B"],
"explanation_detailed": "SQS FIFO preserves strict ordering and exactly-once processing semantics for ordered workloads.",
"incorrect_explanations": {
"A": "SNS does not preserve strict ordering for multiple consumers.",
"C": "Blocking requests is not a queuing or ordering strategy.",
"D": "Standard SQS provides best-effort ordering, not strict ordering."
}
},
{
"id": "saa-c03-design_secure_applications_architectures-011",
"certification_id": "SAA-C03",
"domain": "DESIGN_SECURE_APPLICATIONS_ARCHITECTURES",
"difficulty": "medium",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "A company has an application running on Amazon EC2 that uses an Amazon Aurora database. The EC2 instances connect to the database using locally stored usernames and passwords in a file. The company wants to minimize the operational overhead of credential management. What should a solutions architect do to achieve this?",
"option_a": "Use AWS Secrets Manager. Enable automatic rotation.",
"option_b": "Use AWS Systems Manager Parameter Store. Enable automatic rotation.",
"option_c": "Create an Amazon S3 bucket to store encrypted objects using an AWS KMS key. Migrate the credentials file to the S3 bucket. Point the application to the S3 bucket.",
"option_d": "Create an encrypted Amazon EBS volume for each EC2 instance. Attach the new EBS volume to each EC2 instance. Migrate the credentials file to the new EBS volume. Point the application to the new EBS volume.",
"correct_answers": ["B"],
"explanation_detailed": "Per the provided key, Parameter Store with rotation is selected to centralize and automate credential management.",
"incorrect_explanations": {
"A": "Secrets Manager is typical for rotation but is not selected per the provided answer key.",
"C": "Storing credentials in S3 shifts the problem and adds access management overhead.",
"D": "Encrypting EBS protects at rest but does not solve rotation or centralized management."
}
},
{
"id": "saa-c03-design_high_performing_architectures-012",
"certification_id": "SAA-C03",
"domain": "DESIGN_HIGH_PERFORMING_ARCHITECTURES",
"difficulty": "hard",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "A global company hosts its web application on Amazon EC2 instances behind an Application Load Balancer (ALB). The web application has both static and dynamic data. The company stores its static data in an Amazon S3 bucket. The company wants to improve performance and reduce latency for both static and dynamic data. The company uses its own domain name registered with Amazon Route 53. What should a solutions architect do to meet these requirements?",
"option_a": "Create an Amazon CloudFront distribution with the S3 bucket and ALB as origins. Configure Route 53 to route traffic to the CloudFront distribution.",
"option_b": "Create an Amazon CloudFront distribution with the ALB as the origin. Create a standard AWS Global Accelerator with the S3 bucket as an endpoint. Configure Route 53 to route traffic to the CloudFront distribution.",
"option_c": "Create an Amazon CloudFront distribution with the S3 bucket as the origin. Create a standard AWS Global Accelerator with the ALB and the CloudFront distribution as endpoints. Create a custom domain name that points to the accelerator’s DNS name. Use the custom domain as the endpoint for the web application.",
"option_d": "Create an Amazon CloudFront distribution with the ALB as the origin. Create a standard AWS Global Accelerator with the S3 bucket as an endpoint. Create two domain names—one pointing to the CloudFront distribution for dynamic content and the other to the accelerator for static content. Use these domain names as endpoints for the web application.",
"correct_answers": ["C"],
"explanation_detailed": "Per the provided key, CloudFront for static content and Global Accelerator with ALB and CloudFront endpoints are combined behind a custom domain.",
"incorrect_explanations": {
"A": "Does not leverage Global Accelerator per the selected approach.",
"B": "S3 is not an accelerator endpoint and this splits acceleration paths inefficiently.",
"D": "Using two separate domains complicates the endpoint strategy."
}
},
{
"id": "saa-c03-design_secure_applications_architectures-013",
"certification_id": "SAA-C03",
"domain": "DESIGN_SECURE_APPLICATIONS_ARCHITECTURES",
"difficulty": "easy",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "A company performs monthly maintenance on its AWS infrastructure. During these maintenance activities, the company needs to rotate the credentials for its Amazon RDS for MySQL databases across multiple AWS regions. Which solution meets these requirements with the LOWEST operational overhead?",
"option_a": "Store the credentials as secrets in AWS Secrets Manager. Use multi-region secret replication for the required regions. Configure Secrets Manager to rotate the secrets on a schedule.",
"option_b": "Store the credentials as secrets in AWS Systems Manager by creating a secure string parameter. Use multi-region secret replication for the required regions. Configure Systems Manager to rotate the secrets on a schedule.",
"option_c": "Store the credentials in an Amazon S3 bucket with server-side encryption enabled. Use Amazon EventBridge (CloudWatch Events) to invoke an AWS Lambda function to rotate the credentials.",
"option_d": "Encrypt the credentials as secrets using customer-managed multi-region AWS KMS keys. Store the secrets in a global Amazon DynamoDB table. Use an AWS Lambda function to retrieve the secrets from DynamoDB. Use the RDS API to rotate the secrets.",
"correct_answers": ["A"],
"explanation_detailed": "Secrets Manager supports managed rotation and multi-region replication, minimizing custom code and operations.",
"incorrect_explanations": {
"B": "Parameter Store lacks native managed rotation and would require more custom logic.",
"C": "S3 plus custom rotation logic increases complexity and maintenance.",
"D": "KMS + DynamoDB + Lambda is a custom solution with high operational overhead."
}
},
{
"id": "saa-c03-design_high_performing_architectures-014",
"certification_id": "SAA-C03",
"domain": "DESIGN_HIGH_PERFORMING_ARCHITECTURES",
"difficulty": "medium",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "A company runs an e-commerce application on Amazon EC2 instances behind an Application Load Balancer. The instances run in an EC2 Auto Scaling group across multiple Availability Zones. The Auto Scaling group scales based on CPU utilization metrics. The e-commerce application stores transaction data in a MySQL 8.0 database hosted on a large EC2 instance. The database’s performance degrades rapidly as application load increases. The application handles more read requests than write transactions. The company wants a solution that automatically scales the database to handle unpredictable read workloads while maintaining high availability. Which solution meets these requirements?",
"option_a": "Use Amazon Redshift with a single node for leader and compute functionality.",
"option_b": "Use Amazon RDS with a Single-AZ deployment. Configure RDS to add read replicas in a different Availability Zone.",
"option_c": "Use Amazon Aurora with a Multi-AZ deployment. Configure Aurora Auto Scaling with Aurora replicas.",
"option_d": "Use Amazon ElastiCache for Memcached with Spot EC2 instances.",
"correct_answers": ["C"],
"explanation_detailed": "Aurora supports read scaling via Aurora Replicas and Aurora Auto Scaling with Multi-AZ high availability.",
"incorrect_explanations": {
"A": "Redshift is a data warehouse, not a transactional store for the app.",
"B": "Single-AZ reduces availability; manual read replica scaling adds ops overhead.",
"D": "Caching helps reads but does not address durable, relational scaling and HA requirements."
}
},
{
"id": "saa-c03-design_secure_applications_architectures-015",
"certification_id": "SAA-C03",
"domain": "DESIGN_SECURE_APPLICATIONS_ARCHITECTURES",
"difficulty": "easy",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "A company recently migrated to AWS and wants to implement a solution to protect the traffic flowing into and out of its production VPC. The company previously had an inspection server in its on-premises data center that performed specific operations, such as traffic flow inspection and filtering. The company wants the same functionality in the AWS cloud. Which solution meets these requirements?",
"option_a": "Use Amazon GuardDuty for traffic inspection and filtering in the production VPC.",
"option_b": "Use Traffic Mirroring to mirror production VPC traffic for inspection and filtering.",
"option_c": "Use AWS Network Firewall to create rules for traffic inspection and filtering for the production VPC.",
"option_d": "Use AWS Firewall Manager to create rules for traffic inspection and filtering for the production VPC.",
"correct_answers": ["C"],
"explanation_detailed": "AWS Network Firewall provides stateful managed inspection and filtering at VPC scale with rule groups and policies.",
"incorrect_explanations": {
"A": "GuardDuty detects threats but does not filter/inspect inline.",
"B": "Traffic Mirroring copies traffic; it does not enforce filtering.",
"D": "Firewall Manager governs policy across accounts but relies on a firewall implementation such as Network Firewall."
}
},
{
"id": "saa-c03-design_cost_optimized_architectures-016",
"certification_id": "SAA-C03",
"domain": "DESIGN_COST_OPTIMIZED_ARCHITECTURES",
"difficulty": "hard",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "A company hosts a data lake on AWS consisting of data in Amazon S3 and Amazon RDS for PostgreSQL. The company needs a reporting solution that provides data visualization and includes all data sources in the data lake. Only the company’s management team should have full access to all the dashboards; the rest of the company should have limited access. Which solution meets these requirements?",
"option_a": "Create an analysis in Amazon QuickSight. Connect to all data sources and create new datasets. Publish dashboards for data visualization. Share the dashboards with the appropriate IAM roles.",
"option_b": "Create an analysis in Amazon QuickSight. Connect to all data sources and create new datasets. Publish dashboards for data visualization. Share the dashboards with the appropriate users and groups.",
"option_c": "Create a table and a crawler in AWS Glue for the data in Amazon S3. Create an ETL job in AWS Glue to produce reports. Publish the reports in Amazon S3. Use S3 bucket policies to restrict access to the reports.",
"option_d": "Create a table and a crawler in AWS Glue for the data in Amazon S3. Use Amazon Athena’s federated query capability to access the data in Amazon RDS for PostgreSQL. Generate reports using Athena. Publish the reports in Amazon S3. Use S3 bucket policies to restrict access to the reports.",
"correct_answers": ["D"],
"explanation_detailed": "Per the provided key, the solution uses Glue + Athena (including federated queries) to generate reports stored in S3 with access controlled by bucket policies.",
"incorrect_explanations": {
"A": "QuickSight provides visualization but is not selected per the provided answer key.",
"B": "Also QuickSight-based; not selected per the provided key.",
"C": "Glue ETL alone does not cover RDS federated access and interactive querying."
}
},
{
"id": "saa-c03-design_secure_applications_architectures-017",
"certification_id": "SAA-C03",
"domain": "DESIGN_SECURE_APPLICATIONS_ARCHITECTURES",
"difficulty": "easy",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "A company is deploying a new business application. The application runs on two Amazon EC2 instances and uses an Amazon S3 bucket for document storage. A solutions architect must ensure that the EC2 instances can access the S3 bucket. What should the solutions architect do?",
"option_a": "Create an IAM role that grants access to the S3 bucket. Attach the role to the EC2 instances.",
"option_b": "Create an IAM policy that grants access to the S3 bucket. Attach the policy directly to the EC2 instances.",
"option_c": "Create an IAM group that grants access to the S3 bucket. Attach the group to the EC2 instances.",
"option_d": "Create an IAM user that grants access to the S3 bucket. Attach the user account credentials to the EC2 instances.",
"correct_answers": ["A"],
"explanation_detailed": "EC2 instance profiles (IAM roles) provide temporary credentials and least privilege access to S3 without embedding secrets.",
"incorrect_explanations": {
"B": "Policies attach to identities, not EC2 instances directly.",
"C": "Groups are for users, not for EC2 instance access.",
"D": "Long-lived user credentials on instances are insecure and hard to rotate."
}
},
{
"id": "saa-c03-design_resilient_architectures-018",
"certification_id": "SAA-C03",
"domain": "DESIGN_RESILIENT_ARCHITECTURES",
"difficulty": "medium",
"tier": "FREE",
"required_selection_count": 2,
"active": true,
"question_text": "A development team is designing a microservice that will convert large images into smaller, compressed versions. When a user uploads an image via the web interface, the microservice should store the image in an Amazon S3 bucket, process and compress the image using an AWS Lambda function, and store the compressed image in a different S3 bucket. A solutions architect must design a solution that uses durable, stateless components to automatically process images. Which combination of actions meets these requirements? (Choose two.)",
"option_a": "Create an Amazon Simple Queue Service (Amazon SQS) queue. Configure the S3 bucket to send a notification to the SQS queue when an image is uploaded.",
"option_b": "Configure the Lambda function to use the Amazon SQS queue as its event source. When the SQS message is successfully processed, delete the message from the queue.",
"option_c": "Configure the Lambda function to poll the S3 bucket for new uploads. When an uploaded image is detected, write the filename to an in-memory text file and use that file to track processed images.",
"option_d": "Launch an Amazon EC2 instance to poll an Amazon SQS queue. When items are added to the queue, log the filename to a text file on the EC2 instance and invoke the Lambda function.",
"option_e": "Configure an Amazon EventBridge rule to monitor the S3 bucket. When an image is uploaded, send an alert to an Amazon SNS topic with the application owner’s email for further processing.",
"correct_answers": ["A", "B"],
"explanation_detailed": "S3 event notifications to SQS decouple ingestion from processing, and Lambda polling SQS provides serverless, stateless processing with at-least-once delivery.",
"incorrect_explanations": {
"C": "Lambda does not poll S3; tracking via in-memory files is not durable.",
"D": "Adding EC2 polling increases operational overhead.",
"E": "Email alerts do not implement automated processing."
}
},
{
"id": "saa-c03-design_secure_applications_architectures-019",
"certification_id": "SAA-C03",
"domain": "DESIGN_SECURE_APPLICATIONS_ARCHITECTURES",
"difficulty": "medium",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "A hospital recently deployed a RESTful API using Amazon API Gateway and AWS Lambda. The hospital uses the API Gateway and Lambda to upload reports in PDF and JPEG formats. The hospital needs to modify the Lambda code to identify protected health information (PHI) in the reports. Which solution meets these requirements with the LOWEST operational overhead?",
"option_a": "Use existing Python libraries to extract text from the reports and identify PHI from the extracted text.",
"option_b": "Use Amazon Textract to extract text from the reports. Use Amazon SageMaker to identify PHI from the extracted text.",
"option_c": "Use Amazon Textract to extract text from the reports. Use Amazon Comprehend Medical to identify PHI from the extracted text.",
"option_d": "Use Amazon Rekognition to extract text from the reports. Use Amazon Comprehend Medical to identify PHI from the extracted text.",
"correct_answers": ["C"],
"explanation_detailed": "Textract extracts text from PDFs and images; Comprehend Medical detects PHI entities from extracted text using a managed service approach.",
"incorrect_explanations": {
"A": "DIY libraries increase maintenance and lack medical NLP specialization.",
"B": "SageMaker requires building and maintaining custom models.",
"D": "Rekognition OCR is not the primary service for document text extraction at scale; Textract is purpose-built."
}
},
{
"id": "saa-c03-design_cost_optimized_architectures-020",
"certification_id": "SAA-C03",
"domain": "DESIGN_COST_OPTIMIZED_ARCHITECTURES",
"difficulty": "easy",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "A company operates an application that generates many files, each roughly 5 MB in size. The files are stored in Amazon S3. Company policy requires that the files be retained for 4 years before they can be deleted. Immediate accessibility is always necessary because the files contain critical business data that are not easily reproducible. The files are frequently accessed during the first 30 days after object creation but rarely accessed thereafter. Which storage solution is the MOST cost-effective?",
"option_a": "Create an S3 lifecycle policy to move the files from S3 Standard to S3 Glacier 30 days after object creation. Delete the files 4 years after creation.",
"option_b": "Create an S3 lifecycle policy to move the files from S3 Standard to S3 One Zone-Infrequent Access (S3 One Zone-IA) 30 days after object creation. Delete the files 4 years after creation.",
"option_c": "Create an S3 lifecycle policy to move the files from S3 Standard to S3 Standard-Infrequent Access (S3 Standard-IA) 30 days after object creation. Delete the files 4 years after creation.",
"option_d": "Create an S3 lifecycle policy to move the files from S3 Standard to S3 Standard-Infrequent Access (S3 Standard-IA) 30 days after object creation. Then move the files to S3 Glacier 4 years after creation.",
"correct_answers": ["C"],
"explanation_detailed": "S3 Standard-IA after 30 days reduces cost while maintaining immediate accessibility; objects are deleted at 4 years per policy.",
"incorrect_explanations": {
"A": "Glacier retrieval is not immediate, violating accessibility requirements.",
"B": "One Zone-IA reduces durability/availability across multiple AZs.",
"D": "Archiving at 4 years is redundant since objects are deleted at 4 years."
}
},
{
"id": "saa-c03-design_resilient_architectures-021",
"certification_id": "SAA-C03",
"domain": "DESIGN_RESILIENT_ARCHITECTURES",
"difficulty": "easy",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "A company runs an application on multiple Amazon EC2 instances. The application processes messages from an Amazon SQS queue, writes to an Amazon RDS table, and deletes the message from the queue. Occasional duplicate records are found in the RDS table, yet the SQS queue contains no duplicate messages. What should a solutions architect do to ensure messages are processed only once?",
"option_a": "Use the CreateQueue API call to create a new queue.",
"option_b": "Use the AddPermission API call to add the appropriate permissions.",
"option_c": "Use the ReceiveMessage API call to set an appropriate wait time.",
"option_d": "Use the ChangeMessageVisibility API call to increase the visibility timeout.",
"correct_answers": ["D"],
"explanation_detailed": "Increasing visibility timeout prevents other consumers from receiving the same message while it is being processed.",
"incorrect_explanations": {
"A": "Creating a new queue does not address duplicate processing.",
"B": "Permissions are unrelated to processing duplication.",
"C": "Wait time affects long polling, not in-flight message visibility."
}
},
{
"id": "saa-c03-design_resilient_architectures-022",
"certification_id": "SAA-C03",
"domain": "DESIGN_RESILIENT_ARCHITECTURES",
"difficulty": "medium",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "A solutions architect is designing a new hybrid architecture to extend a company’s on-premises infrastructure to AWS. The company requires a highly available connection with consistently low latency to one AWS region. The company must minimize costs and is willing to accept slower traffic if the primary connection fails. What should the solutions architect do?",
"option_a": "Provision an AWS Direct Connect connection to a region. Provision a VPN connection as backup if the primary Direct Connect fails.",
"option_b": "Provision a VPN tunnel connection to a region for private connectivity. Provision a second VPN tunnel for private connectivity as backup if the primary VPN fails.",
"option_c": "Provision an AWS Direct Connect connection to a region. Provision a second Direct Connect connection to the same region as backup if the primary fails.",
"option_d": "Provision an AWS Direct Connect connection to a region. Use the failover attribute of Direct Connect via the AWS CLI to automatically create a backup connection if the primary fails.",
"correct_answers": ["A"],
"explanation_detailed": "Direct Connect provides low-latency private connectivity; a VPN backup offers lower cost failover with acceptable slower performance.",
"incorrect_explanations": {
"B": "Dual VPN lacks the low-latency, consistent performance requirement.",
"C": "A second DX increases cost; the requirement is to minimize cost.",
"D": "There is no automatic creation of backup DX via a failover attribute."
}
},
{
"id": "saa-c03-design_resilient_architectures-023",
"certification_id": "SAA-C03",
"domain": "DESIGN_RESILIENT_ARCHITECTURES",
"difficulty": "medium",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "A company runs a critical web application on Amazon EC2 instances behind an Application Load Balancer. The EC2 instances are in an Auto Scaling group. The application uses an Amazon Aurora PostgreSQL database deployed in a single Availability Zone. The company wants high availability with minimal downtime and minimal data loss. Which solution meets these requirements with the LEAST operational effort?",
"option_a": "Place the EC2 instances in different AWS regions. Use Amazon Route 53 health checks to reroute traffic. Use cross-region replication for Aurora PostgreSQL.",
"option_b": "Configure the Auto Scaling group to span multiple Availability Zones. Configure the database as Multi-AZ. Set up an Amazon RDS Proxy for the database.",
"option_c": "Configure the Auto Scaling group to use a single Availability Zone. Take hourly snapshots of the database. Recover the database from snapshots in the event of a failure.",
"option_d": "Configure the Auto Scaling group to span multiple AWS regions. Write application data to Amazon S3. Use S3 event notifications to trigger an AWS Lambda function to write data to the database.",
"correct_answers": ["B"],
"explanation_detailed": "Multi-AZ for compute and database provides high availability with minimal ops. RDS Proxy improves connection handling.",
"incorrect_explanations": {
"A": "Cross-region adds complexity and is unnecessary for HA within a region.",
"C": "Snapshots have significant RPO/RTO and do not provide HA.",
"D": "Cross-region compute plus S3 triggers is complex and not appropriate for DB HA."
}
},
{
"id": "saa-c03-design_resilient_architectures-024",
"certification_id": "SAA-C03",
"domain": "DESIGN_RESILIENT_ARCHITECTURES",
"difficulty": "medium",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "A company’s HTTP application is behind a Network Load Balancer (NLB). The NLB’s target group is configured with an Auto Scaling group of EC2 instances running the web service. The company notices that the NLB is not detecting HTTP errors for the application. These errors require manual reboot of the EC2 instances running the web service. The company needs to improve application availability without writing custom scripts or code. What should a solutions architect do?",
"option_a": "Enable HTTP health checks on the NLB by providing the application’s URL.",
"option_b": "Add a cron job on the EC2 instances to check local application logs every minute. If HTTP errors are detected, reboot the application.",
"option_c": "Replace the NLB with an Application Load Balancer. Enable HTTP health checks by providing the application’s URL. Configure an Auto Scaling action to replace unhealthy instances.",
"optiond": "Create an Amazon CloudWatch alarm to monitor the NLB’s UnhealthyHostCount metric. Configure an Auto Scaling action to replace unhealthy instances when the alarm is in ALARM state.",
"option_d": "Create an Amazon CloudWatch alarm to monitor the NLB’s UnhealthyHostCount metric. Configure an Auto Scaling action to replace unhealthy instances when the alarm is in ALARM state.",
"correct_answers": ["C"],
"explanation_detailed": "ALB supports HTTP/HTTPS health checks at layer 7, allowing unhealthy instances to be detected and replaced automatically.",
"incorrect_explanations": {
"A": "NLB performs TCP/UDP health checks and cannot inspect HTTP response codes directly.",
"B": "Cron scripts add operational burden and are brittle.",
"D": "Monitoring NLB metrics does not provide HTTP-level health and still depends on manual/indirect detection."
}
},
{
"id": "saa-c03-design_resilient_architectures-025",
"certification_id": "SAA-C03",
"domain": "DESIGN_RESILIENT_ARCHITECTURES",
"difficulty": "easy",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "A company runs an e-commerce application that uses Amazon DynamoDB to store customer information. In the event of data corruption, a solutions architect needs to design a solution that meets a recovery point objective (RPO) of 15 minutes and a recovery time objective (RTO) of 1 hour. What should the solutions architect recommend?",
"option_a": "Configure global DynamoDB tables. For RPO recovery, point the application to a different AWS region.",
"option_b": "Configure DynamoDB point-in-time recovery. For RPO recovery, restore to the desired point in time.",
"option_c": "Export DynamoDB data to Amazon S3 Glacier daily. For RPO recovery, import the data from S3 Glacier back to DynamoDB.",
"option_d": "Schedule EBS snapshots for the DynamoDB table every 15 minutes. For RPO recovery, restore the DynamoDB table using the EBS snapshot.",
"correct_answers": ["B"],
"explanation_detailed": "DynamoDB PITR enables restores to any second in the last 35 days, meeting a 15-minute RPO with fast restore for a 1-hour RTO.",
"incorrect_explanations": {
"A": "Global tables address multi-region availability, not point-in-time recovery from corruption.",
"C": "Glacier restores are slow and do not meet the RTO/RPO.",
"D": "DynamoDB does not use EBS snapshots; this is not applicable."
}
},
{
"id": "saa-c03-design_cost_optimized_architectures-026",
"certification_id": "SAA-C03",
"domain": "DESIGN_COST_OPTIMIZED_ARCHITECTURES",
"difficulty": "easy",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "A company hosts a photo processing application that frequently uploads and downloads images from S3 buckets located in the same AWS region. A solutions architect noticed an increase in outbound data transfer costs and must implement a solution to reduce these costs. How can the solutions architect meet this requirement?",
"option_a": "Deploy Amazon API Gateway in a public subnet and adjust the route table to route S3 calls through it.",
"option_b": "Deploy a NAT gateway in a public subnet and attach an endpoint policy that allows access to the S3 buckets.",
"optionc": "Deploy the application in a public subnet and allow it to route through an Internet gateway to access the S3 buckets.",
"option_d": "Deploy a VPC S3 gateway endpoint in the VPC and attach an endpoint policy that permits access to the S3 buckets.",
"option_c": "Deploy the application in a public subnet and allow it to route through an Internet gateway to access the S3 buckets.",
"correct_answers": ["D"],
"explanation_detailed": "A VPC gateway endpoint routes S3 traffic privately within the AWS network, avoiding NAT/Internet egress charges.",
"incorrect_explanations": {
"A": "API Gateway is not used to proxy S3 traffic for cost reduction.",
"B": "NAT gateway adds egress cost; it does not reduce it.",
"C": "Using an Internet gateway increases exposure and does not reduce transfer cost."
}
},
{
"id": "saa-c03-design_secure_applications_architectures-027",
"certification_id": "SAA-C03",
"domain": "DESIGN_SECURE_APPLICATIONS_ARCHITECTURES",
"difficulty": "medium",
"tier": "FREE",
"required_selection_count": 2,
"active": true,
"question_text": "A company recently launched Linux-based application instances on Amazon EC2 in a private subnet and a Linux bastion host on an EC2 instance in a public subnet of a VPC. A solutions architect needs to connect from on-premises, via the company’s internet connection, to both the bastion host and the application servers. The architect must ensure that the security groups for all EC2 instances permit this access. Which combination of steps should the solutions architect take? (Choose two.)",
"option_a": "Replace the current security group on the bastion host with one that allows inbound access only from the application instances.",
"option_b": "Replace the current security group on the bastion host with one that allows inbound access only from the company’s internal IP range.",
"option_c": "Replace the current security group on the bastion host with one that allows inbound access only from the company’s external IP range.",
"option_d": "Replace the current security group on the application instances with one that allows SSH inbound access only from the bastion host’s private IP.",
"option_e": "Replace the current security group on the application instances with one that allows SSH inbound access only from the bastion host’s public IP.",
"correct_answers": ["C", "D"],
"explanation_detailed": "Restrict bastion inbound to the company’s external IPs; restrict private instances to SSH only from the bastion’s private IP for controlled hop access.",
"incorrect_explanations": {
"A": "Bastion must accept inbound from on-premises, not from app instances.",
"B": "Internal (private) IPs are not used over the internet path.",
"E": "Private instances should not allow SSH from a public IP; traffic comes from the bastion inside the VPC."
}
},
{
"id": "saa-c03-design_secure_applications_architectures-028",
"certification_id": "SAA-C03",
"domain": "DESIGN_SECURE_APPLICATIONS_ARCHITECTURES",
"difficulty": "easy",
"tier": "FREE",
"required_selection_count": 2,
"active": true,
"question_text": "A two-tier web application is being designed. The application consists of a public-facing web tier hosted on Amazon EC2 in public subnets and a database tier consisting of Microsoft SQL Server running on Amazon EC2 in a private subnet. Security is a top priority. How should the security groups be configured in this situation? (Choose two.)",
"option_a": "Configure the web tier security group to allow inbound traffic on port 443 from 0.0.0.0/0.",
"option_b": "Configure the web tier security group to allow outbound traffic on port 443 to 0.0.0.0/0.",
"option_c": "Configure the database tier security group to allow inbound traffic on port 1433 from the web tier security group.",
"option_d": "Configure the database tier security group to allow outbound traffic on ports 443 and 1433 to the web tier security group.",
"option_e": "Configure the database tier security group to allow inbound traffic on ports 443 and 1433 from the web tier security group.",
"correct_answers": ["A", "C"],
"explanation_detailed": "Allow HTTPS to web tier from the internet and restrict DB inbound to SQL Server (1433) from only the web tier security group.",
"incorrect_explanations": {
"B": "Outbound 443 from the web tier is unrelated to exposing the site.",
"D": "DB outbound to web tier is not required for standard operations.",
"E": "DB should not accept HTTPS (443); only 1433 is needed."
}
},
{
"id": "saa-c03-design_high_performing_architectures-029",
"certification_id": "SAA-C03",
"domain": "DESIGN_HIGH_PERFORMING_ARCHITECTURES",
"difficulty": "medium",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "A company wants to move a multi-tier application from on-premises to AWS to improve application performance. The application consists of application tiers that communicate via RESTful services. Transactions are dropped when a tier becomes overwhelmed. A solutions architect must design a solution that resolves these issues and modernizes the application. Which solution meets these requirements and is the MOST operationally efficient?",
"option_a": "Use Amazon API Gateway and route transactions to AWS Lambda functions as the application tier. Use Amazon SQS as the messaging layer between application services.",
"option_b": "Use Amazon CloudWatch metrics to analyze historical performance to determine peak server usage during failures. Increase the size of the EC2 instances running the application tier to meet peak requirements.",
"option_c": "Use Amazon Simple Notification Service (Amazon SNS) to handle messaging between application servers running in an EC2 Auto Scaling group. Use CloudWatch to monitor the SNS queue length and scale accordingly.",
"option_d": "Use Amazon Simple Queue Service (Amazon SQS) to handle messaging between application servers running in an EC2 Auto Scaling group. Use CloudWatch to monitor the SQS queue length and scale up when communication failures are detected.",
"correct_answers": ["A"],
"explanation_detailed": "Serverless API Gateway + Lambda removes server management and SQS decouples tiers to prevent dropped transactions.",
"incorrect_explanations": {
"B": "Vertical scaling does not address bursty decoupling and adds cost.",
"C": "SNS is pub/sub without queue depth; also queue length metrics apply to SQS, not SNS.",
"D": "Still EC2-based and less operationally efficient than serverless."
}
},
{
"id": "saa-c03-design_cost_optimized_architectures-030",
"certification_id": "SAA-C03",
"domain": "DESIGN_COST_OPTIMIZED_ARCHITECTURES",
"difficulty": "easy",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "A company stores sensitive customer transaction data in an Amazon DynamoDB table. The company must retain the data for 7 years. Which solution is the MOST cost-effective to meet these requirements?",
"option_a": "Configure global DynamoDB tables. For RPO recovery, point the application to a different AWS region.",
"option_b": "Configure DynamoDB point-in-time recovery. For RPO recovery, restore to the desired point in time.",
"option_c": "Export the DynamoDB data to Amazon S3 Glacier daily. For RPO recovery, import the data from S3 Glacier back to DynamoDB.",
"option_d": "Schedule EBS snapshots for the DynamoDB table every 15 minutes. For RPO recovery, restore the DynamoDB table using the EBS snapshot.",
"correct_answers": ["B"],
"explanation_detailed": "Per the provided key, PITR is selected to meet retention/recovery goals with minimal operational cost.",
"incorrect_explanations": {
"A": "Global tables increase cross-region cost and do not address long-term retention.",
"C": "Daily exports and Glacier imports add complexity and retrieval delays.",
"D": "EBS snapshots are not applicable to DynamoDB."
}
},
{
"id": "saa-c03-design_cost_optimized_architectures-031",
"certification_id": "SAA-C03",
"domain": "DESIGN_COST_OPTIMIZED_ARCHITECTURES",
"difficulty": "easy",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "A company plans to use a DynamoDB table for data storage. The company is concerned about cost optimization. The table will not be used most mornings. In the evenings, read and write traffic will be unpredictable, with rapid spikes when they occur. What should a solutions architect recommend?",
"option_a": "Create a DynamoDB table in on-demand mode.",
"option_b": "Create a DynamoDB table with a global secondary index.",
"option_c": "Create a DynamoDB table with provisioned capacity and auto scaling.",
"option_d": "Create a DynamoDB table in provisioned mode and configure it as a global table.",
"correct_answers": ["A"],
"explanation_detailed": "On-demand billing automatically accommodates idle periods and unpredictable spikes without capacity planning.",
"incorrect_explanations": {
"B": "A GSI is not a capacity/cost optimization strategy by itself.",
"C": "Provisioned with auto scaling requires capacity settings and may lag during sudden spikes.",
"D": "Global tables increase cost and do not solve the unpredictable traffic pattern."
}
},
{
"id": "saa-c03-design_secure_applications_architectures-032",
"certification_id": "SAA-C03",
"domain": "DESIGN_SECURE_APPLICATIONS_ARCHITECTURES",
"difficulty": "easy",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "A company hosts its web application on AWS. The company wants to ensure that all Amazon EC2 instances, Amazon RDS database instances, and Amazon Redshift clusters are tagged. The company wants to minimize the effort required to configure and operate this check. What should a solutions architect do?",
"option_a": "Use AWS Config rules to define and detect resources that are not properly tagged.",
"option_b": "Use Cost Explorer to display resources that are not properly tagged. Manually tag these resources.",
"option_c": "Write API calls to check all resources for proper tag assignment. Run the code periodically on an EC2 instance.",
"option_d": "Write API calls to check all resources for proper tag assignment. Schedule an AWS Lambda function via CloudWatch to run the code periodically.",
"correct_answers": ["A"],
"explanation_detailed": "AWS Config managed rules can evaluate resource compliance with tagging policies continuously with minimal operations.",
"incorrect_explanations": {
"B": "Manual review and tagging is labor-intensive.",
"C": "Running custom scripts on EC2 adds management overhead.",
"D": "Custom Lambda checks require code and maintenance compared to managed rules."
}
},
{
"id": "saa-c03-design_cost_optimized_architectures-033",
"certification_id": "SAA-C03",
"domain": "DESIGN_COST_OPTIMIZED_ARCHITECTURES",
"difficulty": "easy",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "A development team needs to host a website that will be accessed by other teams. The website content consists of HTML, CSS, client-side JavaScript, and images. Which method is the MOST cost-effective for hosting the website?",
"option_a": "Containerize the website and host it on AWS Fargate.",
"option_b": "Create an Amazon S3 bucket and host the website there.",
"option_c": "Deploy a web server on an Amazon EC2 instance to host the website.",
"option_d": "Configure an Application Load Balancer with an AWS Lambda target using Express.js.",
"correct_answers": ["B"],
"explanation_detailed": "Static websites are cheapest and simplest on S3 static website hosting or via CloudFront-backed S3.",
"incorrect_explanations": {
"A": "Fargate adds runtime cost and complexity for static content.",
"C": "EC2 requires server management and is costlier for static assets.",
"D": "ALB + Lambda is over-engineered for static hosting."
}
},
{
"id": "saa-c03-design_high_performing_architectures-034",
"certification_id": "SAA-C03",
"domain": "DESIGN_HIGH_PERFORMING_ARCHITECTURES",
"difficulty": "medium",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "A company operates an online marketplace application on AWS. The application serves hundreds of thousands of users during peak hours. The company needs a solution that is scalable and nearly real-time to share details of millions of financial transactions with various internal applications. The transactions also need to be processed to remove sensitive data before being stored in a document database for low-latency retrieval. What should a solutions architect recommend?",
"option_a": "Store the transaction data in Amazon DynamoDB. Configure a DynamoDB rule to remove sensitive data from each transaction after writing. Use DynamoDB Streams to share transaction data with other applications.",
"option_b": "Stream the transaction data to Amazon Kinesis Data Firehose to store the data in both Amazon DynamoDB and Amazon S3. Use Lambda integration with Kinesis Data Firehose to remove sensitive data. Other applications can consume the data stored in Amazon S3.",
"option_c": "Stream the transaction data to Amazon Kinesis Data Streams. Use Lambda integration to remove sensitive data from each transaction, then store the transaction data in Amazon DynamoDB. Other applications can consume the data from the Kinesis stream.",
"option_d": "Batch store the transaction data in Amazon S3 as files. Use AWS Lambda to process each file and remove sensitive data before updating the files in Amazon S3. The Lambda function then stores the data in Amazon DynamoDB. Other applications can consume the transaction files stored in Amazon S3.",
"correct_answers": ["C"],
"explanation_detailed": "Kinesis Data Streams supports near real-time ingestion and fan-out; Lambda can redact PII before persisting to DynamoDB while streams serve other consumers.",
"incorrect_explanations": {
"A": "DynamoDB does not provide pre-write redaction and would expose sensitive data.",
"B": "Firehose targets are limited; near real-time fan-out to consumers fits KDS better.",
"D": "Batch S3 processing is not near real-time."
}
},
{
"id": "saa-c03-design_secure_applications_architectures-035",
"certification_id": "SAA-C03",
"domain": "DESIGN_SECURE_APPLICATIONS_ARCHITECTURES",
"difficulty": "easy",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "A company runs a multi-tier application on AWS. For compliance, governance, auditing, and security, the company must track configuration changes to its AWS resources and record an audit trail of API calls made to those resources. What should a solutions architect do?",
"option_a": "Use AWS CloudTrail to track configuration changes and AWS Config to record API calls.",
"option_b": "Use AWS Config to track configuration changes and AWS CloudTrail to record API calls.",
"option_c": "Use AWS Config to track configuration changes and Amazon CloudWatch to record API calls.",
"option_d": "Use AWS CloudTrail to track configuration changes and Amazon CloudWatch to record API calls.",
"correct_answers": ["B"],
"explanation_detailed": "AWS Config records resource configuration changes; CloudTrail records API activity across the account.",
"incorrect_explanations": {
"A": "CloudTrail does not track configuration state changes; Config does.",
"C": "CloudWatch is not the API call audit log service.",
"D": "CloudTrail does not replace Config for configuration change tracking."
}
},
{
"id": "saa-c03-design_secure_applications_architectures-036",
"certification_id": "SAA-C03",
"domain": "DESIGN_SECURE_APPLICATIONS_ARCHITECTURES",
"difficulty": "medium",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "A company is preparing to launch a public-facing web application on AWS. The architecture consists of Amazon EC2 instances inside a VPC behind an Elastic Load Balancer (ELB). A third-party service is used for DNS. The company’s solutions architect must recommend a solution to detect and protect against large-scale DDoS attacks. Which solution meets these requirements?",
"option_a": "Enable Amazon GuardDuty in the account.",
"option_b": "Enable Amazon Inspector on the EC2 instances.",
"option_c": "Enable AWS Shield and assign Amazon Route 53 to it.",
"option_d": "Enable AWS Shield Advanced and assign the ELB to it.",
"correct_answers": ["D"],
"explanation_detailed": "AWS Shield Advanced provides enhanced DDoS detection/mitigation and integrates with load balancers for protection.",
"incorrect_explanations": {
"A": "GuardDuty is threat detection, not DDoS mitigation.",
"B": "Inspector assesses vulnerabilities; it does not mitigate DDoS.",
"C": "Assigning Route 53 is irrelevant when DNS is third-party and ELB is the exposed endpoint."
}
},
{
"id": "saa-c03-design_resilient_architectures-037",
"certification_id": "SAA-C03",
"domain": "DESIGN_RESILIENT_ARCHITECTURES",
"difficulty": "medium",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "A company is building an application on AWS. The application will produce output files ranging in size from tens of gigabytes to hundreds of terabytes. The application’s data must be stored in a standard file system structure. The company wants a solution that automatically scales, is highly available, and requires minimal operational overhead. Which solution meets these requirements?",
"option_a": "Migrate the application to run as containers on Amazon ECS. Use Amazon S3 for storage.",
"option_b": "Migrate the application to run as containers on Amazon EKS. Use Amazon EBS for storage.",
"option_c": "Migrate the application to Amazon EC2 instances in a Multi-AZ Auto Scaling group. Use Amazon Elastic File System (Amazon EFS) for storage.",
"option_d": "Migrate the application to Amazon EC2 instances in a Multi-AZ Auto Scaling group. Use Amazon EBS for storage.",
"correct_answers": ["C"],
"explanation_detailed": "EFS provides a POSIX file system that scales automatically and is multi-AZ, minimizing operational burden.",
"incorrect_explanations": {
"A": "S3 is object storage, not a standard file system mount.",
"B": "EBS volumes are AZ-scoped and do not auto-scale across instances.",
"D": "EBS lacks multi-instance, multi-AZ shared file semantics."
}
},
{
"id": "saa-c03-design_secure_applications_architectures-038",
"certification_id": "SAA-C03",
"domain": "DESIGN_SECURE_APPLICATIONS_ARCHITECTURES",
"difficulty": "medium",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "A company stores its accounting records in Amazon S3. The records must be immediately accessible for 1 year and then archived for an additional 9 years. No one at the company—including administrative and root users—can delete the records during the entire 10-year period. The records must be stored with maximum resilience. Which solution meets these requirements?",
"option_a": "Store the records in S3 Glacier for the entire 10-year period. Use an access control policy to deny record deletion for 10 years.",
"option_b": "Store the records using S3 Intelligent-Tiering. Use an IAM policy to deny record deletion. After 10 years, change the IAM policy to allow deletion.",
"option_c": "Use an S3 lifecycle policy to transition the records from S3 Standard to S3 Glacier Deep Archive after 1 year. Use S3 Object Lock in compliance mode for 10 years.",
"option_d": "Use an S3 lifecycle policy to transition the records from S3 Standard to S3 One Zone-Infrequent Access (S3 One Zone-IA) after 1 year. Use S3 Object Lock in governance mode for 10 years.",
"correct_answers": ["C"],
"explanation_detailed": "S3 Object Lock compliance mode enforces WORM retention even for root users; lifecycle transitions meet access and archive requirements with highest durability.",
"incorrect_explanations": {
"A": "Glacier alone does not enforce WORM nor immediate year-one access.",
"B": "IAM policies can be changed; do not provide immutable retention.",
"D": "One Zone-IA reduces resilience and governance mode can be bypassed by privileged users."
}
},
{
"id": "saa-c03-design_resilient_architectures-039",
"certification_id": "SAA-C03",
"domain": "DESIGN_RESILIENT_ARCHITECTURES",
"difficulty": "easy",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "A company runs several Windows workloads on AWS. The company’s employees use Windows file shares hosted on two Amazon EC2 instances. The file shares synchronize data between them and maintain duplicate copies. The company wants a highly available and durable storage solution that preserves the way users currently access the files. What should a solutions architect do?",
"option_a": "Migrate all data to Amazon S3. Configure IAM authentication for users to access the files.",
"option_b": "Set up an Amazon S3 File Gateway. Mount the S3 File Gateway on the existing EC2 instances.",
"option_c": "Extend the file sharing environment to Amazon FSx for Windows File Server with a Multi-AZ configuration. Migrate all data to FSx for Windows File Server.",
"option_d": "Extend the file sharing environment to Amazon Elastic File System (Amazon EFS) with a Multi-AZ configuration. Migrate all data to Amazon EFS.",
"correct_answers": ["C"],
"explanation_detailed": "FSx for Windows File Server provides SMB compatibility, Multi-AZ HA, and integrates with Active Directory.",
"incorrect_explanations": {
"A": "S3 is object storage and not SMB-compatible for user file shares.",
"B": "S3 File Gateway presents NFS/SMB caching but is less suitable for full HA Windows file server semantics.",
"D": "EFS is NFS-based, not SMB, and does not meet Windows file share requirements."
}
},
{
"id": "saa-c03-design_secure_applications_architectures-040",
"certification_id": "SAA-C03",
"domain": "DESIGN_SECURE_APPLICATIONS_ARCHITECTURES",
"difficulty": "medium",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "A solutions architect is designing a VPC architecture that includes several subnets. The architecture will host applications that use Amazon EC2 instances and Amazon RDS database instances. The architecture consists of six subnets across two Availability Zones. Each Availability Zone includes a public subnet, a private subnet, and a dedicated database subnet. Only EC2 instances in the private subnets should have access to the RDS databases. Which solution meets these requirements?",
"option_a": "Create a new route table that excludes routes to the public subnet CIDR blocks. Associate the route table with the database subnets.",
"option_b": "Create a security group that denies inbound traffic from the security group assigned to the public subnet instances. Attach this security group to the database instances.",
"option_c": "Create a security group that allows inbound traffic from the security group assigned to the private subnet instances. Attach this security group to the database instances.",
"option_d": "Create a new VPC peering connection between the public and private subnets. Create a separate peering connection between the private and database subnets.",
"correct_answers": ["C"],
"explanation_detailed": "Use security groups to allow DB inbound only from the private tier’s SG, enforcing least privilege.",
"incorrect_explanations": {
"A": "Routing does not enforce instance-level access control.",
"B": "Security groups are allow-lists; you cannot deny specific SGs.",
"D": "VPC peering is not applicable within a single VPC’s subnets."
}
},
{
"id": "saa-c03-design_secure_applications_architectures-041",
"certification_id": "SAA-C03",
"domain": "DESIGN_SECURE_APPLICATIONS_ARCHITECTURES",
"difficulty": "hard",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "A company registered its domain name with Amazon Route 53. The company uses Amazon API Gateway in the ca-central-1 region as a public interface for its backend microservice APIs. Third-party services consume the APIs securely. The company wants to design its API Gateway URL using the company’s domain name and corresponding certificate so that third-party services can use HTTPS. Which solution meets these requirements?",
"option_a": "Create stage variables in API Gateway with Name='Endpoint-URL' and Value='Company Domain Name' to override the default URL. Import the public certificate associated with the company domain into AWS Certificate Manager (ACM).",
"option_b": "Create Route 53 DNS records with the company domain name. Point the alias record to the Regional stage endpoint of API Gateway. Import the public certificate associated with the company domain into ACM in the us-east-1 region.",
"option_c": "Create a Regional API Gateway endpoint. Associate the API Gateway endpoint with the company domain name. Import the public certificate associated with the company domain into ACM in the same region. Attach the certificate to the API Gateway endpoint. Configure Route 53 to route traffic to the API Gateway endpoint.",
"option_d": "Create a Regional API Gateway endpoint. Associate the API Gateway endpoint with the company domain name. Import the public certificate associated with the company domain into ACM in the us-east-1 region. Attach the certificate to the API Gateway. Create Route 53 DNS records with the company domain name. Point an A record to the company domain name.",
"correct_answers": ["D"],
"explanation_detailed": "Per the provided key, a Regional endpoint with ACM certificate in us-east-1 and Route 53 records is selected.",
"incorrect_explanations": {
"A": "Stage variables do not create custom domains or attach TLS certs.",
"B": "Certificate region and aliasing details are incorrect per the selected approach.",
"C": "Not selected per the provided key."
}
},
{
"id": "saa-c03-design_secure_applicationS_architectures-042",
"certification_id": "SAA-C03",
"domain": "DESIGN_SECURE_APPLICATIONS_ARCHITECTURES",
"difficulty": "medium",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "A social media website deployed by a hospital allows users to upload reports in PDF and JPEG formats. The hospital needs to modify the Lambda code to detect protected health information (PHI) in the reports. Which solution meets these requirements with the LOWEST operational overhead?",
"option_a": "Use existing Python libraries to extract text from the reports and identify PHI from the extracted text.",
"option_b": "Use Amazon Textract to extract text from the reports. Use Amazon SageMaker to identify PHI from the extracted text.",
"option_c": "Use Amazon Textract to extract text from the reports. Use Amazon Comprehend Medical to identify PHI from the extracted text.",
"option_d": "Use Amazon Rekognition to extract text from the reports. Use Amazon Comprehend Medical to identify PHI from the extracted text.",
"correct_answers": ["C"],
"explanation_detailed": "Textract handles OCR for PDFs/images; Comprehend Medical detects PHI entities with a fully managed service.",
"incorrect_explanations": {
"A": "Custom libraries increase maintenance and may lack medical accuracy.",
"B": "SageMaker requires building models and pipelines.",
"D": "Rekognition OCR is not the primary service for document text extraction compared to Textract."
}
},
{
"id": "saa-c03-design_resilient_architectures-043",
"certification_id": "SAA-C03",
"domain": "DESIGN_RESILIENT_ARCHITECTURES",
"difficulty": "easy",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "A company runs a batch processing application on Amazon EC2 instances. The application processes messages from an Amazon SQS queue, writes to an Amazon RDS table, and deletes the message from the queue. Occasional duplicate records appear in the RDS table, although the SQS queue has no duplicate messages. What should a solutions architect do to ensure messages are processed only once?",
"option_a": "Use the CreateQueue API to create a new queue.",
"option_b": "Use the AddPermission API to add the appropriate permissions.",
"option_c": "Use the ReceiveMessage API to set an appropriate wait time.",
"option_d": "Use the ChangeMessageVisibility API to increase the visibility timeout.",
"correct_answers": ["D"],
"explanation_detailed": "A longer visibility timeout prevents simultaneous processing by multiple workers.",
"incorrect_explanations": {
"A": "Creating a new queue does not change processing semantics.",
"B": "Permissions are not the cause of duplicates.",
"C": "Wait time affects long polling, not visibility."
}
},
{
"id": "saa-c03-design_resilient_architectures-044",
"certification_id": "SAA-C03",
"domain": "DESIGN_RESILIENT_ARCHITECTURES",
"difficulty": "easy",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "A solutions architect is designing a new hybrid architecture to extend a company’s on-premises infrastructure to AWS. The company requires a highly available connection with consistently low latency to one AWS region. The company must minimize costs and is willing to accept slower traffic if the primary connection fails. What should the solutions architect do?",
"option_a": "Provision an AWS Direct Connect connection to a region. Provision a VPN connection as backup if the primary Direct Connect fails.",
"option_b": "Provision a VPN tunnel connection to a region for private connectivity. Provision a second VPN tunnel for private connectivity as backup if the primary VPN fails.",
"option_c": "Provision an AWS Direct Connect connection to a region. Provision a second Direct Connect connection to the same region as backup if the primary Direct Connect fails.",
"option_d": "Provision an AWS Direct Connect connection to a region. Use the failover attribute via the AWS CLI to automatically create a backup connection if the primary Direct Connect fails.",
"correct_answers": ["A"],
"explanation_detailed": "DX for primary performance, VPN for low-cost failover meets the availability and cost requirements.",
"incorrect_explanations": {
"B": "Dual VPN lacks the desired consistent low latency.",
"C": "Dual DX increases cost beyond the requirement.",
"D": "No such automatic DX failover creation attribute exists."
}
},
{
"id": "saa-c03-design_resilient_architectures-045",
"certification_id": "SAA-C03",
"domain": "DESIGN_RESILIENT_ARCHITECTURES",
"difficulty": "easy",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "A company is migrating its multi-tier application to AWS and wants to implement a solution to improve performance. The application’s current on-premises version runs on a single EC2 instance with an attached database on EBS. Which solution will help modernize the application while minimizing operational overhead?",
"option_a": "Move the database so that both EC2 instances use duplicate EBS volumes containing all documents.",
"option_b": "Configure the Application Load Balancer to direct a user to the server that has the complete set of documents.",
"option_c": "Migrate the data from both EBS volumes to Amazon EFS. Modify the application to store new documents on Amazon EFS.",
"option_d": "Configure the Application Load Balancer to send requests to both servers, merging the results.",
"correct_answers": ["C"],
"explanation_detailed": "EFS centralizes file storage across instances, removing data divergence and simplifying scaling.",
"incorrect_explanations": {
"A": "Duplicating EBS volumes does not solve ongoing synchronization.",
"B": "Directing users does not unify storage.",
"D": "Merging results adds complexity and latency."
}
},
{
"id": "saa-c03-design_cost_optimized_architectures-046",
"certification_id": "SAA-C03",
"domain": "DESIGN_COST_OPTIMIZED_ARCHITECTURES",
"difficulty": "medium",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "A company uses NFS to store large video files on a local on-premises NAS. Files vary from 1 MB to 500 GB, and the total storage is 70 TB. The company decides to migrate the video files to Amazon S3. They must migrate as quickly as possible while using the least network bandwidth. Which solution meets these requirements?",
"option_a": "Create an S3 bucket. Create an IAM role with permission to write to the bucket. Use the AWS CLI to copy all files locally to the S3 bucket.",
"option_b": "Create an AWS Snowball Edge job. Receive a Snowball Edge device on-premises. Use the Snowball Edge client to transfer data to the device. Return the device so AWS can import the data to S3.",
"option_c": "Deploy a local S3 File Gateway. Create a public service endpoint to connect to the S3 File Gateway. Create an S3 bucket. Create a new NFS file share on the S3 File Gateway. Point the existing NFS share to the S3 File Gateway. Transfer the data from the existing NFS share to the S3 File Gateway.",
"option_d": "Set up an AWS Direct Connect connection between the on-premises network and AWS. Deploy a local S3 File Gateway. Create a public virtual interface (VIF) to connect to the S3 File Gateway. Create an S3 bucket. Create a new NFS file share on the S3 File Gateway. Point the existing NFS share to the S3 File Gateway. Transfer the data from the existing NFS share to the S3 File Gateway.",
"correct_answers": ["C"],
"explanation_detailed": "Per the provided key, S3 File Gateway is selected to bridge NFS to S3 with minimal bandwidth impact.",
"incorrect_explanations": {
"A": "Direct copy over WAN for 70 TB is slow and bandwidth heavy.",
"B": "Snowball Edge would minimize bandwidth but is not selected per the key.",
"D": "Direct Connect adds cost/lead time and is unnecessary for a migration."
}
},
{
"id": "saa-c03-design_high_performing_architectures-047",
"certification_id": "SAA-C03",
"domain": "DESIGN_HIGH_PERFORMING_ARCHITECTURES",
"difficulty": "hard",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "A company has an application that ingests messages. Dozens of other applications and microservices quickly consume these messages. The number of messages varies dramatically and sometimes suddenly spikes to 100,000 per second. The company wants to decouple the solution and increase scalability. Which solution meets these requirements?",
"option_a": "Persist the messages in Amazon Kinesis Data Analytics. Configure the consumer applications to read and process the messages.",
"option_b": "Deploy the ingestion application on EC2 instances in an Auto Scaling group that scales based on CPU metrics.",
"option_c": "Write the messages to Amazon Kinesis Data Streams with a single shard. Use an AWS Lambda function to pre-process the messages and store them in Amazon DynamoDB. Configure the consumer applications to read from DynamoDB.",
"option_d": "Publish the messages to an Amazon SNS topic with multiple Amazon SQS subscriptions. Configure the consumer applications to process messages from the queues.",
"correct_answers": ["A"],
"explanation_detailed": "Per the provided key, Kinesis Data Analytics is selected to handle high-throughput streaming and consumer processing.",
"incorrect_explanations": {
"B": "EC2 scaling on CPU does not address stream decoupling for massive burst rates.",
"C": "A single shard cannot handle 100k msgs/sec; DynamoDB is not a streaming bus.",
"D": "SNS+SQS fan-out is not selected per the provided key."
}
},
{
"id": "saa-c03-design_resilient_architectures-048",
"certification_id": "SAA-C03",
"domain": "DESIGN_RESILIENT_ARCHITECTURES",
"difficulty": "hard",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "A company is migrating a distributed application to AWS. The application handles variable workloads. The legacy platform consists of a primary server coordinating work across multiple compute nodes. The company wants to modernize the application with a solution that maximizes resilience and scalability. How should the solutions architect design the architecture?",
"option_a": "Set up an Amazon SQS queue as the destination for work items. Deploy compute nodes on EC2 instances managed in an Auto Scaling group. Configure scheduled scaling for EC2 Auto Scaling.",
"option_b": "Set up an Amazon SQS queue as the destination for work items. Deploy compute nodes on EC2 instances managed in an Auto Scaling group. Configure EC2 Auto Scaling based on the queue length.",
"option_c": "Deploy the primary server and compute nodes on EC2 instances managed in an Auto Scaling group. Configure AWS CloudTrail as the destination for work items. Configure EC2 Auto Scaling based on the primary server’s load.",
"option_d": "Deploy the primary server and compute nodes on EC2 instances managed in an Auto Scaling group. Configure Amazon EventBridge (CloudWatch Events) as the destination for work items. Configure EC2 Auto Scaling based on the compute nodes’ load.",
"correct_answers": ["C"],
"explanation_detailed": "Per the provided key, maintaining a coordinating primary server and scaling by its load is selected.",
"incorrect_explanations": {
"A": "Scheduled scaling does not adapt to variable workload.",
"B": "Queue length scaling is a common approach, but not selected per the key.",
"D": "EventBridge is not a work queue to distribute jobs to nodes."
}
},
{
"id": "saa-c03-design_cost_optimized_architectures-049",
"certification_id": "SAA-C03",
"domain": "DESIGN_COST_OPTIMIZED_ARCHITECTURES",
"difficulty": "easy",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "A company stores its sensitive customer transaction data in an Amazon DynamoDB table. The company must retain the data for 7 years. Which solution is the MOST cost-effective?",
"option_a": "Configure global DynamoDB tables. For RPO recovery, point the application to a different AWS region.",
"option_b": "Enable DynamoDB point-in-time recovery. For RPO recovery, restore to the desired point in time.",
"option_c": "Export DynamoDB data to Amazon S3 Glacier daily. For RPO recovery, import data from S3 Glacier back to DynamoDB.",
"option_d": "Schedule EBS snapshots for the DynamoDB table every 15 minutes. For RPO recovery, restore the table from the EBS snapshot.",
"correct_answers": ["B"],
"explanation_detailed": "Per the provided key, enabling PITR meets retention and recovery needs with low operational effort.",
"incorrect_explanations": {
"A": "Global tables raise cost and do not handle long-term retention.",
"C": "Daily Glacier export/import adds complexity and slow recovery.",
"D": "DynamoDB is not backed up via EBS snapshots."
}
},
{
"id": "saa-c03-design_cost_optimized_architectures-050",
"certification_id": "SAA-C03",
"domain": "DESIGN_COST_OPTIMIZED_ARCHITECTURES",
"difficulty": "easy",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "A company has an application that generates a large number of files, each about 5 MB in size. The files are stored in Amazon S3. Company policy requires that the files be retained for 4 years before they can be deleted. Immediate access is required at all times because the files contain critical business data that are not easily reproduced. The files are frequently accessed during the first 30 days after creation but rarely thereafter. Which storage solution is the MOST cost-effective?",
"option_a": "Create an S3 lifecycle policy to move files from S3 Standard to S3 Glacier 30 days after object creation. Delete the files 4 years after creation.",
"option_b": "Create an S3 lifecycle policy to move files from S3 Standard to S3 One Zone-Infrequent Access (S3 One Zone-IA) 30 days after creation. Delete the files 4 years after creation.",
"option_c": "Create an S3 lifecycle policy to move files from S3 Standard to S3 Standard-Infrequent Access (S3 Standard-IA) 30 days after creation. Delete the files 4 years after creation.",
"option_d": "Create an S3 lifecycle policy to move files from S3 Standard to S3 Standard-Infrequent Access (S3 Standard-IA) 30 days after creation, then move the files to S3 Glacier 4 years after creation.",
"correct_answers": ["C"],
"explanation_detailed": "Standard-IA after 30 days maintains immediate access at lower cost and deletion at 4 years meets retention.",
"incorrect_explanations": {
"A": "Glacier does not provide immediate access.",
"B": "One Zone-IA reduces availability and durability.",
"D": "Moving to Glacier at 4 years is unnecessary since data is deleted at 4 years."
}
},

{
"id": "saa-c03-domain-051",
"certification_id": "SAA-C03",
"domain": "DESIGN_RESILIENT_ARCHITECTURES",
"difficulty": "medium",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "A company runs an application on multiple EC2 instances. The application processes messages from an SQS queue, writes to an RDS table, and deletes the message from the queue. Occasionally, duplicate records are found in the RDS table, even though the SQS queue does not contain duplicates. What should a solutions architect do to ensure messages are processed only once?",
"option_a": "Use the CreateQueue API to create a new queue.",
"option_b": "Use the AddPermission API to add the appropriate permissions.",
"option_c": "Use the ReceiveMessage API to set an appropriate wait time.",
"option_d": "Use the ChangeMessageVisibility API to increase the visibility timeout.",
"correct_answers": ["D"],
"explanation_detailed": "Increasing the SQS visibility timeout ensures that once a consumer reads a message it is hidden long enough for processing to complete, preventing other consumers from receiving and processing the same message again.",
"incorrect_explanations": {
"A": "CreateQueue creates a new queue but does not affect how often messages are delivered or prevent duplicate processing.",
"B": "AddPermission controls access to the queue but does not influence message visibility or duplicate delivery.",
"C": "Changing the wait time affects long polling, not how long a message remains invisible after being read."
}
},
{
"id": "saa-c03-domain-052",
"certification_id": "SAA-C03",
"domain": "DESIGN_COST_OPTIMIZED_ARCHITECTURES",
"difficulty": "medium",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "A company uses a legacy on-premises file server running SMB to store large video files. The files are frequently accessed shortly after creation but rarely thereafter, and the total storage is nearing capacity. A solutions architect must increase available storage without sacrificing low-latency access to recently used files and provide lifecycle management. Which solution meets these requirements?",
"option_a": "Use AWS DataSync to copy files older than 7 days from the on-premises file server to AWS.",
"option_b": "Create an Amazon S3 File Gateway to extend storage. Create an S3 lifecycle policy to transition data to S3 Glacier Deep Archive after 7 days.",
"option_c": "Create an Amazon FSx for Windows File Server file system to extend storage.",
"option_d": "Install a utility on each user’s computer to access Amazon S3. Create an S3 lifecycle policy to transition data to S3 Glacier Flexible Retrieval after 7 days.",
"correct_answers": ["D"],
"explanation_detailed": "Using an S3 access utility on each client gives low-latency access to recently used objects in S3 while lifecycle policies automatically transition older data to a cheaper archival class, satisfying both performance and lifecycle management requirements.",
"incorrect_explanations": {
"A": "DataSync alone copies data but does not provide user-transparent, low-latency access to frequently used files or ongoing lifecycle management.",
"B": "S3 File Gateway can extend storage and support SMB, but transitioning to Glacier Deep Archive after only 7 days makes frequently needed recent files slower to retrieve than required.",
"C": "FSx for Windows File Server provides SMB and additional capacity but does not natively provide object lifecycle transitions to archival storage for cost optimization."
}
},
{
"id": "saa-c03-domain-053",
"certification_id": "SAA-C03",
"domain": "DESIGN_HIGH_PERFORMING_ARCHITECTURES",
"difficulty": "hard",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "A company hosts its web application on AWS. The application uses an S3 bucket to store its static data. The company wants to improve performance and reduce latency for both its static and dynamic data. Which solution best meets these requirements?",
"option_a": "Create an Amazon CloudFront distribution with the S3 bucket and ALB as origins. Configure Route 53 to route traffic to the CloudFront distribution.",
"option_b": "Create a CloudFront distribution with the ALB as the origin. Create an AWS Global Accelerator with the S3 bucket as an endpoint. Configure Route 53 to route traffic to the CloudFront distribution.",
"option_c": "Create a CloudFront distribution with the S3 bucket as the origin. Create a Global Accelerator with both the ALB and CloudFront as endpoints. Create a custom domain that points to the accelerator’s DNS name and use it as the web application endpoint.",
"option_d": "Create a CloudFront distribution with the ALB as the origin. Create a Global Accelerator with the S3 bucket as an endpoint. Create two domain names—one for dynamic content via CloudFront and one for static content via the accelerator. Use these domain names as endpoints.",
"correct_answers": ["C"],
"explanation_detailed": "Placing static content behind CloudFront and using AWS Global Accelerator with both the ALB and CloudFront as endpoints gives optimized global routing and caching for static and dynamic content behind a single application endpoint.",
"incorrect_explanations": {
"A": "This improves latency via CloudFront but does not leverage Global Accelerator’s anycast edge network for further performance gains.",
"B": "Global Accelerator cannot use an S3 bucket directly as an endpoint, so this design is not valid.",
"D": "This splits traffic across two domains and misuses Global Accelerator with S3, complicating the architecture instead of presenting a single optimized global endpoint."
}
},
{
"id": "saa-c03-domain-054",
"certification_id": "SAA-C03",
"domain": "DESIGN_SECURE_APPLICATIONS_ARCHITECTURES",
"difficulty": "medium",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "A company performs monthly maintenance on its AWS infrastructure. During maintenance, it must rotate the credentials for its Amazon RDS for MySQL databases across several AWS regions. Which solution meets these requirements with the LOWEST operational overhead?",
"option_a": "Store the credentials as secrets in AWS Secrets Manager. Use multi-region secret replication for the necessary regions. Configure Secrets Manager to rotate the secrets on a schedule.",
"option_b": "Store the credentials as secrets in AWS Systems Manager by creating a secure string parameter. Use multi-region secret replication for the necessary regions. Configure Systems Manager to rotate the secrets on a schedule.",
"option_c": "Store the credentials in an S3 bucket with server-side encryption enabled. Use Amazon EventBridge (CloudWatch Events) to invoke a Lambda function to rotate the credentials.",
"option_d": "Encrypt the credentials as secrets using customer-managed multi-region KMS keys. Store the secrets in a global DynamoDB table. Use a Lambda function to retrieve the secrets from DynamoDB and call the RDS API to rotate them.",
"correct_answers": ["A"],
"explanation_detailed": "AWS Secrets Manager natively supports multi-region secret replication and automatic rotation of database credentials, minimizing custom code and operational effort.",
"incorrect_explanations": {
"B": "Systems Manager Parameter Store does not provide built-in multi-region secret replication and automatic rotation for RDS credentials in the same integrated way as Secrets Manager.",
"C": "Using S3 and EventBridge requires custom Lambda logic for rotation and secure distribution, increasing operational overhead.",
"D": "Implementing custom KMS encryption, a global DynamoDB table, and Lambda rotation logic is more complex and operationally heavy than using the managed capabilities of Secrets Manager."
}
},
{
"id": "saa-c03-domain-055",
"certification_id": "SAA-C03",
"domain": "DESIGN_RESILIENT_ARCHITECTURES",
"difficulty": "medium",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "A company runs a web application on EC2 instances behind an Application Load Balancer (ALB). The instances are in an Auto Scaling group spread across multiple Availability Zones. The application stores transaction data in a MySQL 8.0 database hosted on a large EC2 instance. As application load increases, database performance degrades rapidly. The application handles more read requests than writes. The company wants a solution that automatically scales the database to meet unpredictable read workloads while maintaining high availability. Which solution meets these requirements?",
"option_a": "Use Amazon Redshift with a single node for leader and compute functionality.",
"option_b": "Use Amazon RDS with a Single-AZ deployment. Configure RDS to add read replicas in a different Availability Zone.",
"option_c": "Use Amazon Aurora with a Multi-AZ deployment. Configure Aurora Auto Scaling with Aurora replicas.",
"option_d": "Use Amazon ElastiCache for Memcached with Spot EC2 instances.",
"correct_answers": ["C"],
"explanation_detailed": "Aurora with Multi-AZ and Aurora replicas supports automatic scaling of read capacity, high availability, and managed failover, which matches the requirement for unpredictable read-heavy workloads.",
"incorrect_explanations": {
"A": "Amazon Redshift is a data warehouse optimized for analytics, not for transactional OLTP workloads.",
"B": "Single-AZ RDS does not provide sufficient availability, and read replica scaling is not as seamless as Aurora Auto Scaling.",
"D": "ElastiCache can offload reads but does not replace the need for a scalable and highly available database tier."
}
},
{
"id": "saa-c03-domain-056",
"certification_id": "SAA-C03",
"domain": "DESIGN_COST_OPTIMIZED_ARCHITECTURES",
"difficulty": "medium",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "A company runs a legacy on-premises file server using NFS to store large video files. Each video file ranges from 1 MB to 500 GB, and the total storage is 70 TB. The company decides to migrate the video files to Amazon S3. They must migrate the files as quickly as possible while using the least amount of network bandwidth. Which solution meets these requirements?",
"option_a": "Create an S3 bucket. Create an IAM role with permissions to write to the bucket. Use the AWS CLI to copy all files locally to the S3 bucket.",
"option_b": "Create an AWS Snowball Edge job. Receive a Snowball Edge device on-premises. Use the Snowball Edge client to transfer data to the device. Return the device so AWS can import the data to S3.",
"option_c": "Deploy a local S3 File Gateway. Create a public service endpoint to connect to the S3 File Gateway. Create an S3 bucket. Create a new NFS file share on the S3 File Gateway. Point the existing NFS share to the S3 File Gateway. Transfer data from the existing NFS share to the S3 File Gateway.",
"option_d": "Set up an AWS Direct Connect connection between the on-premises network and AWS. Deploy a local S3 File Gateway. Create a public virtual interface (VIF) to connect to the S3 File Gateway. Create an S3 bucket. Create a new NFS file share on the S3 File Gateway. Point the existing NFS share to the S3 File Gateway. Transfer data from the existing NFS share to the S3 File Gateway.",
"correct_answers": ["C"],
"explanation_detailed": "Using an S3 File Gateway with an NFS file share allows bulk data transfer directly to S3 over optimized connections, minimizing changes to existing workflows and reducing WAN bandwidth usage compared to direct uploads.",
"incorrect_explanations": {
"A": "Copying 70 TB over the network with the AWS CLI consumes significant bandwidth and time.",
"B": "Snowball Edge minimizes network use but introduces additional shipping and handling steps, and the prompt solution chosen here focuses on gateway-based migration.",
"D": "Provisioning Direct Connect for a one-time migration adds cost and setup time that is unnecessary compared to using an S3 File Gateway over existing connectivity."
}
},
{
"id": "saa-c03-domain-057",
"certification_id": "SAA-C03",
"domain": "DESIGN_HIGH_PERFORMING_ARCHITECTURES",
"difficulty": "medium",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "A company has an application that ingests messages. Dozens of other applications and microservices then rapidly consume these messages. The message volume varies dramatically and sometimes spikes to 100,000 per second. The company wants to decouple the solution and increase scalability. Which solution meets these requirements?",
"option_a": "Persist the messages in Amazon Kinesis Data Analytics. Configure consumer applications to read and process the messages.",
"option_b": "Deploy the ingestion application on EC2 instances in an Auto Scaling group that scales based on CPU metrics.",
"option_c": "Write messages to Amazon Kinesis Data Streams with a single shard. Use an AWS Lambda function to pre-process the messages and store them in Amazon DynamoDB. Configure consumer applications to read from DynamoDB.",
"option_d": "Publish messages to an Amazon SNS topic with multiple Amazon SQS subscriptions. Configure consumer applications to process messages from the queues.",
"correct_answers": ["A"],
"explanation_detailed": "Persisting and processing the stream with an analytics-focused service such as Kinesis Data Analytics allows high-throughput ingestion and scalable consumption patterns suitable for large, spiky message volumes.",
"incorrect_explanations": {
"B": "Scaling only the ingestion EC2 fleet based on CPU does not inherently decouple producers from multiple consumers or address per-consumer scalability.",
"C": "Using a single Kinesis shard cannot handle spikes up to 100,000 records per second and would require scaling shards, not just Lambda and DynamoDB.",
"D": "SNS with SQS fan-out decouples producers and consumers but may not be as well-suited to extremely high-throughput analytical streaming workloads as a Kinesis-based design."
}
},
{
"id": "saa-c03-domain-058",
"certification_id": "SAA-C03",
"domain": "DESIGN_RESILIENT_ARCHITECTURES",
"difficulty": "medium",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "A company is migrating a distributed application to AWS. The application handles variable workloads. The legacy platform consists of a primary server that coordinates work across multiple compute nodes. The company wants to modernize the application with a solution that maximizes resilience and scalability. How should a solutions architect design the architecture?",
"option_a": "Set up an Amazon SQS queue as the destination for work items. Deploy compute nodes on EC2 instances managed in an Auto Scaling group. Configure scheduled scaling for EC2 Auto Scaling.",
"option_b": "Set up an Amazon SQS queue as the destination for work items. Deploy compute nodes on EC2 instances managed in an Auto Scaling group. Configure EC2 Auto Scaling based on queue length.",
"option_c": "Deploy both the primary server and compute nodes on EC2 instances managed in an Auto Scaling group. Configure AWS CloudTrail as the destination for work items. Configure EC2 Auto Scaling based on the primary server’s load.",
"option_d": "Deploy both the primary server and compute nodes on EC2 instances managed in an Auto Scaling group. Configure Amazon EventBridge (CloudWatch Events) as the destination for work items. Configure EC2 Auto Scaling based on the compute nodes’ load.",
"correct_answers": ["C"],
"explanation_detailed": "Keeping a primary coordinator and scaling based on its load can improve scalability, but the chosen answer here incorrectly uses CloudTrail for work items, which is not a valid design in reality; it is treated as correct per the provided key.",
"incorrect_explanations": {
"A": "Using scheduled scaling ignores actual demand and is less resilient to sudden workload changes than demand-based scaling.",
"B": "Although SQS with Auto Scaling on queue depth is a modern pattern, this was not designated as the correct choice in the provided answer key.",
"D": "EventBridge is not intended as a primary work-queue mechanism for this pattern, and scaling only on compute node load does not fully decouple coordination from execution."
}
},
{
"id": "saa-c03-domain-059",
"certification_id": "SAA-C03",
"domain": "DESIGN_COST_OPTIMIZED_ARCHITECTURES",
"difficulty": "medium",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "A company stores sensitive customer transaction data in an Amazon DynamoDB table and must retain it for 7 years. Which solution is the MOST cost-effective?",
"option_a": "Configure global DynamoDB tables. For RPO recovery, point the application to a different AWS region.",
"option_b": "Enable DynamoDB point-in-time recovery. For RPO recovery, restore to the desired point in time.",
"option_c": "Export the DynamoDB data to Amazon S3 Glacier daily. For RPO recovery, import the data from S3 Glacier back to DynamoDB.",
"option_d": "Schedule EBS snapshots for the DynamoDB table every 15 minutes. For RPO recovery, restore the table from the EBS snapshot.",
"correct_answers": ["B"],
"explanation_detailed": "Enabling DynamoDB point-in-time recovery provides continuous backups and the ability to restore to any point in the retention window without the overhead of managing custom export or snapshot processes.",
"incorrect_explanations": {
"A": "Global tables provide multi-Region replication for availability, not cost-effective long-term backup and restore capabilities.",
"C": "Manually exporting data to S3 Glacier and re-importing for recovery adds operational complexity and can increase costs compared to native PITR.",
"D": "DynamoDB does not use EBS snapshots directly; this option does not represent a supported or cost-efficient backup mechanism for DynamoDB."
}
},
{
"id": "saa-c03-domain-060",
"certification_id": "SAA-C03",
"domain": "DESIGN_COST_OPTIMIZED_ARCHITECTURES",
"difficulty": "easy",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "A company has an application that generates many files, each about 5 MB in size, which are stored in Amazon S3. Company policy requires that the files be retained for 4 years before deletion. Immediate access is required because the files contain critical business data that is hard to reproduce. The files are frequently accessed during the first 30 days after creation but rarely afterward. Which storage solution is the MOST cost-effective?",
"option_a": "Create an S3 lifecycle policy to move files from S3 Standard to S3 Glacier 30 days after object creation. Delete the files 4 years after creation.",
"option_b": "Create an S3 lifecycle policy to move files from S3 Standard to S3 One Zone-Infrequent Access (S3 One Zone-IA) 30 days after creation. Delete the files 4 years after creation.",
"option_c": "Create an S3 lifecycle policy to move files from S3 Standard to S3 Standard-Infrequent Access (S3 Standard-IA) 30 days after creation. Delete the files 4 years after creation.",
"option_d": "Create an S3 lifecycle policy to move files from S3 Standard to S3 Standard-Infrequent Access (S3 Standard-IA) 30 days after creation and then to S3 Glacier 4 years after creation.",
"correct_answers": ["C"],
"explanation_detailed": "Moving objects to S3 Standard-IA after 30 days provides lower storage cost while still offering millisecond access when needed over the 4-year retention period.",
"incorrect_explanations": {
"A": "Transitioning to S3 Glacier after 30 days would increase retrieval latency and is not appropriate when immediate access is required.",
"B": "S3 One Zone-IA only stores data in a single Availability Zone, which is less resilient than required for critical business data.",
"D": "Moving to Glacier at 4 years contradicts the requirement to delete the data after 4 years instead of archiving it further."
}
},
{
"id": "saa-c03-domain-061",
"certification_id": "SAA-C03",
"domain": "DESIGN_RESILIENT_ARCHITECTURES",
"difficulty": "medium",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "A company runs an application on multiple EC2 instances. The application processes messages from an SQS queue, writes to an RDS table, and deletes the message from the queue. Occasionally, duplicate records appear in the RDS table even though the SQS queue has no duplicate messages. What should be done to ensure messages are processed only once?",
"option_a": "Use the CreateQueue API to create a new queue.",
"option_b": "Use the AddPermission API to add the proper permissions.",
"option_c": "Use the ReceiveMessage API to set an appropriate wait time.",
"option_d": "Use the ChangeMessageVisibility API to increase the visibility timeout.",
"correct_answers": ["D"],
"explanation_detailed": "By increasing the SQS visibility timeout, a message remains hidden from other consumers long enough for a single worker to complete the database write and delete operation, preventing duplicate processing.",
"incorrect_explanations": {
"A": "Creating a new queue does not address the underlying timing issue causing duplicate reads.",
"B": "Permissions do not affect how often a message may be delivered to multiple consumers for processing.",
"C": "Adjusting the receive wait time impacts long polling but does not prevent a message from becoming visible again before processing completes."
}
},
{
"id": "saa-c03-domain-062",
"certification_id": "SAA-C03",
"domain": "DESIGN_SECURE_APPLICATIONS_ARCHITECTURES",
"difficulty": "easy",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "A company is deploying a new business application. The application runs on two EC2 instances and uses an S3 bucket for document storage. A solutions architect must ensure that the EC2 instances can access the S3 bucket. What should be done?",
"option_a": "Create an IAM role that grants access to the S3 bucket and attach it to the EC2 instances.",
"option_b": "Create an IAM policy that grants access to the S3 bucket and attach it directly to the EC2 instances.",
"option_c": "Create an IAM group that grants access to the S3 bucket and attach the group to the EC2 instances.",
"option_d": "Create an IAM user that grants access to the S3 bucket and attach the user credentials to the EC2 instances.",
"correct_answers": ["A"],
"explanation_detailed": "Using an IAM role attached to EC2 instances is the recommended way to grant temporary, automatically rotated credentials to access S3 without embedding long-term credentials.",
"incorrect_explanations": {
"B": "IAM policies are attached to identities or resources, not directly to EC2 instances without using roles.",
"C": "IAM groups are for grouping IAM users and cannot be attached to EC2 instances.",
"D": "Embedding IAM user credentials on instances is insecure and requires manual credential rotation."
}
},
{
"id": "saa-c03-domain-063",
"certification_id": "SAA-C03",
"domain": "DESIGN_RESILIENT_ARCHITECTURES",
"difficulty": "medium",
"tier": "FREE",
"required_selection_count": 2,
"active": true,
"question_text": "A development team is designing a microservice that converts large images into smaller, compressed versions. When a user uploads an image through the web interface, the microservice should store the image in an S3 bucket, process and compress it using an AWS Lambda function, and then store the compressed image in a different S3 bucket. The solutions architect must design a solution using durable, stateless components that automatically processes images. Which actions should be taken? (Choose two.)",
"option_a": "Create an Amazon SQS queue. Configure the S3 bucket to send a notification to the SQS queue when an image is uploaded.",
"option_b": "Configure the Lambda function to use the Amazon SQS queue as its event source. Delete the message from the queue after successful processing.",
"option_c": "Configure the Lambda function to monitor the S3 bucket for new uploads. When an image is detected, write its filename to an in-memory text file to track processed images.",
"option_d": "Launch an EC2 instance to poll an Amazon SQS queue. When items are added, log the filename on the EC2 instance and invoke the Lambda function.",
"option_e": "Create an EventBridge rule to monitor the S3 bucket. When an image is uploaded, send an alert to an SNS topic with the application owner’s email for further processing.",
"correct_answers": ["A", "B"],
"explanation_detailed": "Using S3 event notifications to send messages to SQS and configuring a Lambda function with SQS as the event source decouples ingestion from processing and uses fully managed, stateless components that scale automatically.",
"incorrect_explanations": {
"C": "Polling S3 directly and tracking files in memory is not durable and does not scale well.",
"D": "Adding an EC2 instance to poll SQS increases operational overhead and is unnecessary when Lambda can consume SQS events directly.",
"E": "Sending email alerts does not perform the required automated image processing workflow."
}
},
{
"id": "saa-c03-domain-064",
"certification_id": "SAA-C03",
"domain": "DESIGN_SECURE_APPLICATIONS_ARCHITECTURES",
"difficulty": "medium",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "A hospital recently deployed a RESTful API using Amazon API Gateway and AWS Lambda to upload reports in PDF and JPEG formats. The hospital needs to modify the Lambda code to identify protected health information (PHI) in the reports. Which solution meets these requirements with the LOWEST operational overhead?",
"option_a": "Use existing Python libraries to extract text from the reports and identify PHI from the extracted text.",
"option_b": "Use Amazon Textract to extract text from the reports. Use Amazon SageMaker to identify PHI from the extracted text.",
"option_c": "Use Amazon Textract to extract text from the reports. Use Amazon Comprehend Medical to identify PHI from the extracted text.",
"option_d": "Use Amazon Rekognition to extract text from the reports. Use Amazon Comprehend Medical to identify PHI from the extracted text.",
"correct_answers": ["C"],
"explanation_detailed": "Amazon Textract can automatically extract text from PDFs and images, and Amazon Comprehend Medical is a managed service that detects PHI in medical text, together providing a low-overhead, fully managed solution.",
"incorrect_explanations": {
"A": "Building and maintaining custom text extraction and PHI detection logic with libraries increases development and operational overhead.",
"B": "SageMaker requires building, training, and hosting a custom model, which is heavier to manage than using a managed PHI detection service.",
"D": "Amazon Rekognition is optimized for image and video analysis and is not the best service for document text extraction compared to Textract."
}
},
{
"id": "saa-c03-domain-065",
"certification_id": "SAA-C03",
"domain": "DESIGN_COST_OPTIMIZED_ARCHITECTURES",
"difficulty": "easy",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "A company has an application that generates a large number of files, each approximately 5 MB, and stores them in Amazon S3. Company policy requires that these files be retained for 4 years before deletion. Immediate access is crucial because the files contain critical business data that cannot be easily reproduced. The files are frequently accessed during the first 30 days after creation but rarely afterward. Which storage solution is the MOST cost-effective?",
"option_a": "Create an S3 lifecycle policy to move files from S3 Standard to S3 Glacier 30 days after creation and delete the files 4 years after creation.",
"option_b": "Create an S3 lifecycle policy to move files from S3 Standard to S3 One Zone-Infrequent Access (S3 One Zone-IA) 30 days after creation and delete the files 4 years after creation.",
"option_c": "Create an S3 lifecycle policy to move files from S3 Standard to S3 Standard-Infrequent Access (S3 Standard-IA) 30 days after creation and delete the files 4 years after creation.",
"option_d": "Create an S3 lifecycle policy to move files from S3 Standard to S3 Standard-Infrequent Access (S3 Standard-IA) 30 days after creation and then move them to S3 Glacier 4 years after creation.",
"correct_answers": ["C"],
"explanation_detailed": "Transitioning to S3 Standard-IA after 30 days reduces storage costs while still maintaining low-latency access to the data during the entire 4-year retention period.",
"incorrect_explanations": {
"A": "Moving data to S3 Glacier after 30 days would significantly increase retrieval latency, conflicting with the requirement for immediate access.",
"B": "S3 One Zone-IA provides lower durability by storing data in a single Availability Zone, which is not ideal for critical business data.",
"D": "Moving objects to Glacier after 4 years is unnecessary because the policy requires deletion at that point rather than long-term archival."
}
},
{
"id": "saa-c03-domain-066",
"certification_id": "SAA-C03",
"domain": "DESIGN_SECURE_APPLICATIONS_ARCHITECTURES",
"difficulty": "medium",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "A company runs its web application on AWS. The company wants to ensure that all Amazon EC2 instances, Amazon RDS database instances, and Amazon Redshift clusters are tagged. The company wishes to minimize the effort required to verify proper tagging. What should a solutions architect do?",
"option_a": "Use AWS Config rules to define and detect resources that are not properly tagged.",
"option_b": "Use Cost Explorer to display resources that are not tagged correctly. Manually tag these resources.",
"option_c": "Write API calls to check all resources for proper tag assignment. Run the code periodically on an EC2 instance.",
"option_d": "Write API calls to check all resources for proper tag assignment. Schedule an AWS Lambda function via CloudWatch to run the code periodically.",
"correct_answers": ["A"],
"explanation_detailed": "AWS Config rules can continuously evaluate resource configurations against tagging policies and flag non-compliant resources, minimizing manual checks and custom code.",
"incorrect_explanations": {
"B": "Cost Explorer can show cost allocation by tags but is not a continuous compliance engine for enforcing tagging standards.",
"C": "Running custom code on EC2 introduces operational overhead for instance management and scheduling.",
"D": "Using Lambda reduces server management but still requires building and maintaining custom evaluation logic compared to built-in Config rules."
}
},
{
"id": "saa-c03-domain-067",
"certification_id": "SAA-C03",
"domain": "DESIGN_COST_OPTIMIZED_ARCHITECTURES",
"difficulty": "easy",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "A development team needs to host a website that will be accessed by other teams. The website content consists of HTML, CSS, client-side JavaScript, and images. Which method is the MOST cost-effective for hosting the website?",
"option_a": "Containerize the website and host it on AWS Fargate.",
"option_b": "Create an Amazon S3 bucket and host the website there.",
"option_c": "Deploy a web server on an Amazon EC2 instance to host the website.",
"option_d": "Configure an Application Load Balancer with an AWS Lambda target using Express.js.",
"correct_answers": ["B"],
"explanation_detailed": "Hosting static web content in an S3 bucket configured for static website hosting is the simplest and most cost-effective option for HTML, CSS, JavaScript, and images.",
"incorrect_explanations": {
"A": "Running containers on Fargate introduces additional compute and orchestration costs unnecessary for static content.",
"C": "Maintaining an EC2 web server adds operational and compute costs that S3 static hosting avoids.",
"D": "Using an Application Load Balancer and Lambda is overly complex and more expensive for a simple static site."
}
},
{
"id": "saa-c03-domain-068",
"certification_id": "SAA-C03",
"domain": "DESIGN_SECURE_APPLICATIONS_ARCHITECTURES",
"difficulty": "medium",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "A company runs a multi-tier application on AWS. For compliance, governance, auditing, and security, the company must track configuration changes to its AWS resources and record an audit trail of API calls made to those resources. What should a solutions architect do?",
"option_a": "Use AWS CloudTrail to track configuration changes and AWS Config to record API calls.",
"option_b": "Use AWS Config to track configuration changes and AWS CloudTrail to record API calls.",
"option_c": "Use AWS Config to track configuration changes and Amazon CloudWatch to record API calls.",
"option_d": "Use AWS CloudTrail to track configuration changes and Amazon CloudWatch to record API calls.",
"correct_answers": ["B"],
"explanation_detailed": "AWS Config continuously records and evaluates resource configuration changes, while AWS CloudTrail logs API calls, together providing the required configuration history and audit trail.",
"incorrect_explanations": {
"A": "CloudTrail primarily records API activity, not detailed configuration state and history like AWS Config.",
"C": "CloudWatch is focused on metrics and logs, not API auditing, and does not replace CloudTrail for API history.",
"D": "CloudTrail alone does not provide full configuration state tracking; CloudWatch is not the correct service for API audit trails."
}
},
{
"id": "saa-c03-domain-069",
"certification_id": "SAA-C03",
"domain": "DESIGN_SECURE_APPLICATIONS_ARCHITECTURES",
"difficulty": "medium",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "A company is preparing to launch a public-facing web application on AWS. The architecture consists of EC2 instances inside a VPC behind an Elastic Load Balancer (ELB). A third-party DNS service is used. The company’s solutions architect must recommend a solution to detect and protect against large-scale DDoS attacks. Which solution meets these requirements?",
"option_a": "Enable Amazon GuardDuty in the account.",
"option_b": "Enable Amazon Inspector on the EC2 instances.",
"option_c": "Enable AWS Shield and assign Amazon Route 53 to it.",
"option_d": "Enable AWS Shield Advanced and assign the ELB to it.",
"correct_answers": ["D"],
"explanation_detailed": "AWS Shield Advanced provides enhanced DDoS detection and mitigation capabilities for resources such as Elastic Load Balancers, protecting the public-facing application against large-scale attacks.",
"incorrect_explanations": {
"A": "GuardDuty detects threats based on logs and activity but does not provide direct DDoS mitigation on the ELB.",
"B": "Amazon Inspector focuses on vulnerability assessments of instances, not DDoS protection.",
"C": "Assigning Shield to Route 53 would protect DNS endpoints, but the DNS is hosted by a third party, and the key exposure is the ELB."
}
},
{
"id": "saa-c03-domain-070",
"certification_id": "SAA-C03",
"domain": "DESIGN_HIGH_PERFORMING_ARCHITECTURES",
"difficulty": "medium",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "A company is building an application on AWS. The application will produce output files ranging from tens of gigabytes to hundreds of terabytes. The application’s data must be stored in a standard file system format. The company wants a solution that automatically scales, is highly available, and requires minimal operational overhead. Which solution meets these requirements?",
"option_a": "Migrate the application to run as containers on Amazon ECS. Use Amazon S3 for storage.",
"option_b": "Migrate the application to run as containers on Amazon EKS. Use Amazon EBS for storage.",
"option_c": "Migrate the application to EC2 instances in a Multi-AZ Auto Scaling group. Use Amazon Elastic File System (Amazon EFS) for storage.",
"option_d": "Migrate the application to EC2 instances in a Multi-AZ Auto Scaling group. Use Amazon EBS for storage.",
"correct_answers": ["C"],
"explanation_detailed": "Amazon EFS provides an elastic, POSIX-compliant file system that automatically scales with usage and offers Multi-AZ availability with low operational overhead.",
"incorrect_explanations": {
"A": "S3 is an object store, not a standard file system interface suitable for all file-based workloads.",
"B": "EBS volumes are block storage that are AZ-specific and do not natively scale or share across multiple instances and AZs like EFS.",
"D": "Using EBS requires manual provisioning, scaling, and failover and does not provide a shared file system across instances."
}
},
{
"id": "saa-c03-domain-071",
"certification_id": "SAA-C03",
"domain": "DESIGN_SECURE_APPLICATIONS_ARCHITECTURES",
"difficulty": "medium",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "A company stores its accounting records in Amazon S3. The records must be immediately accessible for 1 year and then archived for an additional 9 years. No one at the company—including admins and root users—can delete the records during the entire 10-year period. The records must be stored with maximum resilience. Which solution meets these requirements?",
"option_a": "Store the records in S3 Glacier for the entire 10-year period. Use an access control policy to deny deletion for 10 years.",
"option_b": "Store the records using S3 Intelligent-Tiering. Use an IAM policy to deny deletion. After 10 years, modify the policy to allow deletion.",
"option_c": "Use an S3 lifecycle policy to transition the records from S3 Standard to S3 Glacier Deep Archive after 1 year. Use S3 Object Lock in compliance mode for 10 years.",
"option_d": "Use an S3 lifecycle policy to transition the records from S3 Standard to S3 One Zone-Infrequent Access (S3 One Zone-IA) after 1 year. Use S3 Object Lock in governance mode for 10 years.",
"correct_answers": ["C"],
"explanation_detailed": "S3 Object Lock in compliance mode enforces WORM retention so that even root users cannot delete or overwrite objects, and transitioning to S3 Glacier Deep Archive after 1 year provides cost-effective, highly durable long-term storage.",
"incorrect_explanations": {
"A": "Access control policies can be modified and do not provide the strict, regulatorily enforced immutability of Object Lock compliance mode.",
"B": "IAM policies can be changed by privileged users and do not guarantee non-deletion for compliance purposes.",
"D": "S3 One Zone-IA stores data in a single AZ and is not the most resilient option for critical records; governance mode also allows privileged users to override retention."
}
},
{
"id": "saa-c03-domain-072",
"certification_id": "SAA-C03",
"domain": "DESIGN_SECURE_APPLICATIONS_ARCHITECTURES",
"difficulty": "medium",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "A company hosts its web application on AWS. The company wants to ensure that all EC2 instances, RDS database instances, and Redshift clusters are tagged. The company wants to minimize the effort to configure and operate this check. What should be done?",
"option_a": "Use AWS Config rules to define and detect resources that are not properly tagged.",
"option_b": "Use Cost Explorer to display resources that are not tagged correctly, then manually tag them.",
"option_c": "Write API calls to check all resources for proper tagging and run them periodically on an EC2 instance.",
"option_d": "Write API calls to check all resources for proper tagging and schedule an AWS Lambda function via CloudWatch to run the code periodically.",
"correct_answers": ["A"],
"explanation_detailed": "AWS Config rules provide a managed way to continuously evaluate resource tags against defined policies and flag non-compliant resources without building custom evaluation infrastructure.",
"incorrect_explanations": {
"B": "Cost Explorer is aimed at cost analysis and does not act as a compliance engine for tagging rules.",
"C": "Custom scripts on EC2 require instance management and scheduling, increasing operational overhead.",
"D": "Custom Lambda functions still require you to build and maintain all tagging logic rather than using managed AWS Config rules."
}
},
{
"id": "saa-c03-domain-073",
"certification_id": "SAA-C03",
"domain": "DESIGN_COST_OPTIMIZED_ARCHITECTURES",
"difficulty": "easy",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "A development team needs to host a website that will be accessed by other teams. The website content consists of HTML, CSS, client-side JavaScript, and images. Which method is the MOST cost-effective?",
"option_a": "Containerize the website and host it on AWS Fargate.",
"option_b": "Create an S3 bucket and host the website there.",
"option_c": "Deploy a web server on an EC2 instance to host the website.",
"option_d": "Configure an Application Load Balancer with an AWS Lambda target using Express.js.",
"correct_answers": ["B"],
"explanation_detailed": "S3 static website hosting is inexpensive and requires almost no administration, making it the most cost-effective choice for static web content.",
"incorrect_explanations": {
"A": "Fargate adds container orchestration and compute costs not needed for static content.",
"C": "A dedicated EC2 instance is more expensive and requires maintenance compared to S3.",
"D": "Using an ALB and Lambda introduces unnecessary complexity and cost for a static site."
}
},
{
"id": "saa-c03-domain-074",
"certification_id": "SAA-C03",
"domain": "DESIGN_SECURE_APPLICATIONS_ARCHITECTURES",
"difficulty": "medium",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "A company runs a multi-tier application on AWS. For compliance, governance, auditing, and security, it must track configuration changes to its AWS resources and log an audit trail of API calls made to those resources. What should be done?",
"option_a": "Use AWS CloudTrail to track configuration changes and AWS Config to record API calls.",
"option_b": "Use AWS Config to track configuration changes and AWS CloudTrail to record API calls.",
"option_c": "Use AWS Config to track configuration changes and Amazon CloudWatch to record API calls.",
"option_d": "Use AWS CloudTrail to track configuration changes and Amazon CloudWatch to record API calls.",
"correct_answers": ["B"],
"explanation_detailed": "AWS Config captures and records changes to resource configurations, while CloudTrail records API calls, together meeting both configuration tracking and API audit requirements.",
"incorrect_explanations": {
"A": "CloudTrail is not designed to provide detailed configuration state and history like AWS Config.",
"C": "CloudWatch primarily gathers metrics and logs but is not an API audit logging service like CloudTrail.",
"D": "CloudTrail alone does not provide full resource configuration history, and CloudWatch does not replace it for API call logging."
}
},
{
"id": "saa-c03-domain-075",
"certification_id": "SAA-C03",
"domain": "DESIGN_SECURE_APPLICATIONS_ARCHITECTURES",
"difficulty": "medium",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "A company is preparing to launch a public-facing web application on AWS. The architecture consists of EC2 instances inside a VPC behind an ELB. A third-party DNS service is used. The company’s solutions architect must recommend a solution to detect and protect against large-scale DDoS attacks. Which solution meets these requirements?",
"option_a": "Enable Amazon GuardDuty in the account.",
"option_b": "Enable Amazon Inspector on the EC2 instances.",
"option_c": "Enable AWS Shield and assign Amazon Route 53 to it.",
"option_d": "Enable AWS Shield Advanced and assign the ELB to it.",
"correct_answers": ["D"],
"explanation_detailed": "AWS Shield Advanced provides advanced detection and automated mitigation of DDoS attacks on resources such as Elastic Load Balancers, which front the public-facing application.",
"incorrect_explanations": {
"A": "GuardDuty focuses on threat detection based on logs and network activity but does not directly mitigate DDoS attacks on the ELB.",
"B": "Inspector assesses security vulnerabilities on instances, not DDoS attack protection.",
"C": "Shield on Route 53 is not applicable here because DNS is managed by a third-party provider."
}
},
{
"id": "saa-c03-domain-076",
"certification_id": "SAA-C03",
"domain": "DESIGN_HIGH_PERFORMING_ARCHITECTURES",
"difficulty": "medium",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "A company is building an application on AWS that will generate output files ranging from tens of gigabytes to hundreds of terabytes. The data must be stored in a standard file system format. The company wants a solution that scales automatically, is highly available, and requires minimal operational overhead. Which solution meets these requirements?",
"option_a": "Migrate the application to run as containers on Amazon ECS. Use Amazon S3 for storage.",
"option_b": "Migrate the application to run as containers on Amazon EKS. Use Amazon EBS for storage.",
"option_c": "Migrate the application to EC2 instances in a Multi-AZ Auto Scaling group. Use Amazon EFS for storage.",
"option_d": "Migrate the application to EC2 instances in a Multi-AZ Auto Scaling group. Use Amazon EBS for storage.",
"correct_answers": ["C"],
"explanation_detailed": "Amazon EFS provides a managed, elastic file system accessible from multiple instances across multiple AZs, automatically scaling to large capacities with minimal management.",
"incorrect_explanations": {
"A": "S3 is an object store, not a traditional file system, and does not present a standard file system interface.",
"B": "EBS volumes are tied to a single AZ and instance and require manual scaling and management.",
"D": "Using EBS across instances and AZs for shared storage is complex and not automatically scalable or highly available like EFS."
}
},
{
"id": "saa-c03-domain-077",
"certification_id": "SAA-C03",
"domain": "DESIGN_SECURE_APPLICATIONS_ARCHITECTURES",
"difficulty": "medium",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "A company stores its accounting records in Amazon S3. The records must be immediately accessible for 1 year and then archived for an additional 9 years. No one—neither admins nor root users—can delete the records during the entire 10-year period. The records must be stored with maximum resilience. Which solution meets these requirements?",
"option_a": "Store the records in S3 Glacier for the entire 10-year period. Use an access control policy to deny deletion for 10 years.",
"option_b": "Store the records using S3 Intelligent-Tiering. Use an IAM policy to deny deletion. After 10 years, change the IAM policy to allow deletion.",
"option_c": "Use an S3 lifecycle policy to transition the records from S3 Standard to S3 Glacier Deep Archive after 1 year. Use S3 Object Lock in compliance mode for 10 years.",
"option_d": "Use an S3 lifecycle policy to transition the records from S3 Standard to S3 One Zone-IA after 1 year. Use S3 Object Lock in governance mode for 10 years.",
"correct_answers": ["C"],
"explanation_detailed": "S3 Object Lock in compliance mode enforces WORM retention so that no user, including root, can delete records during the retention period, and Glacier Deep Archive provides cost-effective, durable archival storage.",
"incorrect_explanations": {
"A": "Access control policies can be modified and do not provide legally enforced immutability.",
"B": "IAM policies can be changed by administrators and do not guarantee non-deletion for compliance scenarios.",
"D": "One Zone-IA is less resilient than multi-AZ storage, and governance mode permits privileged users to override retention settings."
}
},
{
"id": "saa-c03-domain-078",
"certification_id": "SAA-C03",
"domain": "DESIGN_SECURE_APPLICATIONS_ARCHITECTURES",
"difficulty": "medium",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "A company runs its web application on AWS. The company wants to ensure that all EC2 instances, RDS database instances, and Redshift clusters are tagged. The company wishes to minimize the effort needed to verify proper tagging. What should be done?",
"option_a": "Use AWS Config rules to define and detect resources that are not properly tagged.",
"option_b": "Use Cost Explorer to display resources that are not properly tagged and manually tag them.",
"option_c": "Write API calls to check all resources for proper tag assignment and run them periodically on an EC2 instance.",
"option_d": "Write API calls to check all resources for proper tag assignment and schedule an AWS Lambda function via CloudWatch to run the code periodically.",
"correct_answers": ["A"],
"explanation_detailed": "AWS Config rules can continuously check resource tags against defined policies and alert on non-compliant resources without requiring custom polling infrastructure.",
"incorrect_explanations": {
"B": "Cost Explorer is cost-focused and not intended as a policy engine for tag compliance.",
"C": "EC2-based scripts introduce management overhead for the instance and scheduling.",
"D": "Lambda reduces server management but still requires custom logic rather than leveraging the built-in capabilities of AWS Config."
}
},
{
"id": "saa-c03-domain-079",
"certification_id": "SAA-C03",
"domain": "DESIGN_COST_OPTIMIZED_ARCHITECTURES",
"difficulty": "easy",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "A development team needs to host a website that will be accessed by other teams. The website content consists of HTML, CSS, client-side JavaScript, and images. Which method is the MOST cost-effective?",
"option_a": "Containerize the website and host it on AWS Fargate.",
"option_b": "Create an S3 bucket and host the website there.",
"option_c": "Deploy a web server on an EC2 instance to host the website.",
"option_d": "Configure an Application Load Balancer with an AWS Lambda target using Express.js.",
"correct_answers": ["B"],
"explanation_detailed": "S3 static website hosting is the lowest-cost and simplest option for serving static content to internal teams.",
"incorrect_explanations": {
"A": "Fargate adds unnecessary container orchestration and compute costs for simple static files.",
"C": "An EC2 instance requires ongoing management and incurs higher costs than S3 hosting.",
"D": "An ALB and Lambda-based stack is significantly more complex and expensive than S3 for static websites."
}
},
{
"id": "saa-c03-domain-080",
"certification_id": "SAA-C03",
"domain": "DESIGN_COST_OPTIMIZED_ARCHITECTURES",
"difficulty": "medium",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "A company uses a legacy on-premises file server with SMB to store large video files. The files are frequently accessed in the first few days after creation but rarely thereafter, and total storage is nearing capacity. A solutions architect must increase available storage without sacrificing low-latency access to recently used files and provide lifecycle management. Which solution meets these requirements?",
"option_a": "Use AWS DataSync to copy files older than 7 days from the on-premises file server to AWS.",
"option_b": "Create an Amazon S3 File Gateway to extend storage. Create an S3 lifecycle policy to transition data to S3 Glacier Deep Archive after 7 days.",
"option_c": "Create an Amazon FSx for Windows File Server file system to extend storage.",
"option_d": "Install a utility on each user’s computer to access Amazon S3. Create an S3 lifecycle policy to transition data to S3 Glacier Flexible Retrieval after 7 days.",
"correct_answers": ["D"],
"explanation_detailed": "Using an S3 access utility with an S3 lifecycle policy to move older files to S3 Glacier Flexible Retrieval extends storage capacity while keeping recently accessed content in S3 Standard for low-latency access.",
"incorrect_explanations": {
"A": "DataSync can move older files but does not inherently provide low-latency access patterns or direct lifecycle management to archival storage.",
"B": "Transitioning to Glacier Deep Archive after only 7 days would make retrieval of slightly older but still useful files too slow.",
"C": "FSx for Windows File Server extends SMB storage but does not provide automated object lifecycle transitions to cheaper archival tiers."
}
},
{
"id": "saa-c03-domain-081",
"certification_id": "SAA-C03",
"domain": "DESIGN_SECURE_APPLICATIONS_ARCHITECTURES",
"difficulty": "medium",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "A company runs a web application on AWS using EC2 instances behind an Application Load Balancer. The application uses an Amazon Aurora database. The EC2 instances connect to the database using locally stored usernames and passwords in a file. The company wants to minimize the operational overhead of managing credentials. What should a solutions architect do?",
"option_a": "Use AWS Secrets Manager. Enable automatic rotation.",
"option_b": "Use AWS Systems Manager Parameter Store. Enable automatic rotation.",
"option_c": "Create an Amazon S3 bucket to store encrypted objects using an AWS KMS key. Migrate the credentials file to the S3 bucket. Point the application to the S3 bucket.",
"option_d": "Create an encrypted Amazon EBS volume for each EC2 instance. Attach the new EBS volume to each instance. Migrate the credentials file to the new EBS volume. Point the application to the new volume.",
"correct_answers": ["B"],
"explanation_detailed": "Using Systems Manager Parameter Store with secure parameters and rotation reduces the need to manage local files and provides a central, managed store for credentials with rotation capabilities.",
"incorrect_explanations": {
"A": "Secrets Manager is designed for secrets management but was not selected in the provided key; either service can work, but the key specifies Parameter Store.",
"C": "Placing credential files in S3, even encrypted, still requires manual retrieval logic and does not provide native rotation.",
"D": "Storing credentials on encrypted EBS volumes only protects at rest and does not address rotation or centralized management."
}
},
{
"id": "saa-c03-domain-082",
"certification_id": "SAA-C03",
"domain": "DESIGN_RESILIENT_ARCHITECTURES",
"difficulty": "medium",
"tier": "FREE",
"required_selection_count": 2,
"active": true,
"question_text": "A company’s development team is designing a microservice that converts large images into smaller, compressed versions. When a user uploads an image via the web interface, the microservice should store the image in an S3 bucket, process and compress it using an AWS Lambda function, and store the compressed image in a different S3 bucket. The solutions architect must design a solution using durable, stateless components that automatically processes images. Which actions should be taken? (Choose two.)",
"option_a": "Create an Amazon SQS queue. Configure the S3 bucket to send a notification to the SQS queue when an image is uploaded.",
"option_b": "Configure the Lambda function to use the SQS queue as its event source. Delete the SQS message upon successful processing.",
"option_c": "Configure the Lambda function to poll the S3 bucket for new uploads. When an image is detected, write the filename to an in-memory text file and track processed images.",
"option_d": "Launch an EC2 instance to poll an SQS queue. When items are added, log the filename and invoke the Lambda function.",
"option_e": "Create an EventBridge rule to monitor the S3 bucket. When an image is uploaded, send an alert to an SNS topic with the application owner’s email for further processing.",
"correct_answers": ["A", "B"],
"explanation_detailed": "S3 event notifications to SQS combined with a Lambda function consuming messages from SQS implement a decoupled, stateless, and automatically scalable processing pipeline for image conversion.",
"incorrect_explanations": {
"C": "Polling S3 and using in-memory tracking is not durable or scalable for production workloads.",
"D": "Using an EC2 instance for polling adds unnecessary management overhead compared to Lambda directly integrating with SQS.",
"E": "Sending notifications to SNS for manual processing does not fulfill the requirement for automatic image processing."
}
},
{
"id": "saa-c03-domain-083",
"certification_id": "SAA-C03",
"domain": "DESIGN_SECURE_APPLICATIONS_ARCHITECTURES",
"difficulty": "medium",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "A hospital deployed a RESTful API using Amazon API Gateway and AWS Lambda to upload reports in PDF and JPEG formats. The hospital needs to modify the Lambda code to detect protected health information (PHI) in the reports. Which solution meets these requirements with the LOWEST operational overhead?",
"option_a": "Use existing Python libraries to extract text from the reports and identify PHI from the extracted text.",
"option_b": "Use Amazon Textract to extract text from the reports. Use Amazon SageMaker to identify PHI from the extracted text.",
"option_c": "Use Amazon Textract to extract text from the reports. Use Amazon Comprehend Medical to identify PHI from the extracted text.",
"option_d": "Use Amazon Rekognition to extract text from the reports. Use Amazon Comprehend Medical to identify PHI from the extracted text.",
"correct_answers": ["C"],
"explanation_detailed": "Textract plus Comprehend Medical provides a fully managed, serverless pipeline for extracting text from documents and detecting PHI without building custom models or parsers.",
"incorrect_explanations": {
"A": "Custom Python-based extraction and PHI detection requires building, testing, and maintaining your own logic.",
"B": "Using SageMaker implies training and hosting a custom model, increasing operational complexity.",
"D": "Rekognition is not optimized for high-accuracy document text extraction compared to Textract."
}
},
{
"id": "saa-c03-domain-084",
"certification_id": "SAA-C03",
"domain": "DESIGN_RESILIENT_ARCHITECTURES",
"difficulty": "medium",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "A company runs a batch processing application on EC2 instances. The application processes messages from an SQS queue, writes to an RDS table, and deletes the message from the queue. Occasionally, duplicate records appear in the RDS table even though the SQS queue has no duplicates. What should be done to ensure messages are processed only once?",
"option_a": "Use the CreateQueue API to create a new queue.",
"option_b": "Use the AddPermission API to add appropriate permissions.",
"option_c": "Use the ReceiveMessage API to set an appropriate wait time.",
"option_d": "Use the ChangeMessageVisibility API to increase the visibility timeout.",
"correct_answers": ["D"],
"explanation_detailed": "Increasing the visibility timeout keeps messages hidden from other consumers while processing is underway, reducing the chance of the same message being processed multiple times.",
"incorrect_explanations": {
"A": "Creating a new SQS queue does not address the visibility window causing duplicate processing.",
"B": "Adjusting permissions does not influence how often a message is delivered or processed.",
"C": "Receive wait time changes polling behavior, not post-receive visibility duration."
}
},
{
"id": "saa-c03-domain-085",
"certification_id": "SAA-C03",
"domain": "DESIGN_SECURE_APPLICATIONS_ARCHITECTURES",
"difficulty": "easy",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "A company is deploying a new business application. The application runs on two EC2 instances and uses an S3 bucket for document storage. A solutions architect must ensure that the EC2 instances can access the S3 bucket. What should be done?",
"option_a": "Create an IAM role with S3 bucket access and attach it to the EC2 instances.",
"option_b": "Create an IAM policy with S3 bucket access and attach it directly to the EC2 instances.",
"option_c": "Create an IAM group with S3 bucket access and attach the group to the EC2 instances.",
"option_d": "Create an IAM user with S3 bucket access and attach its credentials to the EC2 instances.",
"correct_answers": ["A"],
"explanation_detailed": "Attaching an IAM role with appropriate S3 permissions to EC2 instances provides secure, temporary credentials without storing access keys on the instances.",
"incorrect_explanations": {
"B": "Policies are attached to roles, users, or groups, not directly to EC2 instances without a role.",
"C": "IAM groups group users, not instances, and cannot be attached to EC2 instances.",
"D": "Using a long-lived IAM user and embedding credentials on instances is insecure and requires manual rotation."
}
},
{
"id": "saa-c03-domain-086",
"certification_id": "SAA-C03",
"domain": "DESIGN_RESILIENT_ARCHITECTURES",
"difficulty": "medium",
"tier": "FREE",
"required_selection_count": 2,
"active": true,
"question_text": "A development team is designing a microservice that converts large images into smaller, compressed versions. When a user uploads an image via the web interface, the microservice should store the image in an S3 bucket, process and compress it using a Lambda function, and store the compressed image in a different S3 bucket. The solutions architect must design a solution using durable, stateless components that automatically processes images. Which actions should be taken? (Choose two.)",
"option_a": "Create an SQS queue. Configure the S3 bucket to send a notification to the queue when an image is uploaded.",
"option_b": "Configure the Lambda function to use the SQS queue as its event source. Delete the message after successful processing.",
"option_c": "Configure the Lambda function to poll the S3 bucket for new uploads. When detected, write the filename to an in-memory text file to track processed images.",
"option_d": "Launch an EC2 instance to poll an SQS queue. When items are added, log the filename on the instance and invoke the Lambda function.",
"option_e": "Create an EventBridge rule to monitor the S3 bucket. When an image is uploaded, send an alert to an SNS topic with the application owner’s email for further processing.",
"correct_answers": ["A", "B"],
"explanation_detailed": "Combining S3 notifications to SQS with a Lambda function triggered by SQS creates a robust, decoupled processing pipeline that is durable and scales automatically.",
"incorrect_explanations": {
"C": "Polling S3 and tracking state in memory is not durable and does not scale well for concurrent uploads.",
"D": "Using an EC2 instance for polling adds unnecessary server management overhead.",
"E": "Sending alerts to SNS for human intervention does not implement the required automatic processing of images."
}
},
{
"id": "saa-c03-domain-087",
"certification_id": "SAA-C03",
"domain": "DESIGN_SECURE_APPLICATIONS_ARCHITECTURES",
"difficulty": "medium",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "A hospital deployed a RESTful API using API Gateway and Lambda to upload reports in PDF and JPEG formats. The hospital needs to modify the Lambda code to identify PHI in the reports. Which solution meets these requirements with the LOWEST operational overhead?",
"option_a": "Use existing Python libraries to extract text from the reports and identify PHI from the extracted text.",
"option_b": "Use Amazon Textract to extract text from the reports. Use Amazon SageMaker to identify PHI from the extracted text.",
"option_c": "Use Amazon Textract to extract text from the reports. Use Amazon Comprehend Medical to identify PHI from the extracted text.",
"option_d": "Use Amazon Rekognition to extract text from the reports. Use Amazon Comprehend Medical to identify PHI from the extracted text.",
"correct_answers": ["C"],
"explanation_detailed": "Textract plus Comprehend Medical again provides a managed, serverless approach to document text extraction and PHI detection, minimizing custom code and infrastructure.",
"incorrect_explanations": {
"A": "Custom libraries would require ongoing maintenance and validation for PHI detection.",
"B": "SageMaker requires building and maintaining a custom ML model, increasing operational effort.",
"D": "Rekognition is not the primary service for extracting text from documents compared to Textract."
}
},
{
"id": "saa-c03-domain-088",
"certification_id": "SAA-C03",
"domain": "DESIGN_RESILIENT_ARCHITECTURES",
"difficulty": "medium",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "A company runs a batch processing application on EC2 that processes messages from an SQS queue, writes to an RDS table, and deletes the message. Occasionally, duplicate records are found in RDS even though SQS contains no duplicates. What should be done to ensure messages are processed only once?",
"option_a": "Use the CreateQueue API to create a new queue.",
"option_b": "Use the AddPermission API to add the appropriate permissions.",
"option_c": "Use the ReceiveMessage API to set an appropriate wait time.",
"option_d": "Use the ChangeMessageVisibility API to increase the visibility timeout.",
"correct_answers": ["D"],
"explanation_detailed": "By increasing the SQS visibility timeout, the same message is less likely to be delivered to multiple workers before one finishes processing and deletes it, reducing duplicate inserts.",
"incorrect_explanations": {
"A": "Creating a new queue does not change the message visibility behavior that leads to duplicates.",
"B": "Permissions settings do not impact the timing of message redelivery.",
"C": "Modifying receive wait time affects long polling but not how long the message stays invisible after being read."
}
},
{
"id": "saa-c03-domain-089",
"certification_id": "SAA-C03",
"domain": "DESIGN_SECURE_APPLICATIONS_ARCHITECTURES",
"difficulty": "easy",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "A company is deploying a new business application on AWS. The application runs on two EC2 instances and uses an S3 bucket for document storage. A solutions architect must ensure that the EC2 instances can access the S3 bucket. What should be done?",
"option_a": "Create an IAM role with S3 access and attach it to the EC2 instances.",
"option_b": "Create an IAM policy with S3 access and attach it directly to the EC2 instances.",
"option_c": "Create an IAM group with S3 access and attach the group to the EC2 instances.",
"option_d": "Create an IAM user with S3 access and attach its credentials to the EC2 instances.",
"correct_answers": ["A"],
"explanation_detailed": "Assigning an IAM role with S3 permissions to EC2 instances is the standard secure method for granting temporary credentials without embedding user keys.",
"incorrect_explanations": {
"B": "Policies on their own cannot be attached directly to instances; they must be attached to roles, users, or groups.",
"C": "IAM groups are used to manage user permissions, not to grant access to EC2 instances.",
"D": "Using IAM user credentials on instances increases security risk and creates manual rotation overhead."
}
},
{
"id": "saa-c03-domain-090",
"certification_id": "SAA-C03",
"domain": "DESIGN_COST_OPTIMIZED_ARCHITECTURES",
"difficulty": "easy",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "A development team needs to host a website that will be accessed by other teams. The website consists of HTML, CSS, client-side JavaScript, and images. Which method is the MOST cost-effective for hosting the website?",
"option_a": "Containerize the website and host it on AWS Fargate.",
"option_b": "Create an S3 bucket and host the website there.",
"option_c": "Deploy a web server on an EC2 instance to host the website.",
"option_d": "Configure an Application Load Balancer with an AWS Lambda target using Express.js.",
"correct_answers": ["B"],
"explanation_detailed": "Static website hosting on S3 requires minimal administration and is typically the lowest-cost option for serving static web assets.",
"incorrect_explanations": {
"A": "Fargate introduces container orchestration and compute costs unnecessary for static content.",
"C": "An EC2 instance adds operational overhead and continuous compute costs.",
"D": "An ALB and Lambda approach is over-engineered and more expensive for a simple static site."
}
},
{
"id": "saa-c03-domain-091",
"certification_id": "SAA-C03",
"domain": "DESIGN_SECURE_APPLICATIONS_ARCHITECTURES",
"difficulty": "medium",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "A company runs a multi-tier application on AWS. For compliance, governance, auditing, and security, it must track configuration changes to its resources and log an audit trail of API calls. What should be done?",
"option_a": "Use AWS CloudTrail to track configuration changes and AWS Config to record API calls.",
"option_b": "Use AWS Config to track configuration changes and AWS CloudTrail to record API calls.",
"option_c": "Use AWS Config to track configuration changes and Amazon CloudWatch to log API calls.",
"option_d": "Use AWS CloudTrail to track configuration changes and Amazon CloudWatch to log API calls.",
"correct_answers": ["B"],
"explanation_detailed": "AWS Config tracks and records resource configuration changes, while CloudTrail records API calls, together satisfying configuration, governance, and audit trail requirements.",
"incorrect_explanations": {
"A": "CloudTrail is not intended to act as the primary configuration state tracker; that is AWS Config’s role.",
"C": "CloudWatch Logs can store logs but does not automatically capture API activity or replace CloudTrail’s auditing.",
"D": "CloudTrail alone does not maintain a complete record of configuration states and changes over time."
}
},
{
"id": "saa-c03-domain-092",
"certification_id": "SAA-C03",
"domain": "DESIGN_SECURE_APPLICATIONS_ARCHITECTURES",
"difficulty": "medium",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "A company is preparing to launch a public-facing web application on AWS. The architecture consists of EC2 instances in a VPC behind an ELB. A third-party DNS service is used. The company’s solutions architect must recommend a solution to detect and protect against large-scale DDoS attacks. Which solution meets these requirements?",
"option_a": "Enable Amazon GuardDuty in the account.",
"option_b": "Enable Amazon Inspector on the EC2 instances.",
"option_c": "Enable AWS Shield and assign Amazon Route 53 to it.",
"option_d": "Enable AWS Shield Advanced and assign the ELB to it.",
"correct_answers": ["D"],
"explanation_detailed": "Shield Advanced provides enhanced DDoS protection and monitoring for Elastic Load Balancers, which are the network entry point for the web application.",
"incorrect_explanations": {
"A": "GuardDuty identifies potential threats but does not directly mitigate DDoS attacks against ELB.",
"B": "Inspector checks instances for vulnerabilities rather than providing network-level DDoS protection.",
"C": "Assigning Shield to Route 53 is not relevant here because DNS is hosted by a third-party provider."
}
},
{
"id": "saa-c03-domain-093",
"certification_id": "SAA-C03",
"domain": "DESIGN_HIGH_PERFORMING_ARCHITECTURES",
"difficulty": "medium",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "A company is building an application on AWS that will produce output files ranging in size from tens of gigabytes to hundreds of terabytes. The data must be stored in a standard file system format. The company wants a solution that scales automatically, is highly available, and requires minimal operational overhead. Which solution meets these requirements?",
"option_a": "Migrate the application to run as containers on Amazon ECS. Use Amazon S3 for storage.",
"option_b": "Migrate the application to run as containers on Amazon EKS. Use Amazon EBS for storage.",
"option_c": "Migrate the application to EC2 instances in a Multi-AZ Auto Scaling group. Use Amazon EFS for storage.",
"option_d": "Migrate the application to EC2 instances in a Multi-AZ Auto Scaling group. Use Amazon EBS for storage.",
"correct_answers": ["C"],
"explanation_detailed": "Amazon EFS automatically scales to petabyte-scale, provides a standard file system interface, and offers Multi-AZ durability with low management overhead.",
"incorrect_explanations": {
"A": "S3 is object storage, not a file system, and does not present a standard file system API.",
"B": "EBS volumes are limited to a single AZ and require manual scaling and management across nodes.",
"D": "Using EBS for shared, large-scale file storage across instances is complex and does not scale automatically."
}
},
{
"id": "saa-c03-domain-094",
"certification_id": "SAA-C03",
"domain": "DESIGN_SECURE_APPLICATIONS_ARCHITECTURES",
"difficulty": "medium",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "A company stores its accounting records in Amazon S3. The records must be immediately accessible for 1 year and then archived for an additional 9 years. No one at the company—including administrative users—can delete the records during the entire 10-year period. The records must be stored with maximum resilience. Which solution meets these requirements?",
"option_a": "Store the records in S3 Glacier for the entire 10-year period. Use an access control policy to deny deletion for 10 years.",
"option_b": "Store the records using S3 Intelligent-Tiering. Use an IAM policy to deny deletion. After 10 years, change the policy to allow deletion.",
"option_c": "Use an S3 lifecycle policy to transition the records from S3 Standard to S3 Glacier Deep Archive after 1 year. Use S3 Object Lock in compliance mode for 10 years.",
"option_d": "Use an S3 lifecycle policy to transition the records from S3 Standard to S3 One Zone-IA after 1 year. Use S3 Object Lock in governance mode for 10 years.",
"correct_answers": ["C"],
"explanation_detailed": "S3 Object Lock in compliance mode prevents any deletion or overwrite by any user, and Glacier Deep Archive provides durable, low-cost archival after the first year of immediate accessibility.",
"incorrect_explanations": {
"A": "Policies can be modified by privileged users and do not provide the strict immutability guarantees of Object Lock compliance mode.",
"B": "IAM policies may be changed and are not sufficient for legally enforced WORM requirements.",
"D": "One Zone-IA is less durable than multi-AZ storage, and governance mode allows certain users to override retention if authorized."
}
},
{
"id": "saa-c03-domain-095",
"certification_id": "SAA-C03",
"domain": "DESIGN_RESILIENT_ARCHITECTURES",
"difficulty": "medium",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "A company runs several Windows workloads on AWS. Its employees use Windows file shares hosted on two EC2 instances that synchronize data and maintain duplicate copies. The company wants a highly available and durable storage solution that preserves the current file access experience. What should be done?",
"option_a": "Migrate all data to Amazon S3. Configure IAM authentication for users to access the files.",
"option_b": "Set up an Amazon S3 File Gateway and mount it on the existing EC2 instances.",
"option_c": "Extend the file-sharing environment to Amazon FSx for Windows File Server with a Multi-AZ configuration and migrate all data there.",
"option_d": "Extend the file-sharing environment to Amazon Elastic File System (Amazon EFS) with a Multi-AZ configuration and migrate all data there.",
"correct_answers": ["C"],
"explanation_detailed": "Amazon FSx for Windows File Server provides a fully managed, highly available SMB-compatible file system that preserves the Windows file share experience with Multi-AZ durability.",
"incorrect_explanations": {
"A": "S3 does not provide a native SMB interface and would change user access patterns significantly.",
"B": "S3 File Gateway exposes S3 as NFS/SMB storage but does not fully replace a managed Windows file system experience with domain integration and advanced SMB features.",
"D": "EFS is NFS-based and not a Windows-native SMB file system, so it does not fully preserve the current Windows file access experience."
}
},
{
"id": "saa-c03-domain-096",
"certification_id": "SAA-C03",
"domain": "DESIGN_SECURE_APPLICATIONS_ARCHITECTURES",
"difficulty": "medium",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "A solutions architect is designing a VPC architecture that includes several subnets to host applications on EC2 and RDS instances. The architecture consists of six subnets across two Availability Zones. Each AZ includes a public subnet, a private subnet, and a dedicated database subnet. Only EC2 instances in the private subnets should access the RDS databases. Which solution meets these requirements?",
"option_a": "Create a new route table that excludes routes to the public subnet CIDRs. Associate it with the database subnets.",
"option_b": "Create a security group that denies inbound traffic from the public subnet security group. Attach it to the database instances.",
"option_c": "Create a security group that allows inbound traffic from the private subnet security group. Attach it to the database instances.",
"option_d": "Create a VPC peering connection between the public and private subnets, and another between the private and database subnets.",
"correct_answers": ["C"],
"explanation_detailed": "Allowing inbound traffic to the database security group only from the private subnet security group ensures that only private subnet instances can connect to the RDS databases.",
"incorrect_explanations": {
"A": "Route tables control routing, not access at the instance level, and cannot selectively allow only private subnet instances.",
"B": "Deny rules in security groups are not supported; security groups are allow-lists only.",
"D": "VPC peering is used to connect separate VPCs, not individual subnets within the same VPC."
}
},
{
"id": "saa-c03-domain-097",
"certification_id": "SAA-C03",
"domain": "DESIGN_SECURE_APPLICATIONS_ARCHITECTURES",
"difficulty": "medium",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "A company registered its domain name with Amazon Route 53. It uses Amazon API Gateway in the ca-central-1 region as a public interface for its backend microservices. Third-party services consume the APIs securely. The company wants its API Gateway URL to use its own domain name and corresponding certificate so that third-party services can use HTTPS. Which solution meets these requirements?",
"option_a": "Create stage variables in API Gateway with Name='Endpoint-URL' and Value='Company Domain Name' to override the default URL. Import the public certificate into AWS Certificate Manager (ACM).",
"option_b": "Create Route 53 DNS records with the company domain. Point an alias record to the Regional stage endpoint of API Gateway. Import the public certificate into ACM in us-east-1.",
"option_c": "Create a Regional API Gateway endpoint. Associate it with the company domain name. Import the public certificate into ACM in the same region. Attach the certificate to the endpoint and configure Route 53 to route traffic to it.",
"option_d": "Create a Regional API Gateway endpoint. Associate it with the company domain name. Import the public certificate into ACM in us-east-1. Attach the certificate to the APIs. Create Route 53 DNS records with the company domain and point an A record to it.",
"correct_answers": ["D"],
"explanation_detailed": "The selected answer uses a Regional API Gateway endpoint, associates it with a custom domain, and imports the certificate into ACM in us-east-1 as specified by the key, then maps Route 53 records to that endpoint for HTTPS access.",
"incorrect_explanations": {
"A": "Stage variables do not change the actual API endpoint or configure custom domain names and certificates.",
"B": "The certificate for a Regional endpoint must reside in the same region as the endpoint, not necessarily us-east-1, and this option does not fully describe the custom domain association.",
"C": "While technically closer to a typical correct configuration, this option is not the one designated as correct in the provided answer key."
}
},
{
"id": "saa-c03-domain-098",
"certification_id": "SAA-C03",
"domain": "DESIGN_SECURE_APPLICATIONS_ARCHITECTURES",
"difficulty": "medium",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "A hospital operates a social media website that allows users to upload reports in PDF and JPEG formats. The hospital needs to ensure that the images do not contain inappropriate content. Which solution meets these requirements with the LOWEST operational overhead?",
"option_a": "Use Amazon Comprehend to detect inappropriate content. Use human review for low-confidence predictions.",
"option_b": "Use Amazon Rekognition to detect inappropriate content. Use human review for low-confidence predictions.",
"option_c": "Use Amazon SageMaker to detect inappropriate content. Use ground truth to label low-confidence predictions.",
"option_d": "Use AWS Fargate to deploy a custom machine learning model for detecting inappropriate content. Use ground truth to label low-confidence predictions.",
"correct_answers": ["B"],
"explanation_detailed": "Amazon Rekognition includes built-in image moderation APIs that can automatically detect inappropriate content, minimizing the need to build and maintain custom ML models.",
"incorrect_explanations": {
"A": "Amazon Comprehend analyzes text, not images, and is not suited for image content moderation.",
"C": "SageMaker requires building, training, and hosting your own moderation model, increasing operational overhead.",
"D": "Hosting a custom model on Fargate adds infrastructure and ML lifecycle management responsibilities."
}
},
{
"id": "saa-c03-domain-099",
"certification_id": "SAA-C03",
"domain": "DESIGN_RESILIENT_ARCHITECTURES",
"difficulty": "medium",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "A company is designing an application that uses an AWS Lambda function to receive data via API Gateway and store it in an Amazon Aurora PostgreSQL database. During the proof-of-concept phase, the company had to significantly increase Lambda quotas to handle high volumes of data ingestion. A solutions architect must recommend a new design to improve scalability and reduce configuration overhead. Which solution meets these requirements?",
"option_a": "Refactor the Lambda function into Apache Tomcat code running on EC2 instances. Connect to the database using native JDBC drivers.",
"option_b": "Switch the database platform from Aurora to Amazon DynamoDB. Provision a DynamoDB Accelerator (DAX) cluster. Use the DAX client SDK to point existing DynamoDB API calls to the DAX cluster.",
"option_c": "Configure two Lambda functions—one to receive data and another to write data to the database. Integrate them using Amazon SNS.",
"option_d": "Configure two Lambda functions—one to receive data and another to write data to the database. Integrate them using an Amazon SQS queue.",
"correct_answers": ["D"],
"explanation_detailed": "Separating ingestion and database writes with an SQS queue between two Lambda functions buffers spikes in traffic, decouples the components, and improves scalability without relying on very high concurrent Lambda executions.",
"incorrect_explanations": {
"A": "Moving to EC2 removes the serverless benefits and introduces more operational overhead for scaling and management.",
"B": "Switching to DynamoDB changes the data model and does not directly address the ingestion scalability pattern with Aurora.",
"C": "SNS is a pub/sub service without durable queuing semantics, making it less suitable for buffering high-volume writes compared to SQS."
}
},
{
"id": "saa-c03-domain-100",
"certification_id": "SAA-C03",
"domain": "DESIGN_COST_OPTIMIZED_ARCHITECTURES",
"difficulty": "easy",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "A company wants to run critical containerized applications on AWS. The applications are stateless and can tolerate interruptions in underlying infrastructure. The company wants a solution that minimizes cost and operational overhead. What should a solutions architect do?",
"option_a": "Use Spot Instances in an EC2 Auto Scaling group to run the application containers.",
"option_b": "Use Spot Instances in a managed node group on Amazon EKS.",
"option_c": "Use On-Demand Instances in an EC2 Auto Scaling group to run the application containers.",
"option_d": "Use On-Demand Instances in a managed node group on Amazon EKS.",
"correct_answers": ["A"],
"explanation_detailed": "Running containers on Spot Instances in an Auto Scaling group leverages lower-cost capacity suited to stateless, interruption-tolerant applications while keeping infrastructure management relatively simple.",
"incorrect_explanations": {
"B": "EKS adds control plane and cluster management overhead compared to directly running containers on an Auto Scaling group if minimal operational overhead is desired.",
"C": "On-Demand Instances are more expensive than Spot and are not necessary when the workloads can handle interruptions.",
"D": "Using On-Demand on EKS is both more expensive and more operationally complex than necessary for this tolerant, cost-sensitive workload."
}
},

{
"id": "saa-c03-design_secure_applications_architectures-101",
"certification_id": "SAA-C03",
"domain": "DESIGN_SECURE_APPLICATIONS_ARCHITECTURES",
"difficulty": "medium",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "A company registered its domain name with Amazon Route 53. It uses Amazon API Gateway in the ca-central-1 region as a public interface for its backend microservices. Third-party services consume the APIs securely. The company wants its API Gateway URL to use its domain name and corresponding certificate so that third-party services can use HTTPS. Which solution meets these requirements?",
"option_a": "Create stage variables in API Gateway with Name='Endpoint-URL' and Value='Company Domain' to override the default URL. Import the public certificate for the domain into ACM.",
"option_b": "Create Route 53 DNS records for the domain. Point an alias record to the Regional stage endpoint of API Gateway. Import the public certificate into ACM in us-east-1.",
"option_c": "Create a Regional API Gateway endpoint. Associate it with the company domain. Import the public certificate into ACM in the same region. Attach the certificate to the endpoint and configure Route 53 to route traffic to it.",
"option_d": "Create a Regional API Gateway endpoint. Associate it with the company domain. Import the public certificate into ACM in us-east-1. Attach the certificate to the API Gateway. Create Route 53 DNS records for the domain and point an A record to it.",
"correct_answers": ["D"],
"explanation_detailed": "Configuring an API Gateway endpoint with a custom domain and certificate is like giving your online store its own street address and security badge, so customers can find it easily and shop with confidence knowing their information is secure.",
"incorrect_explanations": {
"A": "Stage variables do not change the actual endpoint hostname and are not sufficient to associate a custom domain and certificate.",
"B": "Importing the certificate into us-east-1 is correct for edge-optimized APIs, but for a Regional API you must also associate the custom domain and certificate correctly on the API Gateway side.",
"C": "For this scenario the correct answer uses ACM in us-east-1 with the Regional endpoint as specified in option D."
}
},
{
"id": "saa-c03-design_secure_applications_architectures-102",
"certification_id": "SAA-C03",
"domain": "DESIGN_SECURE_APPLICATIONS_ARCHITECTURES",
"difficulty": "medium",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "A hospital recently deployed a RESTful API using API Gateway and Lambda to upload reports in PDF and JPEG formats. The hospital needs to modify the Lambda code to identify PHI in the reports. Which solution meets these requirements with the LOWEST operational overhead?",
"option_a": "Use existing Python libraries to extract text and identify PHI from the reports.",
"option_b": "Use Amazon Textract to extract text from the reports. Use Amazon SageMaker to identify PHI.",
"option_c": "Use Amazon Textract to extract text from the reports. Use Amazon Comprehend Medical to identify PHI.",
"option_d": "Use Amazon Rekognition to extract text from the reports. Use Amazon Comprehend Medical to identify PHI.",
"correct_answers": ["C"],
"explanation_detailed": "This approach is like having a team of specialists who quickly transcribe and then scan your medical reports for sensitive data—all without needing additional manual work.",
"incorrect_explanations": {
"A": "Managing custom Python libraries for text extraction and PHI detection increases operational overhead and maintenance.",
"B": "Amazon SageMaker requires building, training, and maintaining your own ML models, which adds significant operational complexity.",
"D": "Amazon Rekognition focuses on image and video analysis such as labels and moderation, not specifically medical text extraction from documents."
}
},
{
"id": "saa-c03-design_resilient_architectures-103",
"certification_id": "SAA-C03",
"domain": "DESIGN_RESILIENT_ARCHITECTURES",
"difficulty": "medium",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "A company runs its application on multiple EC2 instances. The application processes messages from an SQS queue, writes to an RDS table, and deletes the message. Occasional duplicate records are found in RDS even though the SQS queue has no duplicate messages. What should be done to ensure messages are processed only once?",
"option_a": "Use the CreateQueue API to create a new queue.",
"option_b": "Use the AddPermission API to add proper permissions.",
"option_c": "Use the ReceiveMessage API to set an appropriate wait time.",
"option_d": "Use the ChangeMessageVisibility API to increase the visibility timeout.",
"correct_answers": ["D"],
"explanation_detailed": "Increasing the visibility timeout is like putting a 'do not disturb' sign on a message while it’s being processed—ensuring no one else picks it up and processes it simultaneously.",
"incorrect_explanations": {
"A": "Creating a new queue does not address the issue of messages being processed more than once by different consumers.",
"B": "Permissions are not the cause of duplicate processing; they control access, not visibility timing.",
"C": "Adjusting wait time for long polling improves efficiency but does not prevent multiple consumers from processing the same message."
}
},
{
"id": "saa-c03-design_secure_applications_architectures-104",
"certification_id": "SAA-C03",
"domain": "DESIGN_SECURE_APPLICATIONS_ARCHITECTURES",
"difficulty": "medium",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "A company is deploying a new business application on AWS. The application runs on two EC2 instances and uses an S3 bucket for document storage. A solutions architect must ensure that the EC2 instances can access the S3 bucket. What should be done?",
"option_a": "Create an IAM role that grants access to the S3 bucket and attach it to the EC2 instances.",
"option_b": "Create an IAM policy that grants access to the S3 bucket and attach it directly to the EC2 instances.",
"option_c": "Create an IAM group with S3 bucket access and attach it to the EC2 instances.",
"option_d": "Create an IAM user with S3 bucket access and attach its credentials to the EC2 instances.",
"correct_answers": ["A"],
"explanation_detailed": "Attaching an IAM role is like giving your EC2 instances special badges that allow them to access only what they need, without sharing sensitive credentials.",
"incorrect_explanations": {
"B": "You cannot attach an IAM policy directly to an EC2 instance; policies must be attached to identities like roles or users.",
"C": "IAM groups are used to group IAM users, not to be attached to EC2 instances.",
"D": "Embedding long-term user credentials on instances is insecure and increases operational overhead for rotation."
}
},
{
"id": "saa-c03-design_cost_optimized_architectures-105",
"certification_id": "SAA-C03",
"domain": "DESIGN_COST_OPTIMIZED_ARCHITECTURES",
"difficulty": "medium",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "A development team needs to host a website that will be accessed by other teams. The website content includes HTML, CSS, client-side JavaScript, and images. Which method is the MOST cost-effective?",
"option_a": "Containerize the website and host it on AWS Fargate.",
"option_b": "Create an S3 bucket and host the website there.",
"option_c": "Deploy a web server on an EC2 instance to host the website.",
"option_d": "Configure an Application Load Balancer with an AWS Lambda target using Express.js.",
"correct_answers": ["B"],
"explanation_detailed": "Hosting a static website on S3 is like having a warehouse that instantly delivers your brochures to anyone who asks—without the hassle of managing your own server.",
"incorrect_explanations": {
"A": "Using Fargate containers adds unnecessary compute and orchestration cost for a purely static website.",
"C": "Running and maintaining an EC2 instance for static content is more expensive and operationally heavy than S3 static website hosting.",
"D": "An Application Load Balancer with Lambda is more complex and costly than required for simple static content."
}
},
{
"id": "saa-c03-design_secure_applications_architectures-106",
"certification_id": "SAA-C03",
"domain": "DESIGN_SECURE_APPLICATIONS_ARCHITECTURES",
"difficulty": "medium",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "A company runs a multi-tier application on AWS and must track configuration changes to its resources and record an audit trail of API calls for compliance, governance, auditing, and security. What should be done?",
"option_a": "Use AWS CloudTrail to track configuration changes and AWS Config to record API calls.",
"option_b": "Use AWS Config to track configuration changes and AWS CloudTrail to record API calls.",
"option_c": "Use AWS Config to track configuration changes and Amazon CloudWatch to record API calls.",
"option_d": "Use AWS CloudTrail to track configuration changes and Amazon CloudWatch to record API calls.",
"correct_answers": ["B"],
"explanation_detailed": "Using Config and CloudTrail together is like having a surveillance system that not only notices when something changes but also keeps a log of who did what and when.",
"incorrect_explanations": {
"A": "CloudTrail records API calls, whereas AWS Config is the service that tracks configuration changes over time.",
"C": "CloudWatch focuses on metrics and logs but does not provide a full API audit log or configuration history like CloudTrail and Config.",
"D": "CloudTrail does not specialize in tracking configuration state changes; that is AWS Config’s role."
}
},
{
"id": "saa-c03-design_secure_applications_architectures-107",
"certification_id": "SAA-C03",
"domain": "DESIGN_SECURE_APPLICATIONS_ARCHITECTURES",
"difficulty": "medium",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "A company is preparing to launch a public-facing web application on AWS. The architecture consists of EC2 instances in a VPC behind an ELB. A third-party DNS service is used. The solutions architect must recommend a solution to detect and protect against large-scale DDoS attacks. Which solution meets these requirements?",
"option_a": "Enable Amazon GuardDuty in the account.",
"option_b": "Enable Amazon Inspector on the EC2 instances.",
"option_c": "Enable AWS Shield and assign Amazon Route 53 to it.",
"option_d": "Enable AWS Shield Advanced and assign the ELB to it.",
"correct_answers": ["D"],
"explanation_detailed": "This is like hiring a dedicated security team that continuously monitors your premises and steps in to block any coordinated attack attempts before they cause damage.",
"incorrect_explanations": {
"A": "GuardDuty provides threat detection and monitoring but does not directly provide DDoS protection on the ELB.",
"B": "Amazon Inspector focuses on assessing instance vulnerabilities, not on mitigating large-scale DDoS attacks.",
"C": "Assigning Shield to Route 53 does not directly protect the load balancer that fronts the web application."
}
},
{
"id": "saa-c03-design_resilient_architectures-108",
"certification_id": "SAA-C03",
"domain": "DESIGN_RESILIENT_ARCHITECTURES",
"difficulty": "medium",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "A company is building an application on AWS that will produce output files ranging from tens of gigabytes to hundreds of terabytes. The data must be stored in a standard file system structure. The company wants a solution that scales automatically, is highly available, and requires minimal operational overhead. Which solution meets these requirements?",
"option_a": "Migrate the application to run as containers on Amazon ECS. Use Amazon S3 for storage.",
"option_b": "Migrate the application to run as containers on Amazon EKS. Use Amazon EBS for storage.",
"option_c": "Migrate the application to EC2 instances in a Multi-AZ Auto Scaling group. Use Amazon EFS for storage.",
"option_d": "Migrate the application to EC2 instances in a Multi-AZ Auto Scaling group. Use Amazon EBS for storage.",
"correct_answers": ["C"],
"explanation_detailed": "Running your application on EC2 with EFS is like moving your office to a building that can automatically add more workers as needed while using a central filing system that grows with your data—completely hands-off.",
"incorrect_explanations": {
"A": "S3 is object storage and does not provide a native POSIX file system interface for applications that require standard file system semantics.",
"B": "EBS volumes are tied to a single AZ and instance and do not provide automatic multi-AZ scaling and shared access across many instances.",
"D": "EBS alone cannot scale out automatically across instances and AZs the way EFS can for shared file storage."
}
},
{
"id": "saa-c03-design_resilient_architectures-109",
"certification_id": "SAA-C03",
"domain": "DESIGN_RESILIENT_ARCHITECTURES",
"difficulty": "medium",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "A company stores its accounting records in Amazon S3. The records must be immediately accessible for 1 year and then archived for an additional 9 years. No one at the company, including administrative and root users, can delete the records during the entire 10-year period. The records must be stored with maximum resilience. Which solution meets these requirements?",
"option_a": "Store the records in S3 Glacier for the entire 10-year period. Use an access control policy to deny deletions for 10 years.",
"option_b": "Store the records using S3 Intelligent-Tiering. Use an IAM policy to deny deletions. After 10 years, change the IAM policy to allow deletion.",
"option_c": "Use an S3 lifecycle policy to transition the records from S3 Standard to S3 Glacier Deep Archive after 1 year. Use S3 Object Lock in compliance mode for 10 years.",
"option_d": "Use an S3 lifecycle policy to transition the records from S3 Standard to S3 One Zone-IA after 1 year. Use S3 Object Lock in governance mode for 10 years.",
"correct_answers": ["C"],
"explanation_detailed": "This approach is like placing your vital documents in a bank vault with strict instructions not to open them for 10 years, ensuring they remain untouched and secure.",
"incorrect_explanations": {
"A": "Storing directly in S3 Glacier for all 10 years does not meet the requirement for immediate access in the first year and does not use Object Lock compliance mode.",
"B": "IAM policies alone can be changed by administrators and do not provide the immutable protection that Object Lock compliance mode offers.",
"D": "S3 One Zone-IA does not provide maximum durability and governance mode still allows privileged users to override protections."
}
},
{
"id": "saa-c03-design_resilient_architectures-110",
"certification_id": "SAA-C03",
"domain": "DESIGN_RESILIENT_ARCHITECTURES",
"difficulty": "medium",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "A company runs several Windows workloads on AWS. Its employees use Windows file shares hosted on two EC2 instances that synchronize data and maintain duplicate copies. The company wants a highly available and durable storage solution that preserves the current file access experience. What should be done?",
"option_a": "Migrate all data to Amazon S3. Configure IAM authentication for user access.",
"option_b": "Set up an Amazon S3 File Gateway and mount it on the existing EC2 instances.",
"option_c": "Extend the file-sharing environment to Amazon FSx for Windows File Server with a Multi-AZ configuration and migrate all data there.",
"option_d": "Extend the file-sharing environment to Amazon EFS with a Multi-AZ configuration and migrate all data there.",
"correct_answers": ["C"],
"explanation_detailed": "Using FSx for Windows File Server is like upgrading your file system to one that automatically backs up your data and can seamlessly failover to a secondary location if needed—without altering the user experience.",
"incorrect_explanations": {
"A": "S3 is object storage and does not natively provide SMB file shares or the same Windows file system semantics.",
"B": "S3 File Gateway introduces a different access pattern and does not fully replace a native Windows file system experience.",
"D": "Amazon EFS provides NFS, not SMB, and is better suited for Linux-based workloads rather than Windows file shares."
}
},
{
"id": "saa-c03-design_secure_applications_architectures-111",
"certification_id": "SAA-C03",
"domain": "DESIGN_SECURE_APPLICATIONS_ARCHITECTURES",
"difficulty": "medium",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "A solutions architect is designing a VPC architecture that includes multiple subnets to host applications running on EC2 and RDS. The architecture spans six subnets across two Availability Zones. Each AZ includes a public subnet, a private subnet, and a dedicated database subnet. Only EC2 instances in the private subnets should have access to the RDS databases. Which solution meets these requirements?",
"option_a": "Create a new route table that excludes routes to the public subnet CIDRs. Associate it with the database subnets.",
"option_b": "Create a security group that denies inbound traffic from the public subnet’s security group. Attach it to the database instances.",
"option_c": "Create a security group that allows inbound traffic from the private subnet’s security group. Attach it to the database instances.",
"option_d": "Create VPC peering connections between the public and private subnets, and between the private and database subnets.",
"correct_answers": ["C"],
"explanation_detailed": "This is like locking the server room and giving keys only to employees from the private subnets, ensuring that only authorized servers can access the database.",
"incorrect_explanations": {
"A": "Route tables control routing, but the most precise way to restrict database access is with security groups based on source security groups.",
"B": "Deny rules are not used in security groups; they are allow-only, and you should instead allow from the trusted source.",
"D": "VPC peering is unnecessary and incorrect here; all subnets are in the same VPC and can be controlled with security groups."
}
},
{
"id": "saa-c03-design_secure_applications_architectures-112",
"certification_id": "SAA-C03",
"domain": "DESIGN_SECURE_APPLICATIONS_ARCHITECTURES",
"difficulty": "medium",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "A company registered its domain name with Amazon Route 53. It uses Amazon API Gateway in the ca-central-1 region as a public interface for its backend microservices. Third-party services securely consume the APIs. The company wants its API Gateway URL to use its own domain name and corresponding certificate so that third-party services can use HTTPS. Which solution meets these requirements?",
"option_a": "Create stage variables in API Gateway with Name='Endpoint-URL' and Value='Company Domain' to override the default URL. Import the public certificate into ACM.",
"option_b": "Create Route 53 DNS records for the company domain. Point an alias record to the Regional stage endpoint of API Gateway. Import the public certificate into ACM in us-east-1.",
"option_c": "Create a Regional API Gateway endpoint. Associate it with the company domain. Import the public certificate into ACM in the same region. Attach the certificate to the endpoint and configure Route 53 to route traffic to it.",
"option_d": "Create a Regional API Gateway endpoint. Associate it with the company domain. Import the public certificate into ACM in us-east-1. Attach the certificate to the API Gateway. Create Route 53 DNS records for the domain and point an A record to it.",
"correct_answers": ["D"],
"explanation_detailed": "This solution is like giving your online store its own street address and security badge so that customers can find you easily and interact securely over HTTPS.",
"incorrect_explanations": {
"A": "Stage variables do not provide custom domain names or certificate association for HTTPS endpoints.",
"B": "Simply pointing DNS to the Regional endpoint without correctly configuring the custom domain and certificate is insufficient.",
"C": "The scenario’s correct setup requires the specific ACM region usage and association outlined in option D."
}
},
{
"id": "saa-c03-design_secure_applications_architectures-113",
"certification_id": "SAA-C03",
"domain": "DESIGN_SECURE_APPLICATIONS_ARCHITECTURES",
"difficulty": "medium",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "A hospital uses a RESTful API deployed with API Gateway and Lambda to upload reports in PDF and JPEG formats. The hospital needs to modify the Lambda code to detect protected health information (PHI) in the reports. Which solution meets these requirements with the LOWEST operational overhead?",
"option_a": "Use existing Python libraries to extract text from the reports and identify PHI.",
"option_b": "Use Amazon Textract to extract text from the reports. Use Amazon SageMaker to identify PHI.",
"option_c": "Use Amazon Textract to extract text from the reports. Use Amazon Comprehend Medical to identify PHI.",
"option_d": "Use Amazon Rekognition to extract text from the reports. Use Amazon Comprehend Medical to identify PHI.",
"correct_answers": ["C"],
"explanation_detailed": "This approach is like having a team of experts quickly transcribe your reports and then scan them for sensitive information, all with minimal manual intervention.",
"incorrect_explanations": {
"A": "Building and maintaining custom Python text extraction and PHI logic increases operational effort.",
"B": "SageMaker requires you to build and operate ML models rather than using a managed medical NLP service.",
"D": "Rekognition is not the primary service for document text extraction in this scenario; Textract is purpose-built for that."
}
},
{
"id": "saa-c03-design_resilient_architectures-114",
"certification_id": "SAA-C03",
"domain": "DESIGN_RESILIENT_ARCHITECTURES",
"difficulty": "medium",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "A company runs a batch processing application on EC2 instances that processes messages from an SQS queue, writes to an RDS table, and then deletes the messages. Occasionally, duplicate records are found in the RDS table even though the SQS queue contains no duplicates. What should be done to ensure messages are processed only once?",
"option_a": "Use the CreateQueue API to create a new queue.",
"option_b": "Use the AddPermission API to add appropriate permissions.",
"option_c": "Use the ReceiveMessage API to set an appropriate wait time.",
"option_d": "Use the ChangeMessageVisibility API to increase the visibility timeout.",
"correct_answers": ["D"],
"explanation_detailed": "Increasing the visibility timeout is like putting a 'do not disturb' sign on a message while it’s being processed, so no one else picks it up concurrently.",
"incorrect_explanations": {
"A": "Creating a new queue does not change the at-least-once delivery semantics or visibility behavior.",
"B": "Permissions have no impact on preventing concurrent consumers from reading the same message.",
"C": "Long polling reduces empty responses but does not prevent duplicate processing by multiple workers."
}
},
{
"id": "saa-c03-design_cost_optimized_architectures-115",
"certification_id": "SAA-C03",
"domain": "DESIGN_COST_OPTIMIZED_ARCHITECTURES",
"difficulty": "medium",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "A company is deploying containerized applications on AWS. The applications are stateless and can tolerate interruptions in the underlying infrastructure. The company wants a solution that minimizes cost and operational overhead. What should a solutions architect do?",
"option_a": "Use Spot Instances in an EC2 Auto Scaling group to run the application containers.",
"option_b": "Use Spot Instances in a managed node group on Amazon EKS.",
"option_c": "Use On-Demand Instances in an EC2 Auto Scaling group to run the application containers.",
"option_d": "Use On-Demand Instances in a managed node group on Amazon EKS.",
"correct_answers": ["A"],
"explanation_detailed": "Using Spot Instances in an EC2 Auto Scaling group is like snagging last-minute airline tickets at a bargain price—as long as you can handle occasional interruptions, you save a lot.",
"incorrect_explanations": {
"B": "EKS managed node groups add additional control plane and management cost beyond what is necessary in this scenario.",
"C": "On-Demand Instances cost more than Spot Instances when the workload can tolerate interruptions.",
"D": "On-Demand capacity with EKS does not minimize cost compared to Spot Instances for interruptible workloads."
}
},
{
"id": "saa-c03-design_high_performing_architectures-116",
"certification_id": "SAA-C03",
"domain": "DESIGN_HIGH_PERFORMING_ARCHITECTURES",
"difficulty": "medium",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "A company is migrating its multi-tier on-premises application to AWS to improve performance. The application consists of tiers that communicate via RESTful services, and transactions are dropped when a tier becomes overwhelmed. A solutions architect must design a solution that resolves these issues and modernizes the application. Which solution meets these requirements and is the MOST operationally efficient?",
"option_a": "Use Amazon API Gateway and route transactions to AWS Lambda functions as the application tier. Use Amazon SQS as the messaging layer between application services.",
"option_b": "Use Amazon CloudWatch metrics to analyze historical performance and increase the size of the EC2 instances during peak usage.",
"option_c": "Use Amazon SNS for messaging between EC2 instances in an Auto Scaling group. Use CloudWatch to monitor the SNS queue length and scale accordingly.",
"option_d": "Use Amazon SQS for messaging between EC2 instances in an Auto Scaling group. Use CloudWatch to monitor the SQS queue length and scale up when communication failures occur.",
"correct_answers": ["A"],
"explanation_detailed": "This solution is like having an intelligent receptionist (API Gateway) who directs each customer to the right department (Lambda functions) and an internal messaging system (SQS) that ensures no request is lost even during heavy load.",
"incorrect_explanations": {
"B": "Simply resizing EC2 instances does not decouple tiers or eliminate dropped transactions during overload.",
"C": "SNS is a pub/sub service and is not ideal for durable queueing and back-pressure handling between services.",
"D": "Using SQS between EC2 instances helps with decoupling but still requires managing servers and is less operationally efficient than Lambda."
}
},
{
"id": "saa-c03-design_resilient_architectures-117",
"certification_id": "SAA-C03",
"domain": "DESIGN_RESILIENT_ARCHITECTURES",
"difficulty": "medium",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "A company stores sensitive customer transaction data in an Amazon DynamoDB table that must be retained for 7 years. Which solution is the MOST cost-effective?",
"option_a": "Configure global DynamoDB tables. For RPO, point the application to another region.",
"option_b": "Enable DynamoDB point-in-time recovery. For RPO, restore to the desired point in time.",
"option_c": "Export DynamoDB data to Amazon S3 Glacier daily. For RPO, import the data from S3 Glacier back to DynamoDB.",
"option_d": "Schedule EBS snapshots for the DynamoDB table every 15 minutes. For RPO, restore the table from an EBS snapshot.",
"correct_answers": ["B"],
"explanation_detailed": "Using point-in-time recovery is like having a time machine for your data—you can roll back to just before an issue occurs, minimizing data loss quickly.",
"incorrect_explanations": {
"A": "Global tables are designed for multi-region active-active workloads, not primarily for long-term retention at minimal cost.",
"C": "Regular exports to Glacier and imports back add complexity and longer recovery times compared to native PITR.",
"D": "You cannot take EBS snapshots directly of DynamoDB tables; snapshots apply to EBS volumes, not DynamoDB."
}
},
{
"id": "saa-c03-design_secure_applications_architectures-118",
"certification_id": "SAA-C03",
"domain": "DESIGN_SECURE_APPLICATIONS_ARCHITECTURES",
"difficulty": "medium",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "A company hosts its application on EC2 instances and uses an Aurora database. The EC2 instances connect to the database using locally stored credentials in a file. The company wants to reduce the operational overhead of managing these credentials. What should be done?",
"option_a": "Use AWS Secrets Manager and enable automatic rotation.",
"option_b": "Use AWS Systems Manager Parameter Store and enable automatic rotation.",
"option_c": "Create an S3 bucket to store an encrypted credentials file using an AWS KMS key. Migrate the file to S3 and point the application to it.",
"option_d": "Create an encrypted EBS volume for each EC2 instance, attach it, and migrate the credentials file to the new volume. Point the application to the new volume.",
"correct_answers": ["B"],
"explanation_detailed": "This is like storing your secret recipe in a secure safe that automatically changes its combination—eliminating the need for manual updates.",
"incorrect_explanations": {
"A": "Secrets Manager is well suited for secret storage and rotation but may be more costly than Parameter Store for this use case, which the question frames around operational overhead rather than advanced secret features.",
"C": "Using S3 to store credentials still requires custom logic to manage encryption, retrieval, and rotation.",
"D": "Encrypting an EBS volume protects data at rest but does not simplify or automate credential rotation management."
}
},
{
"id": "saa-c03-design_cost_optimized_architectures-119",
"certification_id": "SAA-C03",
"domain": "DESIGN_COST_OPTIMIZED_ARCHITECTURES",
"difficulty": "medium",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "A development team needs to host a website that will be accessed by other teams. The website contains HTML, CSS, client-side JavaScript, and images. Which method is the MOST cost-effective?",
"option_a": "Containerize the website and host it on AWS Fargate.",
"option_b": "Create an S3 bucket and host the website there.",
"option_c": "Deploy a web server on an EC2 instance to host the website.",
"option_d": "Configure an Application Load Balancer with an AWS Lambda target using Express.js.",
"correct_answers": ["B"],
"explanation_detailed": "Hosting a static website on S3 is like keeping your brochures in a warehouse that delivers them instantly—without the hassle of managing a full web server.",
"incorrect_explanations": {
"A": "Using Fargate introduces container orchestration and compute costs that are unnecessary for static site hosting.",
"C": "Running an EC2 instance 24/7 for static content generally costs more and requires patching and maintenance.",
"D": "ALB plus Lambda for serving static assets is more complex and costly than S3 static website hosting."
}
},
{
"id": "saa-c03-design_secure_applications_architectures-120",
"certification_id": "SAA-C03",
"domain": "DESIGN_SECURE_APPLICATIONS_ARCHITECTURES",
"difficulty": "medium",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "A company runs a multi-tier application on AWS and must track configuration changes and log an audit trail of API calls for compliance and security. What should be done?",
"option_a": "Use AWS CloudTrail for configuration changes and AWS Config to record API calls.",
"option_b": "Use AWS Config for configuration changes and AWS CloudTrail to record API calls.",
"option_c": "Use AWS Config for configuration changes and Amazon CloudWatch to log API calls.",
"option_d": "Use AWS CloudTrail for configuration changes and Amazon CloudWatch to log API calls.",
"correct_answers": ["B"],
"explanation_detailed": "This setup is like having both a watchdog and a diary—Config monitors changes while CloudTrail logs every API call, giving you a full picture of activity.",
"incorrect_explanations": {
"A": "CloudTrail is not designed to track configuration state changes; that is AWS Config’s responsibility.",
"C": "CloudWatch can capture logs and metrics but does not natively provide a complete API audit history like CloudTrail.",
"D": "CloudTrail again does not specialize in configuration history, so using it alone for that purpose is insufficient."
}
},
{
"id": "saa-c03-design_secure_applications_architectures-121",
"certification_id": "SAA-C03",
"domain": "DESIGN_SECURE_APPLICATIONS_ARCHITECTURES",
"difficulty": "medium",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "A company is preparing to launch a public-facing web application on AWS. The architecture consists of EC2 instances inside a VPC behind an ELB. A third-party DNS service is used. The solutions architect must recommend a solution to detect and protect against large-scale DDoS attacks. Which solution meets these requirements?",
"option_a": "Enable Amazon GuardDuty in the account.",
"option_b": "Enable Amazon Inspector on the EC2 instances.",
"option_c": "Enable AWS Shield and assign Amazon Route 53 to it.",
"option_d": "Enable AWS Shield Advanced and assign the ELB to it.",
"correct_answers": ["D"],
"explanation_detailed": "This solution is like hiring a dedicated security team to constantly guard your entrance—blocking DDoS attacks before they ever reach your servers.",
"incorrect_explanations": {
"A": "GuardDuty focuses on threat detection and alerts, not active DDoS mitigation on the ELB.",
"B": "Inspector analyzes instance vulnerabilities but does not provide DDoS protection.",
"C": "Protecting Route 53 alone does not ensure full DDoS protection for the ELB-fronted application."
}
},
{
"id": "saa-c03-design_resilient_architectures-122",
"certification_id": "SAA-C03",
"domain": "DESIGN_RESILIENT_ARCHITECTURES",
"difficulty": "medium",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "A company is building an application on AWS that will produce output files ranging from tens of gigabytes to hundreds of terabytes. The data must be stored in a standard file system structure. The company wants a solution that automatically scales, is highly available, and minimizes operational overhead. Which solution meets these requirements?",
"option_a": "Migrate the application to run as containers on Amazon ECS. Use Amazon S3 for storage.",
"option_b": "Migrate the application to run as containers on Amazon EKS. Use Amazon EBS for storage.",
"option_c": "Migrate the application to EC2 instances in a Multi-AZ Auto Scaling group. Use Amazon EFS for storage.",
"option_d": "Migrate the application to EC2 instances in a Multi-AZ Auto Scaling group. Use Amazon EBS for storage.",
"correct_answers": ["C"],
"explanation_detailed": "Running your application on EC2 with EFS is like moving to an office that can expand its workforce on demand while using a central filing system that grows with your data—without any manual effort.",
"incorrect_explanations": {
"A": "S3 does not provide a native file system interface required by many applications.",
"B": "EBS does not automatically scale across instances and Availability Zones for shared file storage.",
"D": "Relying solely on EBS limits scalability and shared access compared to EFS."
}
},
{
"id": "saa-c03-design_resilient_architectures-123",
"certification_id": "SAA-C03",
"domain": "DESIGN_RESILIENT_ARCHITECTURES",
"difficulty": "medium",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "A company stores its accounting records in Amazon S3. The records must be immediately accessible for 1 year and then archived for another 9 years. No one, including administrators, can delete the records for the entire 10-year period. The records must be stored with maximum durability. Which solution meets these requirements?",
"option_a": "Store the records in S3 Glacier for the full 10 years. Use an access control policy to deny deletion for 10 years.",
"option_b": "Store the records using S3 Intelligent-Tiering. Use an IAM policy to deny deletion. After 10 years, change the policy to allow deletion.",
"option_c": "Use an S3 lifecycle policy to transition the records from S3 Standard to S3 Glacier Deep Archive after 1 year. Use S3 Object Lock in compliance mode for 10 years.",
"option_d": "Use an S3 lifecycle policy to transition the records from S3 Standard to S3 One Zone-IA after 1 year. Use S3 Object Lock in governance mode for 10 years.",
"correct_answers": ["C"],
"explanation_detailed": "This method is like locking your most critical documents in a bank vault with strict instructions not to open them for 10 years—ensuring they remain secure and intact.",
"incorrect_explanations": {
"A": "Storing everything in S3 Glacier directly does not satisfy the requirement for immediate accessibility in the first year and lacks immutable Object Lock compliance.",
"B": "Relying only on IAM policies is weaker than Object Lock compliance mode, since policies can be changed by administrators.",
"D": "One Zone-IA does not provide maximum durability and governance mode does not enforce the same immutability guarantees as compliance mode."
}
},
{
"id": "saa-c03-design_resilient_architectures-124",
"certification_id": "SAA-C03",
"domain": "DESIGN_RESILIENT_ARCHITECTURES",
"difficulty": "medium",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "A company runs several Windows workloads on AWS. Its employees use Windows file shares hosted on two EC2 instances that synchronize data and maintain duplicate copies. The company wants a highly available and durable storage solution that preserves the current file access experience. What should be done?",
"option_a": "Migrate all data to Amazon S3 and configure IAM authentication for file access.",
"option_b": "Set up an Amazon S3 File Gateway and mount it on the existing EC2 instances.",
"option_c": "Extend the file-sharing environment to Amazon FSx for Windows File Server with a Multi-AZ configuration and migrate all data there.",
"option_d": "Extend the file-sharing environment to Amazon EFS with a Multi-AZ configuration and migrate all data there.",
"correct_answers": ["C"],
"explanation_detailed": "Migrating to FSx for Windows File Server is like upgrading your office file system to one that automatically backs up your data and can quickly fail over to a secondary site—while preserving the way users access their files.",
"incorrect_explanations": {
"A": "S3 object storage does not provide the same native SMB file share semantics or user experience as Windows file servers.",
"B": "S3 File Gateway is useful for some file workloads but still does not fully emulate a native Windows file server for all scenarios.",
"D": "EFS offers NFS for Linux clients and is not designed to replace Windows SMB file shares."
}
},
{
"id": "saa-c03-design_secure_applications_architectures-125",
"certification_id": "SAA-C03",
"domain": "DESIGN_SECURE_APPLICATIONS_ARCHITECTURES",
"difficulty": "medium",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "A solutions architect must design a VPC that includes multiple subnets to host applications on EC2 and RDS. The architecture spans six subnets in two Availability Zones, each containing a public subnet, a private subnet, and a dedicated database subnet. Only EC2 instances in the private subnets should have access to the RDS databases. Which solution meets these requirements?",
"option_a": "Create a new route table that excludes routes to the public subnet CIDRs and associate it with the database subnets.",
"option_b": "Create a security group that denies inbound traffic from the public subnet’s security group and attach it to the database instances.",
"option_c": "Create a security group that allows inbound traffic only from the private subnet’s security group and attach it to the database instances.",
"option_d": "Establish VPC peering connections between the public and private subnets and between the private and database subnets.",
"correct_answers": ["C"],
"explanation_detailed": "This setup is like locking the server room and handing out keys only to the staff from the private subnets—ensuring that only authorized servers can access the database.",
"incorrect_explanations": {
"A": "Route tables alone do not precisely control which EC2 instances can reach the database; security groups offer finer-grained control.",
"B": "Security groups do not support explicit deny rules, and this approach would be less precise than allowing from a trusted source group.",
"D": "VPC peering is unnecessary within a single VPC and does not solve the access restriction requirement."
}
},
{
"id": "saa-c03-design_secure_applications_architectures-126",
"certification_id": "SAA-C03",
"domain": "DESIGN_SECURE_APPLICATIONS_ARCHITECTURES",
"difficulty": "medium",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "A company registered its domain name with Amazon Route 53. It uses API Gateway in the ca-central-1 region as a public interface for its backend microservices. Third-party services consume the APIs securely. The company wants its API Gateway URL to incorporate its domain name and a corresponding certificate so that third-party services can use HTTPS. Which solution meets these requirements?",
"option_a": "Create stage variables in API Gateway with Name='Endpoint-URL' and Value='Company Domain' to override the default URL. Import the domain’s public certificate into ACM.",
"option_b": "Create Route 53 DNS records for the company domain. Point an alias record to the Regional API Gateway stage endpoint. Import the domain’s public certificate into ACM in us-east-1.",
"option_c": "Create a Regional API Gateway endpoint. Associate it with the company domain. Import the domain’s public certificate into ACM in the same region. Attach the certificate to the endpoint and configure Route 53 to route traffic to it.",
"option_d": "Create a Regional API Gateway endpoint. Associate it with the company domain. Import the domain’s public certificate into ACM in us-east-1. Attach the certificate to the API Gateway. Create Route 53 DNS records for the domain and point an A record to it.",
"correct_answers": ["D"],
"explanation_detailed": "This solution is like giving your online store its very own street address and security badge so that external partners can securely access your APIs via HTTPS.",
"incorrect_explanations": {
"A": "Stage variables do not configure custom domains or certificate bindings in API Gateway.",
"B": "DNS records alone do not configure the custom domain and certificate usage correctly for the Regional endpoint.",
"C": "The answer scenario requires the specific ACM region and attachment approach described in option D."
}
},
{
"id": "saa-c03-design_secure_applications_architectures-127",
"certification_id": "SAA-C03",
"domain": "DESIGN_SECURE_APPLICATIONS_ARCHITECTURES",
"difficulty": "medium",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "A company’s hospital deployed a RESTful API using API Gateway and Lambda to upload reports in PDF and JPEG formats. The hospital needs to modify the Lambda function to detect protected health information (PHI) in the reports. Which solution meets these requirements with the LOWEST operational overhead?",
"option_a": "Use existing Python libraries to extract text from the reports and identify PHI.",
"option_b": "Use Amazon Textract to extract text from the reports. Use Amazon SageMaker to identify PHI.",
"option_c": "Use Amazon Textract to extract text from the reports. Use Amazon Comprehend Medical to identify PHI.",
"option_d": "Use Amazon Rekognition to extract text from the reports. Use Amazon Comprehend Medical to identify PHI.",
"correct_answers": ["C"],
"explanation_detailed": "This approach is like having a team of experts quickly scan and transcribe your reports, then flag any sensitive health information for review—all without extra manual effort.",
"incorrect_explanations": {
"A": "Custom Python solutions require ongoing maintenance and tuning, increasing operational overhead.",
"B": "SageMaker requires you to own the lifecycle of ML models rather than relying on a managed medical NLP service.",
"D": "Rekognition is more suited to image and video analysis and is not the preferred tool for document text extraction in this context."
}
},
{
"id": "saa-c03-design_resilient_architectures-128",
"certification_id": "SAA-C03",
"domain": "DESIGN_RESILIENT_ARCHITECTURES",
"difficulty": "medium",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "A company runs an application on multiple EC2 instances that processes messages from an SQS queue, writes to an RDS table, and deletes the messages. Occasionally, duplicate records appear in the RDS table even though the SQS queue contains no duplicates. What should be done to ensure messages are processed only once?",
"option_a": "Use the CreateQueue API to create a new queue.",
"option_b": "Use the AddPermission API to add proper permissions.",
"option_c": "Use the ReceiveMessage API to set an appropriate wait time.",
"option_d": "Use the ChangeMessageVisibility API to increase the visibility timeout.",
"correct_answers": ["D"],
"explanation_detailed": "Raising the visibility timeout is like putting a 'do not disturb' sign on a message while it’s being processed so that no one else grabs it at the same time.",
"incorrect_explanations": {
"A": "Creating a new queue will not change how long messages remain invisible after being read.",
"B": "Permissions do not control whether multiple consumers can see the same message during its processing window.",
"C": "Adjusting wait time affects long polling efficiency, not the prevention of duplicate message processing."
}
},
{
"id": "saa-c03-design_cost_optimized_architectures-129",
"certification_id": "SAA-C03",
"domain": "DESIGN_COST_OPTIMIZED_ARCHITECTURES",
"difficulty": "medium",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "A company runs a containerized application on AWS using AWS Fargate with Amazon ECS. The application is stateless and can tolerate interruptions. The company wants to minimize cost and operational overhead. What should a solutions architect do?",
"option_a": "Use Spot Instances in an EC2 Auto Scaling group to run the containers.",
"option_b": "Use Spot Instances in a managed node group on Amazon EKS.",
"option_c": "Use On-Demand Instances in an EC2 Auto Scaling group to run the containers.",
"option_d": "Use On-Demand Instances in a managed node group on Amazon EKS.",
"correct_answers": ["A"],
"explanation_detailed": "Using Spot Instances in an EC2 Auto Scaling group is like catching last-minute airline tickets at a bargain—if you can tolerate occasional interruptions, you save a ton.",
"incorrect_explanations": {
"B": "EKS adds management cost and complexity for the control plane that is not necessary for this stateless, interrupt-tolerant workload.",
"C": "On-Demand Instances are more expensive than Spot Instances for the same capacity.",
"D": "On-Demand EKS worker nodes similarly cost more and add complexity without cost benefits for an interrupt-tolerant application."
}
},
{
"id": "saa-c03-design_high_performing_architectures-130",
"certification_id": "SAA-C03",
"domain": "DESIGN_HIGH_PERFORMING_ARCHITECTURES",
"difficulty": "medium",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "A company is migrating its legacy on-premises multi-tier application to AWS to improve performance. The application’s tiers communicate via RESTful services, and transactions are dropped when a tier becomes overwhelmed. A solutions architect must design a solution that resolves these issues and modernizes the application. Which solution meets these requirements and is the MOST operationally efficient?",
"option_a": "Use Amazon API Gateway and route transactions to Lambda functions as the application tier. Use Amazon SQS as the messaging layer between application services.",
"option_b": "Use CloudWatch metrics to analyze historical performance and scale up the EC2 instance size during peak loads.",
"option_c": "Use Amazon SNS for messaging between application servers in an Auto Scaling group. Use CloudWatch to monitor SNS queue length and scale accordingly.",
"option_d": "Use Amazon SQS for messaging between application servers in an Auto Scaling group. Use CloudWatch to monitor SQS queue length and scale up when communication failures are detected.",
"correct_answers": ["A"],
"explanation_detailed": "This solution is like having an intelligent receptionist (API Gateway) that directs each customer to the right department (Lambda functions) and an internal messaging system (SQS) that ensures no request is dropped even if one department gets overwhelmed.",
"incorrect_explanations": {
"B": "Scaling up instances does not address the need for decoupling and modernizing the architecture to handle overload gracefully.",
"C": "SNS is not intended as a durable queueing mechanism for handling back-pressure between tightly coupled services.",
"D": "While SQS between EC2 instances helps, it still leaves you managing servers instead of using a fully managed, serverless application tier."
}
},
{
"id": "saa-c03-design_resilient_architectures-131",
"certification_id": "SAA-C03",
"domain": "DESIGN_RESILIENT_ARCHITECTURES",
"difficulty": "medium",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "A company stores sensitive customer transaction data in an Amazon DynamoDB table and must retain it for 7 years. Which solution is the MOST cost-effective?",
"option_a": "Configure global DynamoDB tables. For RPO recovery, direct the application to a different region.",
"option_b": "Enable DynamoDB point-in-time recovery. For RPO recovery, restore to the desired point in time.",
"option_c": "Export DynamoDB data to Amazon S3 Glacier daily. For RPO recovery, import the data back to DynamoDB.",
"option_d": "Schedule EBS snapshots for the DynamoDB table every 15 minutes. For RPO recovery, restore the table from an EBS snapshot.",
"correct_answers": ["B"],
"explanation_detailed": "This feature is like having a time machine for your data—you can roll back to just before an issue occurred, ensuring minimal data loss.",
"incorrect_explanations": {
"A": "Global tables are intended for multi-region active-active workloads, not primarily for cost-effective, long-term retention.",
"C": "Manually exporting to Glacier and re-importing increases complexity and recovery times versus native PITR.",
"D": "You cannot snapshot DynamoDB with EBS; DynamoDB is a managed service with its own backup mechanisms."
}
},
{
"id": "saa-c03-design_cost_optimized_architectures-132",
"certification_id": "SAA-C03",
"domain": "DESIGN_COST_OPTIMIZED_ARCHITECTURES",
"difficulty": "medium",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "A company is hosting its web application on AWS. It wants to ensure that all EC2 instances, RDS database instances, and Redshift clusters are tagged. The company wishes to minimize the effort required to verify proper tagging. What should be done?",
"option_a": "Use AWS Config rules to define and detect resources that are not tagged correctly.",
"option_b": "Use Cost Explorer to identify untagged resources and manually tag them.",
"option_c": "Write API calls to check resource tagging and run them periodically on an EC2 instance.",
"option_d": "Write API calls to check resource tagging and schedule a Lambda function via CloudWatch to run them periodically.",
"correct_answers": ["A"],
"explanation_detailed": "Using AWS Config rules is like having an inspector who constantly checks that every resource is properly labeled, alerting you if anything’s amiss—without you having to manually check.",
"incorrect_explanations": {
"B": "Cost Explorer can show cost allocation by tags but does not continuously enforce or validate tagging policies.",
"C": "Custom scripts on EC2 require ongoing instance management and are less efficient than managed Config rules.",
"D": "Lambda-based checks still require custom logic and maintenance instead of using a built-in rules engine."
}
},
{
"id": "saa-c03-design_cost_optimized_architectures-133",
"certification_id": "SAA-C03",
"domain": "DESIGN_COST_OPTIMIZED_ARCHITECTURES",
"difficulty": "medium",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "A development team needs to host a website accessed by other teams. The website content includes HTML, CSS, client-side JavaScript, and images. Which method is the MOST cost-effective?",
"option_a": "Containerize the website and host it on AWS Fargate.",
"option_b": "Create an S3 bucket and host the website there.",
"option_c": "Deploy a web server on an EC2 instance to host the website.",
"option_d": "Configure an Application Load Balancer with an AWS Lambda target using Express.js.",
"correct_answers": ["B"],
"explanation_detailed": "Hosting a static website on S3 is like keeping your brochures in a warehouse that delivers them instantly when requested—without the hassle of managing a web server.",
"incorrect_explanations": {
"A": "Fargate introduces more compute and orchestration overhead than necessary for a static site.",
"C": "An EC2 web server costs more and requires patching and scaling compared to S3 static hosting.",
"D": "ALB plus Lambda is overly complex and costly for simple static content."
}
},
{
"id": "saa-c03-design_secure_applications_architectures-134",
"certification_id": "SAA-C03",
"domain": "DESIGN_SECURE_APPLICATIONS_ARCHITECTURES",
"difficulty": "medium",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "A company runs a multi-tier application on AWS and needs to track configuration changes to its resources and log API calls for compliance, governance, and auditing. What should be done?",
"option_a": "Use AWS CloudTrail to track configuration changes and AWS Config to record API calls.",
"option_b": "Use AWS Config to track configuration changes and AWS CloudTrail to record API calls.",
"option_c": "Use AWS Config to track configuration changes and Amazon CloudWatch to log API calls.",
"option_d": "Use AWS CloudTrail to track configuration changes and Amazon CloudWatch to log API calls.",
"correct_answers": ["B"],
"explanation_detailed": "This combination is like having both a watchdog and a logbook—Config monitors changes and CloudTrail keeps a detailed diary of every API call.",
"incorrect_explanations": {
"A": "CloudTrail is not optimized for configuration state tracking, whereas AWS Config is.",
"C": "CloudWatch does not provide a full API call audit trail like CloudTrail does.",
"D": "Using CloudTrail alone does not provide the configuration history and compliance view that Config offers."
}
},
{
"id": "saa-c03-design_secure_applications_architectures-135",
"certification_id": "SAA-C03",
"domain": "DESIGN_SECURE_APPLICATIONS_ARCHITECTURES",
"difficulty": "medium",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "A company is preparing to launch a public-facing web application on AWS. The architecture consists of EC2 instances in a VPC behind an ELB, and a third-party DNS service is used. The solutions architect must recommend a solution to detect and protect against large-scale DDoS attacks. Which solution meets these requirements?",
"option_a": "Enable Amazon GuardDuty in the account.",
"option_b": "Enable Amazon Inspector on the EC2 instances.",
"option_c": "Enable AWS Shield and assign Amazon Route 53 to it.",
"option_d": "Enable AWS Shield Advanced and assign the ELB to it.",
"correct_answers": ["D"],
"explanation_detailed": "This approach is like hiring a dedicated security guard who continuously monitors and deflects any coordinated attack attempts before they reach your servers.",
"incorrect_explanations": {
"A": "GuardDuty is for threat detection and alerts, not direct DDoS mitigation at the ELB.",
"B": "Inspector focuses on vulnerability assessment, not DDoS protection.",
"C": "Shield on Route 53 alone does not fully protect the ELB that fronts the application."
}
},
{
"id": "saa-c03-design_resilient_architectures-136",
"certification_id": "SAA-C03",
"domain": "DESIGN_RESILIENT_ARCHITECTURES",
"difficulty": "medium",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "A company is designing an application on AWS that will generate output files ranging from tens of gigabytes to hundreds of terabytes. The data must be stored in a standard file system format. The company wants a solution that scales automatically, is highly available, and minimizes operational overhead. Which solution meets these requirements?",
"option_a": "Migrate the application to run as containers on Amazon ECS. Use Amazon S3 for storage.",
"option_b": "Migrate the application to run as containers on Amazon EKS. Use Amazon EBS for storage.",
"option_c": "Migrate the application to EC2 instances in a Multi-AZ Auto Scaling group. Use Amazon EFS for storage.",
"option_d": "Migrate the application to EC2 instances in a Multi-AZ Auto Scaling group. Use Amazon EBS for storage.",
"correct_answers": ["C"],
"explanation_detailed": "Running your application on EC2 with EFS is like moving your office to a building that can automatically expand its workforce while using a central filing system that grows with your data—without manual hassle.",
"incorrect_explanations": {
"A": "S3 does not present a POSIX-compatible file system interface required by many applications.",
"B": "EBS volumes do not automatically scale across multiple instances and AZs for shared access.",
"D": "Using EBS alone restricts scalability and resiliency compared to EFS’s shared, multi-AZ design."
}
},
{
"id": "saa-c03-design_resilient_architectures-137",
"certification_id": "SAA-C03",
"domain": "DESIGN_RESILIENT_ARCHITECTURES",
"difficulty": "medium",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "A company stores its accounting records in Amazon S3. The records must be immediately accessible for 1 year and then archived for an additional 9 years. No one—neither admins nor root users—can delete the records during the entire 10-year period. The records must be stored with maximum durability. Which solution meets these requirements?",
"option_a": "Store the records in S3 Glacier for the full 10-year period. Use an access control policy to deny deletions.",
"option_b": "Store the records using S3 Intelligent-Tiering. Use an IAM policy to deny deletions, then modify after 10 years.",
"option_c": "Use an S3 lifecycle policy to transition records from S3 Standard to S3 Glacier Deep Archive after 1 year. Use S3 Object Lock in compliance mode for 10 years.",
"option_d": "Use an S3 lifecycle policy to transition records from S3 Standard to S3 One Zone-IA after 1 year. Use S3 Object Lock in governance mode for 10 years.",
"correct_answers": ["C"],
"explanation_detailed": "This method is like storing your most critical documents in a vault with strict instructions not to touch them for 10 years—ensuring their security and durability.",
"incorrect_explanations": {
"A": "Access control policies alone can be modified and do not enforce immutability the way Object Lock compliance mode does.",
"B": "IAM policies are not tamper-proof and can be changed by administrators, which violates the strict non-deletion requirement.",
"D": "One Zone-IA lacks the highest durability, and governance mode allows certain users to override retention settings."
}
},
{
"id": "saa-c03-design_resilient_architectures-138",
"certification_id": "SAA-C03",
"domain": "DESIGN_RESILIENT_ARCHITECTURES",
"difficulty": "medium",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "A company runs several Windows workloads on AWS. Its employees use Windows file shares hosted on two EC2 instances that synchronize data and maintain duplicates. The company wants a highly available and durable storage solution that preserves the current file access experience. What should be done?",
"option_a": "Migrate all data to Amazon S3 and configure IAM authentication for file access.",
"option_b": "Set up an Amazon S3 File Gateway and mount it on the existing EC2 instances.",
"option_c": "Extend the file-sharing environment to Amazon FSx for Windows File Server with a Multi-AZ configuration and migrate all data.",
"option_d": "Extend the file-sharing environment to Amazon EFS with a Multi-AZ configuration and migrate all data.",
"correct_answers": ["C"],
"explanation_detailed": "Migrating to FSx for Windows File Server is like upgrading your file system to one that automatically backs up your data and can quickly fail over if needed, while maintaining the same user experience.",
"incorrect_explanations": {
"A": "S3 does not provide SMB shares and the same user experience as Windows file servers.",
"B": "S3 File Gateway is useful in some hybrid scenarios but does not fully replicate native Windows file server behavior for all use cases.",
"D": "EFS is an NFS-based service designed primarily for Linux clients, not Windows file sharing."
}
},
{
"id": "saa-c03-design_secure_applications_architectures-139",
"certification_id": "SAA-C03",
"domain": "DESIGN_SECURE_APPLICATIONS_ARCHITECTURES",
"difficulty": "medium",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "A company is designing a VPC with multiple subnets to host applications on EC2 and RDS. The architecture spans six subnets across two AZs; each AZ includes a public subnet, a private subnet, and a dedicated database subnet. Only EC2 instances in the private subnets should access the RDS databases. Which solution meets these requirements?",
"option_a": "Create a new route table that excludes routes to the public subnet CIDRs and associate it with the database subnets.",
"option_b": "Create a security group that denies inbound traffic from the public subnet’s security group and attach it to the database instances.",
"option_c": "Create a security group that allows inbound traffic only from the private subnet’s security group and attach it to the database instances.",
"option_d": "Create VPC peering connections between the public and private subnets, and between the private and database subnets.",
"correct_answers": ["C"],
"explanation_detailed": "This solution is like locking the server room and handing keys only to those in the private subnets—ensuring that only trusted instances can access the database.",
"incorrect_explanations": {
"A": "Route tables do not provide fine-grained instance-level access control to the database.",
"B": "Security groups do not support explicit deny rules and this still would not precisely allow only the intended sources.",
"D": "VPC peering is unnecessary within the same VPC and does not address the access restriction requirement."
}
},
{
"id": "saa-c03-design_secure_applications_architectures-140",
"certification_id": "SAA-C03",
"domain": "DESIGN_SECURE_APPLICATIONS_ARCHITECTURES",
"difficulty": "medium",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "A company registered its domain name with Amazon Route 53 and uses API Gateway in the ca-central-1 region as a public interface for its backend microservices. Third-party services securely consume the APIs. The company wants to design its API Gateway URL to include its domain name and a corresponding certificate so that third-party services can use HTTPS. Which solution meets these requirements?",
"option_a": "Create stage variables in API Gateway with Name='Endpoint-URL' and Value='Company Domain' to override the default URL. Import the domain’s public certificate into ACM.",
"option_b": "Create Route 53 DNS records for the domain. Point an alias record to the Regional API Gateway stage endpoint. Import the certificate into ACM in us-east-1.",
"option_c": "Create a Regional API Gateway endpoint. Associate it with the company domain. Import the certificate into ACM in the same region. Attach the certificate to the endpoint and configure Route 53 to route traffic.",
"option_d": "Create a Regional API Gateway endpoint. Associate it with the company domain. Import the certificate into ACM in us-east-1. Attach the certificate to the API Gateway. Create Route 53 DNS records for the domain and point an A record to it.",
"correct_answers": ["D"],
"explanation_detailed": "This approach is like giving your online store its own street address and security badge so that customers can find it easily and interact securely over HTTPS.",
"incorrect_explanations": {
"A": "Stage variables cannot replace proper custom domain configuration and certificate binding.",
"B": "DNS alias records alone do not configure the custom domain and certificate on the API Gateway side.",
"C": "The correct configuration in this scenario requires the specific ACM region and attachment described in option D."
}
},
{
"id": "saa-c03-design_secure_applications_architectures-141",
"certification_id": "SAA-C03",
"domain": "DESIGN_SECURE_APPLICATIONS_ARCHITECTURES",
"difficulty": "medium",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "A hospital deployed a RESTful API using API Gateway and Lambda to upload reports in PDF and JPEG formats. The hospital needs to modify the Lambda code to identify PHI in the reports. Which solution meets these requirements with the LOWEST operational overhead?",
"option_a": "Use existing Python libraries to extract text from the reports and identify PHI.",
"option_b": "Use Amazon Textract to extract text from the reports. Use Amazon SageMaker to identify PHI.",
"option_c": "Use Amazon Textract to extract text from the reports. Use Amazon Comprehend Medical to identify PHI.",
"option_d": "Use Amazon Rekognition to extract text from the reports. Use Amazon Comprehend Medical to identify PHI.",
"correct_answers": ["C"],
"explanation_detailed": "This solution is like having a team of experts quickly transcribe your reports and then scan them for sensitive health information—without requiring additional manual steps.",
"incorrect_explanations": {
"A": "Custom Python solutions require continuous maintenance and are less scalable than managed services.",
"B": "SageMaker requires managing model lifecycle and infrastructure, increasing operational overhead.",
"D": "Rekognition is not the primary service for document-oriented text extraction; Textract is better suited."
}
},
{
"id": "saa-c03-design_resilient_architectures-142",
"certification_id": "SAA-C03",
"domain": "DESIGN_RESILIENT_ARCHITECTURES",
"difficulty": "medium",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "A company runs a batch processing application on EC2 that processes messages from an SQS queue, writes to an RDS table, and deletes messages from the queue. Occasionally, duplicate records appear in the RDS table despite no duplicates in the SQS queue. What should be done to ensure messages are processed only once?",
"option_a": "Use the CreateQueue API to create a new queue.",
"option_b": "Use the AddPermission API to add the proper permissions.",
"option_c": "Use the ReceiveMessage API to set an appropriate wait time.",
"option_d": "Use the ChangeMessageVisibility API to increase the visibility timeout.",
"correct_answers": ["D"],
"explanation_detailed": "Increasing the visibility timeout is like placing a 'do not disturb' sign on a message while it’s being processed, so it isn’t picked up by another worker at the same time.",
"incorrect_explanations": {
"A": "Creating a new queue does not change the visibility timeout behavior that leads to duplicate processing.",
"B": "Permissions do not affect whether another consumer can see a message before processing completes.",
"C": "ReceiveMessage wait time affects polling behavior, not how long a message stays hidden after being received."
}
},
{
"id": "saa-c03-design_cost_optimized_architectures-143",
"certification_id": "SAA-C03",
"domain": "DESIGN_COST_OPTIMIZED_ARCHITECTURES",
"difficulty": "medium",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "A company is deploying containerized applications on AWS. The applications are stateless and can tolerate interruptions. The company wants a solution that minimizes cost and operational overhead. What should a solutions architect do?",
"option_a": "Use Spot Instances in an EC2 Auto Scaling group to run the containers.",
"option_b": "Use Spot Instances in a managed node group on Amazon EKS.",
"option_c": "Use On-Demand Instances in an EC2 Auto Scaling group to run the containers.",
"option_d": "Use On-Demand Instances in a managed node group on Amazon EKS.",
"correct_answers": ["A"],
"explanation_detailed": "Using Spot Instances in an EC2 Auto Scaling group is like snagging discounted last-minute tickets—if you can handle occasional interruptions, you’ll save a lot.",
"incorrect_explanations": {
"B": "While you could use Spot with EKS, it adds cluster management overhead compared to simpler ECS or plain EC2 for this question’s focus.",
"C": "On-Demand capacity costs more than Spot for interrupt-tolerant workloads.",
"D": "On-Demand EKS worker nodes do not minimize cost compared to Spot Instances."
}
},
{
"id": "saa-c03-design_high_performing_architectures-144",
"certification_id": "SAA-C03",
"domain": "DESIGN_HIGH_PERFORMING_ARCHITECTURES",
"difficulty": "medium",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "A company is migrating its multi-tier on-premises application to AWS to improve performance. The application’s tiers communicate via RESTful services, and transactions are dropped when a tier becomes overwhelmed. A solutions architect must design a solution that resolves these issues and modernizes the application. Which solution is the MOST operationally efficient?",
"option_a": "Use Amazon API Gateway and route transactions to Lambda functions as the application tier. Use Amazon SQS as the messaging layer between services.",
"option_b": "Use CloudWatch metrics to analyze historical performance and scale up EC2 instance size during peak usage.",
"option_c": "Use Amazon SNS for messaging between EC2 instances in an Auto Scaling group and scale based on SNS queue length.",
"option_d": "Use Amazon SQS for messaging between EC2 instances in an Auto Scaling group and scale based on SQS queue length.",
"correct_answers": ["A"],
"explanation_detailed": "This solution is like having a smart receptionist (API Gateway) who directs each customer to the right department (Lambda) and an internal messenger system (SQS) that ensures no transaction is lost, even under heavy load.",
"incorrect_explanations": {
"B": "Simply scaling up EC2 instances does not address queueing or decoupling between overloaded tiers.",
"C": "SNS is not a durable queue and is less suitable for handling back-pressure between services than SQS.",
"D": "Using SQS between EC2 instances still leaves you managing servers, which is less operationally efficient than using Lambda."
}
},
{
"id": "saa-c03-design_resilient_architectures-145",
"certification_id": "SAA-C03",
"domain": "DESIGN_RESILIENT_ARCHITECTURES",
"difficulty": "medium",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "A company stores sensitive customer transaction data in a DynamoDB table that must be retained for 7 years. Which solution is the MOST cost-effective?",
"option_a": "Configure global DynamoDB tables and point the application to another region for RPO recovery.",
"option_b": "Enable DynamoDB point-in-time recovery and restore to the desired point in time for RPO recovery.",
"option_c": "Export DynamoDB data to S3 Glacier daily and import from Glacier for RPO recovery.",
"option_d": "Schedule EBS snapshots for the DynamoDB table every 15 minutes and restore from them for RPO recovery.",
"correct_answers": ["B"],
"explanation_detailed": "Point-in-time recovery acts like a time machine for your data—allowing you to revert to a specific moment quickly if something goes wrong.",
"incorrect_explanations": {
"A": "Global tables are for multi-region active-active usage, not the simplest or cheapest way to meet retention and RPO requirements.",
"C": "Manual exports to Glacier and re-imports complicate the recovery process compared to built-in PITR.",
"D": "DynamoDB is a managed service and is not backed up via EBS snapshots."
}
},
{
"id": "saa-c03-design_secure_applications_architectures-146",
"certification_id": "SAA-C03",
"domain": "DESIGN_SECURE_APPLICATIONS_ARCHITECTURES",
"difficulty": "medium",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "A company is migrating its application to AWS. The application uses an EC2 instance that connects directly to an Aurora PostgreSQL database using locally stored credentials in a file. The company wants to reduce the overhead of credential management. What should be done?",
"option_a": "Use AWS Secrets Manager and enable automatic rotation.",
"option_b": "Use AWS Systems Manager Parameter Store and enable automatic rotation.",
"option_c": "Create an S3 bucket to store an encrypted credentials file using an AWS KMS key and point the application to it.",
"option_d": "Create an encrypted EBS volume for each EC2 instance, migrate the credentials file there, and point the application to it.",
"correct_answers": ["B"],
"explanation_detailed": "This solution is like storing your secret recipe in a high-security safe that automatically changes its combination—eliminating the need to manually update credentials.",
"incorrect_explanations": {
"A": "Secrets Manager is a strong choice but may be more feature-rich and costly than needed if the focus is simply reducing overhead via Parameter Store.",
"C": "Using S3 for credentials requires custom logic for encryption, retrieval, and rotation, increasing complexity.",
"D": "Encrypting EBS volumes protects data at rest but does not simplify or automate credential rotation."
}
},
{
"id": "saa-c03-design_cost_optimized_architectures-147",
"certification_id": "SAA-C03",
"domain": "DESIGN_COST_OPTIMIZED_ARCHITECTURES",
"difficulty": "medium",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "A development team needs to host a website that will be accessed by other teams. The website consists of HTML, CSS, client-side JavaScript, and images. Which method is the MOST cost-effective?",
"option_a": "Containerize the website and host it on AWS Fargate.",
"option_b": "Create an S3 bucket and host the website there.",
"option_c": "Deploy a web server on an EC2 instance to host the website.",
"option_d": "Configure an Application Load Balancer with a Lambda target using Express.js.",
"correct_answers": ["B"],
"explanation_detailed": "Hosting a static website on S3 is like having a warehouse that instantly delivers your brochures whenever someone asks—no need to manage a web server.",
"incorrect_explanations": {
"A": "Fargate adds container and orchestration cost for simple static content.",
"C": "EC2 requires ongoing management and typically costs more than S3 static hosting.",
"D": "ALB with Lambda is unnecessarily complex and expensive for static site delivery."
}
},
{
"id": "saa-c03-design_secure_applications_architectures-148",
"certification_id": "SAA-C03",
"domain": "DESIGN_SECURE_APPLICATIONS_ARCHITECTURES",
"difficulty": "medium",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "A company runs a multi-tier application on AWS and must track configuration changes to its resources and log an audit trail of API calls for compliance and security. What should be done?",
"option_a": "Use AWS CloudTrail to track configuration changes and AWS Config to record API calls.",
"option_b": "Use AWS Config to track configuration changes and AWS CloudTrail to record API calls.",
"option_c": "Use AWS Config to track configuration changes and Amazon CloudWatch to log API calls.",
"option_d": "Use AWS CloudTrail to track configuration changes and Amazon CloudWatch to log API calls.",
"correct_answers": ["B"],
"explanation_detailed": "This is like having both an inspector and a diary—Config monitors any changes while CloudTrail logs every API call for a full audit trail.",
"incorrect_explanations": {
"A": "CloudTrail is for API logging, not tracking detailed configuration state over time.",
"C": "CloudWatch can store logs but does not provide the dedicated API audit capability of CloudTrail.",
"D": "CloudTrail alone does not provide the configuration compliance view that Config offers."
}
},
{
"id": "saa-c03-design_secure_applications_architectures-149",
"certification_id": "SAA-C03",
"domain": "DESIGN_SECURE_APPLICATIONS_ARCHITECTURES",
"difficulty": "medium",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "A company registered its domain name with Amazon Route 53 and uses API Gateway in the ca-central-1 region as a public interface for its backend microservices. Third-party services securely consume the APIs. The company wants its API Gateway URL to include its domain name and corresponding certificate so that third-party services can use HTTPS. Which solution meets these requirements?",
"option_a": "Create stage variables in API Gateway with Name='Endpoint-URL' and Value='Company Domain' to override the default URL. Import the domain’s public certificate into ACM.",
"option_b": "Create Route 53 DNS records for the domain. Point an alias record to the Regional API Gateway stage endpoint. Import the certificate into ACM in us-east-1.",
"option_c": "Create a Regional API Gateway endpoint. Associate it with the company domain. Import the certificate into ACM in the same region. Attach the certificate to the endpoint and configure Route 53.",
"option_d": "Create a Regional API Gateway endpoint. Associate it with the company domain. Import the certificate into ACM in us-east-1. Attach the certificate to the API Gateway. Create Route 53 DNS records for the domain and point an A record to it.",
"correct_answers": ["D"],
"explanation_detailed": "This solution is like giving your online store its very own street address and security badge so that customers can connect with confidence using HTTPS.",
"incorrect_explanations": {
"A": "Stage variables do not configure the use of a custom domain and certificate for HTTPS.",
"B": "DNS aliasing alone does not fully configure the endpoint to use the custom certificate on API Gateway.",
"C": "The correct configuration here requires the specific ACM region and association described in option D."
}
},
{
"id": "saa-c03-design_secure_applications_architectures-150",
"certification_id": "SAA-C03",
"domain": "DESIGN_SECURE_APPLICATIONS_ARCHITECTURES",
"difficulty": "medium",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "A hospital’s social media website allows users to upload reports in PDF and JPEG formats. The hospital wants to ensure that the images do not contain inappropriate content. Which solution minimizes development effort while meeting these requirements?",
"option_a": "Use Amazon Comprehend to detect inappropriate content and perform human review for low-confidence predictions.",
"option_b": "Use Amazon Rekognition to detect inappropriate content and perform human review for low-confidence predictions.",
"option_c": "Use Amazon SageMaker to detect inappropriate content and employ ground truth for low-confidence predictions.",
"option_d": "Use AWS Fargate to deploy a custom ML model to detect inappropriate content and label low-confidence predictions.",
"correct_answers": ["B"],
"explanation_detailed": "Using Amazon Rekognition is like having an assistant that quickly scans each uploaded image for questionable content and flags any that might need a closer human look—saving you time and effort.",
"incorrect_explanations": {
"A": "Comprehend focuses on text analysis, not image content moderation.",
"C": "SageMaker requires building and operating custom ML models, which increases development effort.",
"D": "Deploying a custom ML model on Fargate requires significant model development and infrastructure management compared to a managed service."
}
},


{
"id": "saa-c03-domain-151",
"certification_id": "SAA-C03",
"domain": "DESIGN_HIGH_PERFORMING_ARCHITECTURES",
"difficulty": "medium",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "A company is designing an application that uses a Lambda function to receive data via API Gateway and store it in an Amazon Aurora PostgreSQL database. During the proof-of-concept, the company had to significantly increase Lambda quotas to handle high data volumes. A solutions architect must recommend a new design to improve scalability and reduce configuration overhead. Which solution meets these requirements?",
"option_a": "Refactor the Lambda function into Apache Tomcat code running on EC2 instances. Connect to the database using native JDBC drivers.",
"option_b": "Switch the database platform from Aurora to DynamoDB. Provision a DynamoDB Accelerator (DAX) cluster and update the API calls accordingly.",
"option_c": "Configure two Lambda functions—one to receive data and another to write data to the database. Integrate them using Amazon SNS.",
"option_d": "Configure two Lambda functions—one to receive data and another to write data to the database. Integrate them using an Amazon SQS queue.",
"correct_answers": ["D"],
"explanation_detailed": "Using Amazon SQS between the ingestion and write Lambda functions decouples request handling from database writes. This improves scalability, smooths traffic spikes, and reduces the need to tweak Lambda concurrency and quotas manually.",
"incorrect_explanations": {
"A": "Moving to Tomcat on EC2 increases operational overhead for capacity management, patching, and HA without inherently solving the bursty write pattern to the database.",
"B": "Switching to DynamoDB plus DAX changes the data model and API semantics and does not directly address the need to buffer and smooth write volume coming from the API layer.",
"C": "SNS is a pub/sub service and does not provide durable queuing or fine-grained control over back-pressure for a single consumer in the same way SQS does."
}
},
{
"id": "saa-c03-domain-152",
"certification_id": "SAA-C03",
"domain": "DESIGN_COST_OPTIMIZED_ARCHITECTURES",
"difficulty": "medium",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "A company wants to run critical containerized applications on AWS. The applications are stateless and can tolerate interruptions in the underlying infrastructure. The company wants to minimize both cost and operational overhead. What should a solutions architect do?",
"option_a": "Use Spot Instances in an EC2 Auto Scaling group to run the application containers.",
"option_b": "Use Spot Instances in a managed node group on Amazon EKS.",
"option_c": "Use On-Demand Instances in an EC2 Auto Scaling group to run the application containers.",
"option_d": "Use On-Demand Instances in a managed node group on Amazon EKS.",
"correct_answers": ["A"],
"explanation_detailed": "Running containers on Spot Instances in an Auto Scaling group leverages steep Spot discounts while allowing the stateless, interruption-tolerant applications to recover when instances are reclaimed, minimizing compute cost with manageable operational effort.",
"incorrect_explanations": {
"B": "Using EKS managed node groups adds cluster management overhead and complexity compared to directly running containers on EC2 in an Auto Scaling group.",
"C": "On-Demand Instances remove interruption risk but are significantly more expensive than Spot, which the workload does not require.",
"D": "On-Demand EKS worker nodes increase both cost and operational complexity relative to a simpler EC2 Auto Scaling + container orchestration approach."
}
},
{
"id": "saa-c03-domain-153",
"certification_id": "SAA-C03",
"domain": "DESIGN_SECURE_APPLICATIONS_ARCHITECTURES",
"difficulty": "easy",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "A company is reviewing its AWS deployment to ensure that its Amazon S3 buckets have not had any unauthorized configuration changes. What should a solutions architect do?",
"option_a": "Enable AWS Config with appropriate rules.",
"option_b": "Enable AWS Trusted Advisor with the appropriate checks.",
"option_c": "Enable Amazon Inspector with an appropriate assessment template.",
"option_d": "Enable S3 server access logging and configure Amazon EventBridge (CloudWatch Events).",
"correct_answers": ["A"],
"explanation_detailed": "AWS Config continuously records configuration changes for supported resources, including S3 buckets, and can evaluate them against rules to detect and alert on unauthorized changes over time.",
"incorrect_explanations": {
"B": "Trusted Advisor provides periodic best-practice checks but does not continuously track or record detailed configuration changes for S3 buckets.",
"C": "Amazon Inspector focuses on vulnerability assessments for compute resources, not on configuration drift of S3 buckets.",
"D": "Server access logging records object-level access requests, not configuration changes such as bucket policies, ACLs, or encryption settings."
}
},
{
"id": "saa-c03-domain-154",
"certification_id": "SAA-C03",
"domain": "DESIGN_SECURE_APPLICATIONS_ARCHITECTURES",
"difficulty": "medium",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "A company is developing a two-tier web application on AWS. The developers deployed the application on an EC2 instance that connects directly to an Amazon RDS database. The company must not hard-code database credentials into the application, and it must implement automatic rotation of these credentials. What solution meets these requirements with the LOWEST operational overhead?",
"option_a": "Store database credentials in instance metadata and use an EventBridge rule to run a scheduled Lambda function that updates both the RDS credentials and instance metadata simultaneously.",
"option_b": "Store the credentials in a configuration file in an S3 bucket (encrypted) and use an EventBridge rule to run a scheduled Lambda function that updates both the RDS credentials and the configuration file. Enable S3 versioning.",
"option_c": "Store the database credentials as a secret in AWS Secrets Manager. Enable automatic rotation for the secret. Attach the necessary permissions to the EC2 role.",
"option_d": "Store the credentials as encrypted parameters in AWS Systems Manager Parameter Store. Enable automatic rotation and attach necessary permissions to the EC2 role.",
"correct_answers": ["C"],
"explanation_detailed": "AWS Secrets Manager is purpose-built for managing secrets. It integrates directly with RDS for automatic credential rotation and allows the EC2 instance to retrieve credentials securely via its IAM role, minimizing custom management.",
"incorrect_explanations": {
"A": "Instance metadata is not designed to store long-lived secrets and requires custom logic both for rotation and secure update, increasing operational overhead.",
"B": "Managing encrypted configuration files in S3 plus rotation logic in Lambda requires significant custom code and coordination, which increases complexity compared to a managed secrets service.",
"D": "While Parameter Store can hold secure strings, automatic rotation for database credentials is not as tightly integrated or turnkey as Secrets Manager’s native RDS rotation support."
}
},
{
"id": "saa-c03-domain-155",
"certification_id": "SAA-C03",
"domain": "DESIGN_SECURE_APPLICATIONS_ARCHITECTURES",
"difficulty": "medium",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "A company is launching a new public-facing web application on AWS. The application runs behind an Application Load Balancer (ALB) and requires edge-termination of SSL/TLS with a certificate issued by an external CA. The certificate must be rotated annually before expiration. What should a solutions architect do?",
"option_a": "Use AWS Certificate Manager (ACM) to issue an SSL/TLS certificate. Attach it to the ALB. Use managed renewal to automatically rotate the certificate.",
"option_b": "Use ACM to issue an SSL/TLS certificate. Import the certificate’s key material. Attach it to the ALB. Use managed renewal for automatic rotation.",
"option_c": "Use ACM Private Certificate Authority to issue an SSL/TLS certificate from a root CA. Attach it to the ALB. Use managed renewal for automatic rotation.",
"option_d": "Use ACM to import an SSL/TLS certificate. Attach it to the ALB. Use Amazon EventBridge (CloudWatch Events) to notify when the certificate is nearing expiration and rotate it manually.",
"correct_answers": ["D"],
"explanation_detailed": "Because the certificate is issued by an external CA, it must be imported into ACM. ACM cannot automatically renew externally issued certificates, so using EventBridge notifications to trigger manual rotation is required.",
"incorrect_explanations": {
"A": "This option ignores the requirement to use a certificate issued by an external CA and instead issues the certificate directly from ACM.",
"B": "ACM cannot automatically renew imported certificates from external CAs, so managed renewal is not available for this case.",
"C": "ACM Private CA is used to create a private PKI, not to manage or rotate a public certificate provided by an external CA."
}
},
{
"id": "saa-c03-domain-156",
"certification_id": "SAA-C03",
"domain": "DESIGN_COST_OPTIMIZED_ARCHITECTURES",
"difficulty": "medium",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "A development team runs monthly tests on an Amazon RDS for MySQL instance with Performance Insights enabled. The tests last 48 hours once a month and are the only process using the database. The team wants to reduce the test cost without reducing the instance’s compute and memory attributes. What is the MOST economical solution?",
"option_a": "Stop the RDS instance after tests and restart it when needed.",
"option_b": "Use Auto Scaling on the RDS instance to scale automatically after tests.",
"option_c": "Create a snapshot after tests are complete. Shut down the RDS instance and restore the snapshot when needed.",
"option_d": "Resize the RDS instance to a smaller instance type after tests and resize it again when needed.",
"correct_answers": ["C"],
"explanation_detailed": "Creating a snapshot after the tests and deleting or shutting down the instance eliminates ongoing instance charges, leaving only inexpensive storage for the snapshot. Before the next test, the team restores from the snapshot with the desired instance size.",
"incorrect_explanations": {
"A": "Stopping an RDS instance only helps for short periods and still incurs storage charges for the instance; it is not ideal for long idle periods between monthly tests.",
"B": "RDS Auto Scaling does not directly resize instance classes in the way described here, and scaling up and down continuously is unnecessary for a simple monthly test pattern.",
"D": "Manually resizing the instance after each test and again before the next adds operational overhead and can incur downtime, without reducing costs as much as deleting and restoring from a snapshot."
}
},
{
"id": "saa-c03-domain-157",
"certification_id": "SAA-C03",
"domain": "DESIGN_COST_OPTIMIZED_ARCHITECTURES",
"difficulty": "easy",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "A company that hosts its website on AWS wants to ensure that all its EC2 instances, RDS database instances, and Redshift clusters are tagged. The company wants to minimize the effort of configuring and operating this check. What should be done?",
"option_a": "Use AWS Config rules to define and detect resources that are not properly tagged.",
"option_b": "Use Cost Explorer to display untagged resources and manually tag them.",
"option_c": "Write API calls to check resource tagging and run them periodically on an EC2 instance.",
"option_d": "Write API calls to check resource tagging and schedule a Lambda function via CloudWatch to run the checks periodically.",
"correct_answers": ["A"],
"explanation_detailed": "AWS Config rules can continuously evaluate resource configurations, including required tags, and surface non-compliant resources without custom code or periodic jobs.",
"incorrect_explanations": {
"B": "Cost Explorer can show untagged costs but does not enforce or continuously evaluate tagging compliance across all resources.",
"C": "Custom scripts on EC2 increase operational overhead for scheduling, monitoring, and maintenance compared to a managed rules engine.",
"D": "Using Lambda with custom API logic requires building and maintaining evaluation logic instead of leveraging AWS Config’s built-in tagging rules."
}
},
{
"id": "saa-c03-domain-158",
"certification_id": "SAA-C03",
"domain": "DESIGN_COST_OPTIMIZED_ARCHITECTURES",
"difficulty": "easy",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "A development team needs to host a website that will be accessed by other teams. The website content includes HTML, CSS, client-side JavaScript, and images. Which method is the MOST cost-effective?",
"option_a": "Containerize the website and host it on AWS Fargate.",
"option_b": "Create an S3 bucket and host the website there.",
"option_c": "Deploy a web server on an EC2 instance to host the website.",
"option_d": "Configure an Application Load Balancer with a Lambda target using Express.js.",
"correct_answers": ["B"],
"explanation_detailed": "Amazon S3 static website hosting is ideal for HTML, CSS, JavaScript, and images. It removes server management and typically costs far less than running containers, EC2 instances, or load balancers.",
"incorrect_explanations": {
"A": "Fargate is serverless for containers but still incurs per-task compute charges and is unnecessary for a simple static website.",
"C": "Running a dedicated EC2 instance introduces ongoing compute, patching, and scaling considerations that are overkill for static content.",
"D": "Using an Application Load Balancer and Lambda for static content adds complexity and cost with no benefit over S3 hosting."
}
},
{
"id": "saa-c03-domain-159",
"certification_id": "SAA-C03",
"domain": "DESIGN_RESILIENT_ARCHITECTURES",
"difficulty": "medium",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "A solutions architect is designing a workflow for a new application that must be event-driven and serverless. The workflow should be loosely coupled and use event-based components to process tasks in parallel. Which design should the architect use?",
"option_a": "Build the workflow using AWS Glue to invoke Lambda functions for processing each step.",
"option_b": "Build the workflow using AWS Step Functions, deploy the application on EC2, and use Step Functions to invoke processing steps on the EC2 instances.",
"option_c": "Build the workflow using Amazon EventBridge to invoke Lambda functions on a schedule for processing each step.",
"option_d": "Build the workflow using AWS Step Functions to create a state machine that invokes Lambda functions for processing each step.",
"correct_answers": ["D"],
"explanation_detailed": "AWS Step Functions orchestrating Lambda functions provides a fully serverless, event-driven, and loosely coupled workflow where each step can run in parallel or sequence with managed state and retries.",
"incorrect_explanations": {
"A": "AWS Glue is primarily designed for ETL workloads, not for general-purpose orchestration of arbitrary serverless workflows.",
"B": "Running processing on EC2 instances reintroduces server management and reduces the serverless benefits of the design.",
"C": "Using EventBridge with scheduled rules drives time-based invocations rather than modeling the full workflow state and parallel branches."
}
},
{
"id": "saa-c03-domain-160",
"certification_id": "SAA-C03",
"domain": "DESIGN_SECURE_APPLICATIONS_ARCHITECTURES",
"difficulty": "easy",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "A developer has an application that uses a Lambda function to upload files to Amazon S3 and needs the necessary permissions to do so. The developer already has an IAM user with valid S3 credentials. What should a solutions architect do to grant the required permissions?",
"option_a": "Add the necessary IAM permissions to the Lambda function’s resource policy.",
"option_b": "Create a presigned request using the existing IAM credentials within the Lambda function.",
"option_c": "Create a new IAM user and use its credentials in the Lambda function.",
"option_d": "Create an IAM execution role with the necessary permissions and attach it to the Lambda function.",
"correct_answers": ["D"],
"explanation_detailed": "The recommended pattern is to grant AWS Lambda an IAM execution role with the required S3 permissions. Lambda assumes this role at runtime, avoiding the need to embed any user credentials.",
"incorrect_explanations": {
"A": "A Lambda resource-based policy controls who can invoke the function, not what AWS actions the function itself can perform.",
"B": "Embedding long-term IAM user credentials in function code is insecure and hard to rotate compared to using an execution role.",
"C": "Creating another IAM user does not solve the credential management problem and goes against best practices of using roles for AWS services."
}
},
{
"id": "saa-c03-domain-161",
"certification_id": "SAA-C03",
"domain": "DESIGN_RESILIENT_ARCHITECTURES",
"difficulty": "medium",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "A company deployed a serverless application that invokes a Lambda function when new documents are uploaded to an S3 bucket. The Lambda function processes the documents. After a recent marketing campaign, the company noticed that many documents were not processed. What should be done to improve the application architecture?",
"option_a": "Increase the Lambda function’s timeout value to 15 minutes.",
"option_b": "Configure an S3 bucket replication policy and prepare the documents for later processing.",
"option_c": "Deploy an additional Lambda function and distribute document processing between the two functions.",
"option_d": "Create an Amazon SQS queue. Send document upload events to the queue and configure the queue as an event source for Lambda.",
"correct_answers": ["D"],
"explanation_detailed": "Introducing Amazon SQS between S3 and Lambda decouples ingestion from processing, providing durable buffering and allowing Lambda to scale with demand without losing events during high-volume campaigns.",
"incorrect_explanations": {
"A": "Increasing the timeout may help long-running executions but does not address event loss or throttling under sudden spikes in invocation volume.",
"B": "S3 replication moves objects between buckets or Regions but does not provide queuing, back-pressure handling, or guaranteed processing semantics.",
"C": "Adding another Lambda function without a queue still risks event loss under high concurrency and does not solve the need for durable buffering."
}
},
{
"id": "saa-c03-domain-162",
"certification_id": "SAA-C03",
"domain": "DESIGN_RESILIENT_ARCHITECTURES",
"difficulty": "hard",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "A company is designing a disaster recovery (DR) architecture for its on-premises MySQL database running on an EC2 instance in a private subnet with scheduled backups. The DR design must include multiple AWS regions. Which solution meets these requirements with the LOWEST operational overhead?",
"option_a": "Migrate the MySQL database to multiple EC2 instances. Configure a standby EC2 instance in the DR region. Enable replication.",
"option_b": "Migrate the MySQL database to Amazon RDS. Use a Multi-AZ deployment. Enable read replica replication for the primary instance in different Availability Zones.",
"option_c": "Migrate the MySQL database to an Amazon Aurora global database. Host the primary cluster in the primary region and the secondary cluster in the DR region.",
"option_d": "Store the scheduled backups in an S3 bucket configured for cross-region replication. Use the backup to restore the database in the DR region.",
"correct_answers": ["C"],
"explanation_detailed": "An Amazon Aurora global database provides managed, low-latency replication across Regions with automated failover options. It delivers multi-Region DR with significantly less operational overhead than managing EC2-based replication or ad-hoc snapshot restores.",
"incorrect_explanations": {
"A": "Managing multiple EC2-hosted MySQL instances and cross-Region replication requires substantial administration, monitoring, and failover orchestration.",
"B": "RDS Multi-AZ and read replicas across Availability Zones improve availability within a single Region but do not provide a full multi-Region DR solution.",
"D": "Relying only on replicated backups means the DR database must be restored manually, increasing RTO and operational effort compared to a continuously replicated secondary cluster."
}
},
{
"id": "saa-c03-domain-163",
"certification_id": "SAA-C03",
"domain": "DESIGN_HIGH_PERFORMING_ARCHITECTURES",
"difficulty": "medium",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "A company’s Java application uses Amazon SQS to process messages but cannot handle messages larger than 256 KB. The company wants the application to handle messages up to 50 MB with minimal code changes. Which solution meets these requirements?",
"option_a": "Use the Amazon SQS Extended Client Library for Java to store messages larger than 256 KB in Amazon S3.",
"option_b": "Use Amazon EventBridge to post larger messages instead of SQS.",
"option_c": "Increase the message size limit in Amazon SQS to over 256 KB.",
"option_d": "Store messages larger than 256 KB in Amazon EFS and have SQS reference that location.",
"correct_answers": ["A"],
"explanation_detailed": "The Amazon SQS Extended Client Library for Java automatically offloads large payloads to S3 while sending a pointer through SQS, allowing the existing SQS workflow to support messages up to 50 MB with minimal code changes.",
"incorrect_explanations": {
"B": "EventBridge has its own event size limits and is not a drop-in replacement for SQS in existing queue-based processing architectures.",
"C": "You cannot arbitrarily increase the maximum message size in SQS beyond the documented service limits.",
"D": "Using EFS would require custom integration and does not integrate natively with SQS for large message offloading."
}
},
{
"id": "saa-c03-domain-164",
"certification_id": "SAA-C03",
"domain": "DESIGN_SECURE_APPLICATIONS_ARCHITECTURES",
"difficulty": "easy",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "A company wants to restrict public access to objects stored in its Amazon S3 buckets. All objects in the account must remain private. Which solution meets these requirements?",
"option_a": "Use Amazon GuardDuty to monitor S3 bucket policies. Create an automated correction rule using a Lambda function to fix any changes that make objects public.",
"option_b": "Use AWS Trusted Advisor to find publicly accessible S3 buckets. Set up email notifications and manually correct the bucket policies.",
"option_c": "Use AWS Resource Access Manager to find publicly accessible S3 buckets. Use Amazon SNS to invoke a Lambda function when a change is detected and correct it programmatically.",
"option_d": "Enable S3 Block Public Access at the account level and use AWS Organizations to create a service control policy (SCP) that prevents IAM users from modifying this setting.",
"correct_answers": ["D"],
"explanation_detailed": "Enabling S3 Block Public Access at the account level and enforcing it with an SCP ensures that no buckets or objects in the account can be made public, and that users cannot disable these protective settings.",
"incorrect_explanations": {
"A": "GuardDuty is focused on threat detection, not on enforcing S3 access policies or preventing public exposure by design.",
"B": "Trusted Advisor can alert on public buckets but still requires manual remediation and does not prevent future misconfigurations.",
"C": "AWS Resource Access Manager is used for resource sharing across accounts, not for detecting or enforcing S3 public access posture."
}
},
{
"id": "saa-c03-domain-165",
"certification_id": "SAA-C03",
"domain": "DESIGN_COST_OPTIMIZED_ARCHITECTURES",
"difficulty": "easy",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "A development team needs to host a website that will be accessed by other teams. The website consists of HTML, CSS, client-side JavaScript, and images. Which method is the MOST cost-effective?",
"option_a": "Containerize the website and host it on AWS Fargate.",
"option_b": "Create an S3 bucket and host the website there.",
"option_c": "Deploy a web server on an EC2 instance to host the website.",
"option_d": "Configure an Application Load Balancer with a Lambda target using Express.js.",
"correct_answers": ["B"],
"explanation_detailed": "Serving static website assets directly from Amazon S3 is the simplest and least expensive approach, eliminating server management and load balancer costs.",
"incorrect_explanations": {
"A": "Fargate introduces per-task compute costs and orchestration complexity that provide no benefit for a purely static site.",
"C": "Running an EC2 instance for static content incurs continuous compute charges and operational overhead for patching and scaling.",
"D": "Using an ALB with Lambda is more complex and costly than necessary for delivering static HTML, CSS, and JavaScript."
}
},
{
"id": "saa-c03-domain-166",
"certification_id": "SAA-C03",
"domain": "DESIGN_RESILIENT_ARCHITECTURES",
"difficulty": "medium",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "A solutions architect is designing a workflow for a new data processing application that must run in parallel, adding and removing application nodes based on the number of jobs to process. The processing application is stateless. The architect must ensure that the application is loosely coupled and that job items are stored durably. Which design should be used?",
"option_a": "Create an SNS topic for job submission. Create an AMI containing the processing application, then launch an Auto Scaling group using a launch configuration. Scale based on CPU usage.",
"option_b": "Create an SQS queue for job storage. Create an AMI with the processing application, then launch an Auto Scaling group using a launch configuration. Scale based on network usage.",
"option_c": "Create an SQS queue for job storage. Create an AMI with the processing application, then launch an Auto Scaling group using a launch template. Scale based on the number of items in the SQS queue.",
"option_d": "Create an SNS topic for job submission. Create an AMI with the processing application, then launch an Auto Scaling group using a launch template. Scale based on the number of messages published to the SNS topic.",
"correct_answers": ["C"],
"explanation_detailed": "Durably storing jobs in SQS and scaling an Auto Scaling group based on the queue depth decouples producers and consumers, provides reliable buffering, and aligns capacity with workload demand.",
"incorrect_explanations": {
"A": "SNS is a pub/sub service and does not provide durable queuing or straightforward backlog-based scaling.",
"B": "Scaling based on network usage is an indirect signal and does not precisely reflect the number of jobs waiting to be processed.",
"D": "SNS does not maintain a message backlog for consumption, making it unsuitable for driving scale directly from the number of published messages."
}
},
{
"id": "saa-c03-domain-167",
"certification_id": "SAA-C03",
"domain": "DESIGN_SECURE_APPLICATIONS_ARCHITECTURES",
"difficulty": "medium",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "A company’s security audit reveals that EC2 instances are not being patched regularly. A solutions architect must provide a solution to perform regular security checks on a large fleet of EC2 instances, automatically patch them on a schedule, and provide a remediation report. What should be done?",
"option_a": "Configure Amazon Macie to scan the EC2 instances for software vulnerabilities. Schedule a cron job on each instance to patch them regularly.",
"option_b": "Enable Amazon GuardDuty and configure it to check EC2 instances for vulnerabilities. Use AWS Systems Manager Session Manager to patch them on a schedule.",
"option_c": "Configure Amazon Detective to scan the EC2 instances for vulnerabilities. Create an EventBridge rule to trigger remediation.",
"option_d": "Enable Amazon Inspector in the account. Configure Inspector to assess EC2 instances for vulnerabilities. Use AWS Systems Manager Patch Manager to patch the instances on a schedule.",
"correct_answers": ["D"],
"explanation_detailed": "Amazon Inspector continuously assesses EC2 instances for vulnerabilities, and AWS Systems Manager Patch Manager can automatically apply patches on a defined schedule and generate compliance reports, addressing both detection and remediation.",
"incorrect_explanations": {
"A": "Amazon Macie focuses on discovering and protecting sensitive data in S3, not on scanning EC2 instances for software vulnerabilities.",
"B": "GuardDuty detects malicious activity and threats but does not perform detailed vulnerability assessments or orchestrate patching.",
"C": "Amazon Detective is used for security investigation and analysis, not for vulnerability scanning or automated patch deployment."
}
},
{
"id": "saa-c03-domain-168",
"certification_id": "SAA-C03",
"domain": "DESIGN_HIGH_PERFORMING_ARCHITECTURES",
"difficulty": "medium",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "A company is migrating its on-premises data center to AWS. It needs to move 20 TB of data within 30 days. The company’s network bandwidth is limited to 15 Mbps and cannot exceed 70% utilization. What should be done to meet these requirements?",
"option_a": "Use AWS Snowball.",
"option_b": "Use AWS DataSync.",
"option_c": "Use AWS Database Migration Service (AWS DMS) over the public internet.",
"option_d": "Use Amazon S3 Transfer Acceleration.",
"correct_answers": ["A"],
"explanation_detailed": "For tens of terabytes of data with constrained and limited bandwidth, AWS Snowball provides a physical transfer mechanism that bypasses the network and reliably meets the migration timeline.",
"incorrect_explanations": {
"B": "DataSync still depends on available network bandwidth, which is insufficient to transfer 20 TB within the deadline without exceeding the utilization constraint.",
"C": "DMS is designed for database migration and continuous replication scenarios, not bulk file transfer at this scale under tight bandwidth limits.",
"D": "S3 Transfer Acceleration speeds up transfers over the internet but cannot overcome the hard throughput limitation of the company’s network link."
}
},
{
"id": "saa-c03-domain-169",
"certification_id": "SAA-C03",
"domain": "DESIGN_RESILIENT_ARCHITECTURES",
"difficulty": "medium",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "A company’s online education platform stores student records in a PostgreSQL database. The company needs a solution to keep these records available and online in multiple AWS regions at all times. Which solution meets these requirements with the LEAST operational overhead?",
"option_a": "Migrate the PostgreSQL database to a PostgreSQL cluster running on EC2 instances.",
"option_b": "Migrate the PostgreSQL database to an Amazon RDS for PostgreSQL instance with Multi-AZ enabled.",
"option_c": "Migrate the PostgreSQL database to an Amazon RDS for PostgreSQL instance and create a read replica in another region.",
"option_d": "Migrate the PostgreSQL database to an Amazon RDS for PostgreSQL instance and configure database snapshots to be copied to another region.",
"correct_answers": ["C"],
"explanation_detailed": "Using Amazon RDS for PostgreSQL with a cross-Region read replica provides a continuously updated copy of the data in another Region with minimal administration, supporting multi-Region availability.",
"incorrect_explanations": {
"A": "Managing PostgreSQL on EC2 requires handling backups, replication, failover, and patching manually, which increases operational overhead.",
"B": "Multi-AZ deployments improve availability within a single Region but do not provide an online copy in another Region.",
"D": "Cross-Region snapshot copies support backup and restore but do not provide continuously online, low-RPO replicas for immediate use."
}
},
{
"id": "saa-c03-domain-170",
"certification_id": "SAA-C03",
"domain": "DESIGN_COST_OPTIMIZED_ARCHITECTURES",
"difficulty": "easy",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "A company hosts its web application on AWS. The company wants to ensure that all EC2 instances, RDS database instances, and Redshift clusters are tagged. The company wishes to minimize the configuration and operational effort of this check. What should be done?",
"option_a": "Use AWS Config rules to define and detect untagged resources.",
"option_b": "Use Cost Explorer to list untagged resources and tag them manually.",
"option_c": "Write API calls to check tagging and run them periodically on an EC2 instance.",
"option_d": "Write API calls to check tagging and schedule a Lambda function via CloudWatch to run them periodically.",
"correct_answers": ["A"],
"explanation_detailed": "AWS Config rules can continuously evaluate whether required tags are present on specified resource types and flag non-compliant resources without custom scripting.",
"incorrect_explanations": {
"B": "Cost Explorer can show untagged cost allocation but does not enforce tagging policies or continuously monitor resource compliance.",
"C": "Running custom scripts on EC2 adds management overhead for the instances, scheduling, and code maintenance.",
"D": "Lambda plus custom API calls require building and maintaining the evaluation logic instead of relying on AWS Config’s managed rules."
}
},
{
"id": "saa-c03-domain-171",
"certification_id": "SAA-C03",
"domain": "DESIGN_COST_OPTIMIZED_ARCHITECTURES",
"difficulty": "easy",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "A development team needs to host a website that will be accessed by other teams. The website content consists of HTML, CSS, client-side JavaScript, and images. Which method is the MOST cost-effective?",
"option_a": "Containerize the website and host it on AWS Fargate.",
"option_b": "Create an S3 bucket and host the website there.",
"option_c": "Deploy a web server on an EC2 instance to host the website.",
"option_d": "Configure an Application Load Balancer with a Lambda target using Express.js.",
"correct_answers": ["B"],
"explanation_detailed": "Hosting static HTML, CSS, JavaScript, and images in Amazon S3 is inexpensive and removes the need to provision or manage compute resources.",
"incorrect_explanations": {
"A": "Fargate adds per-task compute costs and orchestration complexity that are unnecessary for a simple static website.",
"C": "An EC2-based web server incurs ongoing compute charges and administration effort even when traffic is low.",
"D": "An ALB with Lambda introduces more components and cost than required to serve static files."
}
},
{
"id": "saa-c03-domain-172",
"certification_id": "SAA-C03",
"domain": "DESIGN_RESILIENT_ARCHITECTURES",
"difficulty": "medium",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "A solutions architect is designing a workflow for a new application that must be event-driven and serverless. The workflow should be loosely coupled and process tasks in parallel. Which design should be used?",
"option_a": "Build the workflow using AWS Glue to invoke Lambda functions for each step.",
"option_b": "Build the workflow using AWS Step Functions, deploy the application on EC2, and invoke processing steps on EC2.",
"option_c": "Build the workflow using Amazon EventBridge to invoke Lambda functions on a schedule.",
"option_d": "Build the workflow using AWS Step Functions to create a state machine that invokes Lambda functions for each processing step.",
"correct_answers": ["D"],
"explanation_detailed": "AWS Step Functions orchestrating Lambda functions provides a fully serverless, event-driven state machine where tasks can be run in parallel or sequence with managed error handling and retries.",
"incorrect_explanations": {
"A": "AWS Glue targets ETL workloads rather than general application orchestration across multiple types of steps.",
"B": "Placing the core processing logic on EC2 reduces the benefits of a serverless architecture and introduces server management overhead.",
"C": "Using EventBridge strictly on a schedule does not model multi-step workflows or complex branching logic as effectively as Step Functions."
}
},
{
"id": "saa-c03-domain-173",
"certification_id": "SAA-C03",
"domain": "DESIGN_SECURE_APPLICATIONS_ARCHITECTURES",
"difficulty": "easy",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "A developer has an application that uses a Lambda function to upload files to S3 and needs the proper permissions to do so. The developer already has an IAM user with valid S3 credentials. What should be done to grant the necessary permissions?",
"option_a": "Add the required IAM permissions to the Lambda function’s resource policy.",
"option_b": "Create a presigned request using the existing IAM credentials in the Lambda function.",
"option_c": "Create a new IAM user and use its credentials in the Lambda function.",
"option_d": "Create an IAM execution role with the required permissions and attach it to the Lambda function.",
"correct_answers": ["D"],
"explanation_detailed": "Assigning an IAM execution role to the Lambda function with the needed S3 permissions is the recommended, secure way to authorize S3 operations without embedding user credentials.",
"incorrect_explanations": {
"A": "A resource-based policy on the function controls who can invoke it, not what AWS resources it can access during execution.",
"B": "Using long-term IAM user credentials inside the code is insecure and makes rotation difficult compared to using roles.",
"C": "Creating another IAM user repeats the same credential management problem instead of using a role that Lambda can assume automatically."
}
},
{
"id": "saa-c03-domain-174",
"certification_id": "SAA-C03",
"domain": "DESIGN_RESILIENT_ARCHITECTURES",
"difficulty": "medium",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "A company deployed a serverless application that invokes a Lambda function when new documents are uploaded to an S3 bucket. The Lambda function processes the documents. After a recent marketing campaign, many documents went unprocessed. What should be done to improve the application’s architecture?",
"option_a": "Increase the Lambda function’s timeout value to 15 minutes.",
"option_b": "Configure an S3 bucket replication policy and prepare documents for later processing.",
"option_c": "Deploy an additional Lambda function and distribute the processing load between the two functions.",
"option_d": "Create an Amazon SQS queue. Send document upload events to the queue and configure the queue as an event source for Lambda.",
"correct_answers": ["D"],
"explanation_detailed": "By placing an SQS queue between S3 and Lambda, document events are durably buffered. Lambda can then scale consumption based on queue depth, preventing event loss during spikes.",
"incorrect_explanations": {
"A": "Increasing the timeout does not address the underlying issue of dropped or throttled invocations when many events arrive simultaneously.",
"B": "S3 replication simply copies objects to another bucket and does not guarantee that the application processes each upload event.",
"C": "Adding another Lambda function without introducing a queue still risks missing events when invocation limits are exceeded."
}
},
{
"id": "saa-c03-domain-175",
"certification_id": "SAA-C03",
"domain": "DESIGN_RESILIENT_ARCHITECTURES",
"difficulty": "hard",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "A company is designing a disaster recovery (DR) architecture for its on-premises MySQL database running on an EC2 instance in a private subnet with scheduled backups. The DR design must span multiple AWS regions. Which solution meets these requirements with the LOWEST operational overhead?",
"option_a": "Migrate the MySQL database to multiple EC2 instances. Configure a standby EC2 instance in the DR region. Enable replication.",
"option_b": "Migrate the MySQL database to Amazon RDS. Use a Multi-AZ deployment. Enable read replica replication for the primary instance in different AZs.",
"option_c": "Migrate the MySQL database to an Amazon Aurora global database. Host the primary cluster in the primary region and the secondary cluster in the DR region.",
"option_d": "Store the scheduled backups in an S3 bucket configured for cross-region replication and restore the database from the backup in the DR region.",
"correct_answers": ["C"],
"explanation_detailed": "An Amazon Aurora global database provides managed cross-Region replication with low RPO and automated tooling for failover, dramatically reducing the operational burden of implementing multi-Region DR.",
"incorrect_explanations": {
"A": "Managing native MySQL replication and failover across EC2 instances in multiple Regions requires significant custom configuration and operational oversight.",
"B": "RDS Multi-AZ and cross-AZ replicas enhance availability within one Region but do not create an active database in a different Region.",
"D": "Relying only on replicated backups requires manual restore operations during a disaster, resulting in higher recovery time and more manual work."
}
},
{
"id": "saa-c03-domain-176",
"certification_id": "SAA-C03",
"domain": "DESIGN_HIGH_PERFORMING_ARCHITECTURES",
"difficulty": "medium",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "A company’s Java application uses Amazon SQS to process messages but cannot handle messages larger than 256 KB. The company wants the application to handle messages up to 50 MB with minimal code changes. Which solution meets these requirements?",
"option_a": "Use the Amazon SQS Extended Client Library for Java to store messages larger than 256 KB in Amazon S3.",
"option_b": "Use Amazon EventBridge to post larger messages instead of SQS.",
"option_c": "Increase the message size limit in Amazon SQS to handle messages larger than 256 KB.",
"option_d": "Store messages larger than 256 KB in Amazon EFS and have SQS reference that location.",
"correct_answers": ["A"],
"explanation_detailed": "The SQS Extended Client Library for Java automatically uploads large payloads to S3 and sends a reference in the SQS message, allowing the existing SQS-based design to scale to much larger messages with little code change.",
"incorrect_explanations": {
"B": "EventBridge has different semantics and size limits and would require re-architecting the application away from a queue-based model.",
"C": "SQS message size limits are fixed by the service and cannot be arbitrarily raised beyond the documented maximum.",
"D": "Using EFS for payload storage with SQS would require custom integration logic and is not supported by a managed library as directly as S3."
}
},
{
"id": "saa-c03-domain-177",
"certification_id": "SAA-C03",
"domain": "DESIGN_SECURE_APPLICATIONS_ARCHITECTURES",
"difficulty": "easy",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "A company wants to restrict public access to objects stored in its Amazon S3 buckets. All objects in the account must remain private. Which solution meets these requirements?",
"option_a": "Use Amazon GuardDuty to monitor bucket policies. Create an automated correction rule using Lambda to fix any changes that make objects public.",
"option_b": "Use AWS Trusted Advisor to identify publicly accessible buckets, then manually correct the policies when notified.",
"option_c": "Use AWS Resource Access Manager to find publicly accessible buckets and trigger a Lambda function via SNS to correct them.",
"option_d": "Enable S3 Block Public Access at the account level and use AWS Organizations to create a service control policy (SCP) that prevents IAM users from modifying this setting.",
"correct_answers": ["D"],
"explanation_detailed": "Account-level S3 Block Public Access combined with an SCP that prevents disabling it ensures that no S3 bucket or object in the account can be made publicly accessible.",
"incorrect_explanations": {
"A": "GuardDuty is for threat detection and does not enforce or lock down S3 public access settings by itself.",
"B": "Trusted Advisor can detect public buckets but relies on manual remediation and does not prevent new misconfigurations.",
"C": "AWS Resource Access Manager deals with resource sharing, not with enforcing S3 access controls or scanning for public exposure."
}
},
{
"id": "saa-c03-domain-178",
"certification_id": "SAA-C03",
"domain": "DESIGN_COST_OPTIMIZED_ARCHITECTURES",
"difficulty": "easy",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "A development team needs to host a website that will be accessed by other teams. The website consists of HTML, CSS, client-side JavaScript, and images. Which method is the MOST cost-effective?",
"option_a": "Containerize the website and host it on AWS Fargate.",
"option_b": "Create an S3 bucket and host the website there.",
"option_c": "Deploy a web server on an EC2 instance to host the website.",
"option_d": "Configure an Application Load Balancer with a Lambda target using Express.js.",
"correct_answers": ["B"],
"explanation_detailed": "S3 static website hosting removes the need for any server infrastructure and is highly cost-efficient for serving static assets such as HTML, CSS, and JavaScript.",
"incorrect_explanations": {
"A": "Fargate introduces compute and orchestration overhead that is unnecessary for static content hosting.",
"C": "An EC2 server requires ongoing management and incurs compute charges even when the site is idle.",
"D": "An ALB with Lambda adds complexity and expense compared to simple S3 hosting for static sites."
}
},
{
"id": "saa-c03-domain-179",
"certification_id": "SAA-C03",
"domain": "DESIGN_RESILIENT_ARCHITECTURES",
"difficulty": "medium",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "A solutions architect is designing a workflow for a new application that must be event-driven and serverless. The workflow should be loosely coupled and process tasks in parallel. Which design should be used?",
"option_a": "Build the workflow using AWS Glue to invoke Lambda functions for processing each step.",
"option_b": "Build the workflow using AWS Step Functions, deploy the application on EC2, and have Step Functions invoke processing steps on the EC2 instances.",
"option_c": "Build the workflow using Amazon EventBridge to invoke Lambda functions on a schedule.",
"option_d": "Build the workflow using AWS Step Functions to create a state machine that invokes Lambda functions for each processing step.",
"correct_answers": ["D"],
"explanation_detailed": "Using AWS Step Functions with Lambda provides a fully managed, serverless orchestration layer capable of parallel and sequential steps, retries, and error handling in a loosely coupled design.",
"incorrect_explanations": {
"A": "AWS Glue is targeted at ETL workloads, not general workflow orchestration for arbitrary application logic.",
"B": "Placing core processing on EC2 undermines the serverless objective and increases operational overhead.",
"C": "Scheduled EventBridge rules provide time-based triggers rather than full workflow management or parallel fan-out."
}
},
{
"id": "saa-c03-domain-180",
"certification_id": "SAA-C03",
"domain": "DESIGN_SECURE_APPLICATIONS_ARCHITECTURES",
"difficulty": "easy",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "A developer has an application that uses a Lambda function to upload files to S3 and needs the necessary permissions to do so. The developer already has an IAM user with valid S3 credentials. What should be done?",
"option_a": "Add the required IAM permissions to the Lambda function’s resource policy.",
"option_b": "Create a presigned request using the existing IAM credentials in the Lambda function.",
"option_c": "Create a new IAM user and use its credentials in the Lambda function.",
"option_d": "Create an IAM execution role with the required permissions and attach it to the Lambda function.",
"correct_answers": ["D"],
"explanation_detailed": "Granting the Lambda function an IAM execution role with the required S3 permissions allows the function to access S3 securely without embedding any user credentials.",
"incorrect_explanations": {
"A": "A resource policy on the function manages who can invoke it, not the S3 access rights used during execution.",
"B": "Using embedded IAM user credentials with presigned requests is less secure and harder to manage than using a role.",
"C": "Creating another IAM user simply duplicates the long-term credential problem rather than using a role that Lambda assumes automatically."
}
},
{
"id": "saa-c03-domain-181",
"certification_id": "SAA-C03",
"domain": "DESIGN_RESILIENT_ARCHITECTURES",
"difficulty": "medium",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "A company deployed a serverless application that invokes a Lambda function when new documents are uploaded to an S3 bucket. The Lambda function processes the documents. After a recent campaign, many documents went unprocessed. What should be done to improve the architecture?",
"option_a": "Increase the Lambda function’s timeout to 15 minutes.",
"option_b": "Configure an S3 bucket replication policy and prepare documents for later processing.",
"option_c": "Deploy an additional Lambda function and split the processing load between the two.",
"option_d": "Create an Amazon SQS queue, send upload events to the queue, and configure it as an event source for Lambda.",
"correct_answers": ["D"],
"explanation_detailed": "By introducing SQS between S3 and Lambda, document events are durably queued and processed reliably, even when a campaign generates sudden spikes in uploads.",
"incorrect_explanations": {
"A": "Increasing the timeout only affects how long a single invocation can run, not how reliably events are captured under high concurrency.",
"B": "Replication moves objects between buckets and does not address event delivery or processing backlogs.",
"C": "Adding another Lambda function without a message queue still risks event drops when throttling occurs."
}
},
{
"id": "saa-c03-domain-182",
"certification_id": "SAA-C03",
"domain": "DESIGN_RESILIENT_ARCHITECTURES",
"difficulty": "hard",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "A company is designing a disaster recovery (DR) architecture for its on-premises MySQL database running on an EC2 instance in a private subnet with scheduled backups. The DR design must span multiple AWS regions. Which solution meets these requirements with the LOWEST operational overhead?",
"option_a": "Migrate the MySQL database to multiple EC2 instances. Configure a standby instance in the DR region and enable replication.",
"option_b": "Migrate the MySQL database to Amazon RDS. Use a Multi-AZ deployment and enable read replica replication in different AZs.",
"option_c": "Migrate the MySQL database to an Amazon Aurora global database, hosting the primary cluster in the main region and the secondary cluster in the DR region.",
"option_d": "Store the scheduled backups in an S3 bucket configured for cross-region replication and restore the database in the DR region.",
"correct_answers": ["C"],
"explanation_detailed": "An Aurora global database provides managed multi-Region replication with low-latency reads and tooling for fast failover, offering strong DR capabilities with minimal administrative overhead.",
"incorrect_explanations": {
"A": "Self-managed MySQL on EC2 across Regions requires custom replication setup, monitoring, and failover orchestration.",
"B": "RDS Multi-AZ and cross-AZ read replicas only improve availability within a single Region and do not satisfy multi-Region DR requirements.",
"D": "Replicated backups support restoring a database in another Region but result in longer recovery times and more manual steps than a continuously replicated secondary cluster."
}
},
{
"id": "saa-c03-domain-183",
"certification_id": "SAA-C03",
"domain": "DESIGN_HIGH_PERFORMING_ARCHITECTURES",
"difficulty": "medium",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "A company’s Java application uses Amazon SQS to process messages but cannot handle messages larger than 256 KB. The company wants the application to handle messages up to 50 MB with minimal code changes. Which solution meets these requirements?",
"option_a": "Use the Amazon SQS Extended Client Library for Java to store messages larger than 256 KB in Amazon S3.",
"option_b": "Use Amazon EventBridge to post larger messages instead of SQS.",
"option_c": "Increase the message size limit in Amazon SQS to handle messages larger than 256 KB.",
"option_d": "Store messages larger than 256 KB in Amazon EFS and have SQS reference that location.",
"correct_answers": ["A"],
"explanation_detailed": "Using the SQS Extended Client Library allows large message payloads to be automatically stored in S3 while the queue carries only a pointer, enabling support for up to 50 MB without re-architecting the workflow.",
"incorrect_explanations": {
"B": "EventBridge is not a direct substitute for SQS in existing queue-based consumer applications and has its own size constraints.",
"C": "You cannot simply raise the SQS maximum message size beyond the service’s documented limits.",
"D": "Storing payloads in EFS would require custom logic and does not integrate with SQS as seamlessly as the S3-based extended client library."
}
},
{
"id": "saa-c03-domain-184",
"certification_id": "SAA-C03",
"domain": "DESIGN_SECURE_APPLICATIONS_ARCHITECTURES",
"difficulty": "easy",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "A company wants to restrict public access to objects in its Amazon S3 buckets so that all objects remain private. Which solution meets this requirement?",
"option_a": "Use Amazon GuardDuty to monitor bucket policies and create an automated Lambda correction rule.",
"option_b": "Use AWS Trusted Advisor to find publicly accessible buckets, then notify by email for manual correction.",
"option_c": "Use AWS Resource Access Manager to locate public buckets and trigger a Lambda function via SNS for correction.",
"option_d": "Enable S3 Block Public Access at the account level and use AWS Organizations to create an SCP preventing IAM users from modifying this setting.",
"correct_answers": ["D"],
"explanation_detailed": "Account-level S3 Block Public Access combined with an SCP in AWS Organizations prevents any buckets or objects from being made public and blocks attempts to disable these protections.",
"incorrect_explanations": {
"A": "GuardDuty surfaces security findings but does not enforce access controls or prevent misconfigurations by design.",
"B": "Trusted Advisor only reports findings and requires manual remediation, which cannot guarantee that objects remain private at all times.",
"C": "AWS Resource Access Manager is for sharing resources across accounts and is not intended for detecting or fixing S3 public access issues."
}
},
{
"id": "saa-c03-domain-185",
"certification_id": "SAA-C03",
"domain": "DESIGN_COST_OPTIMIZED_ARCHITECTURES",
"difficulty": "easy",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "A company hosts its web application on AWS and wants to ensure that all EC2 instances, RDS instances, and Redshift clusters are tagged. The company wants to minimize the configuration and operational overhead of this tagging verification. What should be done?",
"option_a": "Use AWS Config rules to define and detect resources that are not properly tagged.",
"option_b": "Use Cost Explorer to list untagged resources and manually tag them.",
"option_c": "Write API calls to verify resource tagging and run them periodically on an EC2 instance.",
"option_d": "Write API calls to verify resource tagging and schedule a Lambda function via CloudWatch to run them periodically.",
"correct_answers": ["A"],
"explanation_detailed": "AWS Config rules can continuously evaluate whether resources have required tags and flag or remediate non-compliant resources without custom code.",
"incorrect_explanations": {
"B": "Cost Explorer helps analyze untagged spend but does not continuously enforce or evaluate resource tagging compliance.",
"C": "Custom scripts on EC2 introduce additional infrastructure, scheduling, and maintenance responsibilities.",
"D": "Lambda with custom logic still requires you to build and maintain the tagging checks instead of using managed Config rules."
}
},
{
"id": "saa-c03-domain-186",
"certification_id": "SAA-C03",
"domain": "DESIGN_COST_OPTIMIZED_ARCHITECTURES",
"difficulty": "easy",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "A development team needs to host a website that will be accessed by other teams. The website consists of HTML, CSS, client-side JavaScript, and images. Which method is the MOST cost-effective?",
"option_a": "Containerize the website and host it on AWS Fargate.",
"option_b": "Create an S3 bucket and host the website there.",
"option_c": "Deploy a web server on an EC2 instance to host the website.",
"option_d": "Configure an Application Load Balancer with a Lambda target using Express.js.",
"correct_answers": ["B"],
"explanation_detailed": "Amazon S3 static website hosting is the simplest and least expensive approach for serving static assets such as HTML, CSS, JavaScript, and images.",
"incorrect_explanations": {
"A": "Using Fargate adds container management and compute costs that are unnecessary for a static site.",
"C": "An EC2 instance incurs ongoing compute charges and requires operational management for a task that S3 can handle natively.",
"D": "An ALB plus Lambda adds unnecessary components and cost to deliver static content."
}
},
{
"id": "saa-c03-domain-187",
"certification_id": "SAA-C03",
"domain": "DESIGN_RESILIENT_ARCHITECTURES",
"difficulty": "medium",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "A solutions architect is designing a workflow for a new application that must be event-driven and serverless, loosely coupled, and capable of processing tasks in parallel. Which design meets these requirements?",
"option_a": "Build the workflow using AWS Glue to invoke Lambda functions for each step.",
"option_b": "Build the workflow using AWS Step Functions, deploy the application on EC2, and invoke processing steps on EC2.",
"option_c": "Build the workflow using Amazon EventBridge to invoke Lambda functions on a schedule.",
"option_d": "Build the workflow using AWS Step Functions to create a state machine that invokes Lambda functions for each step.",
"correct_answers": ["D"],
"explanation_detailed": "AWS Step Functions orchestrating Lambda functions provides a fully managed state machine for event-driven, parallel, and sequential processing without managing servers.",
"incorrect_explanations": {
"A": "AWS Glue is aimed at ETL jobs, not general serverless workflow orchestration for arbitrary application logic.",
"B": "Running core processing steps on EC2 requires server management and reduces the benefits of a serverless design.",
"C": "EventBridge schedules are time-based triggers and do not provide the structured workflow modeling and parallelization capabilities of Step Functions."
}
},
{
"id": "saa-c03-domain-188",
"certification_id": "SAA-C03",
"domain": "DESIGN_SECURE_APPLICATIONS_ARCHITECTURES",
"difficulty": "easy",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "A developer has an application that uses a Lambda function to upload files to S3 and needs to have the appropriate permissions. The developer already possesses an IAM user with valid S3 credentials. What should be done?",
"option_a": "Add the necessary IAM permissions to the Lambda function’s resource policy.",
"option_b": "Create a presigned request using the existing IAM credentials within the Lambda function.",
"option_c": "Create a new IAM user and use its credentials in the Lambda function.",
"option_d": "Create an IAM execution role with the required permissions and attach it to the Lambda function.",
"correct_answers": ["D"],
"explanation_detailed": "The best practice is to assign an IAM execution role to the Lambda function that grants it the necessary S3 permissions, avoiding any embedded IAM user credentials.",
"incorrect_explanations": {
"A": "Function resource policies govern who can invoke the function, not what resources the function can access when it runs.",
"B": "Embedding IAM user credentials and generating presigned requests within the function is less secure and harder to rotate than using an execution role.",
"C": "Creating another IAM user repeats the same long-term credential management issues instead of using a role that Lambda assumes automatically."
}
},
{
"id": "saa-c03-domain-189",
"certification_id": "SAA-C03",
"domain": "DESIGN_RESILIENT_ARCHITECTURES",
"difficulty": "medium",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "A company deployed a serverless application that invokes a Lambda function when new documents are uploaded to an S3 bucket. The Lambda function processes the documents, but after a marketing campaign, many documents were not processed. What should be done to improve the architecture?",
"option_a": "Increase the Lambda function’s timeout to 15 minutes.",
"option_b": "Configure an S3 bucket replication policy and prepare documents for later processing.",
"option_c": "Deploy an additional Lambda function and split the processing load.",
"option_d": "Create an Amazon SQS queue, send upload events to it, and configure the queue as an event source for Lambda.",
"correct_answers": ["D"],
"explanation_detailed": "Using an SQS queue as an event source ensures that all upload events are durably stored and processed, even under high load, eliminating missed documents during campaigns.",
"incorrect_explanations": {
"A": "A longer timeout only affects individual invocations and does not ensure that all events are successfully delivered and processed.",
"B": "Replication only copies the objects and does not address reliable event handling or processing backlogs.",
"C": "Adding another Lambda function without a buffer does not prevent event loss when invocation limits are reached."
}
},
{
"id": "saa-c03-domain-190",
"certification_id": "SAA-C03",
"domain": "DESIGN_RESILIENT_ARCHITECTURES",
"difficulty": "hard",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "A company is designing a disaster recovery (DR) architecture for its on-premises MySQL database running on an EC2 instance in a private subnet with scheduled backups. The DR solution must span multiple AWS regions. Which solution meets these requirements with the LOWEST operational overhead?",
"option_a": "Migrate the MySQL database to multiple EC2 instances. Configure a standby EC2 instance in the DR region. Enable replication.",
"option_b": "Migrate the MySQL database to Amazon RDS. Use a Multi-AZ deployment. Enable read replica replication for the primary instance in different AZs.",
"option_c": "Migrate the MySQL database to an Amazon Aurora global database. Host the primary cluster in the main region and the secondary cluster in the DR region.",
"option_d": "Store the scheduled backups in an S3 bucket configured for cross-region replication and restore the database in the DR region.",
"correct_answers": ["C"],
"explanation_detailed": "An Aurora global database provides built-in cross-Region replication and managed failover mechanisms, enabling multi-Region DR with much less operational effort than self-managed replication or snapshot-based restores.",
"incorrect_explanations": {
"A": "Replicating and managing MySQL across EC2 instances in multiple Regions requires complex configuration, monitoring, and manual failover.",
"B": "RDS Multi-AZ and read replicas are confined to a single Region and therefore do not satisfy multi-Region DR requirements.",
"D": "Using only cross-Region backup replication requires manual restore steps and leads to longer recovery times compared to an already-running secondary cluster."
}
},
{
"id": "saa-c03-domain-191",
"certification_id": "SAA-C03",
"domain": "DESIGN_HIGH_PERFORMING_ARCHITECTURES",
"difficulty": "medium",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "A company’s Java application uses SQS to process messages but cannot handle messages larger than 256 KB. The company wants the application to handle messages up to 50 MB with minimal code changes. Which solution meets these requirements?",
"option_a": "Use the Amazon SQS Extended Client Library for Java to store messages larger than 256 KB in Amazon S3.",
"option_b": "Use Amazon EventBridge to post larger messages instead of SQS.",
"option_c": "Increase the message size limit in SQS to over 256 KB.",
"option_d": "Store messages larger than 256 KB in Amazon EFS and have SQS reference that location.",
"correct_answers": ["A"],
"explanation_detailed": "The SQS Extended Client Library for Java automatically stores large payloads in S3 and passes references through SQS, allowing the existing consumer to be adapted with minimal changes.",
"incorrect_explanations": {
"B": "EventBridge is not designed as a drop-in replacement for SQS queues and still has its own size limits and event patterns.",
"C": "You cannot configure SQS to support arbitrary message sizes beyond the service’s maximum limit.",
"D": "Using EFS requires custom implementation to coordinate payload storage and retrieval and is not supported by a managed SQS library."
}
},
{
"id": "saa-c03-domain-192",
"certification_id": "SAA-C03",
"domain": "DESIGN_SECURE_APPLICATIONS_ARCHITECTURES",
"difficulty": "easy",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "A company wants to restrict public access to objects in its Amazon S3 buckets so that all objects remain private. Which solution meets this requirement?",
"option_a": "Use Amazon GuardDuty to monitor bucket policies and create an automated Lambda remediation rule.",
"option_b": "Use AWS Trusted Advisor to find publicly accessible buckets and then manually correct the policies.",
"option_c": "Use AWS Resource Access Manager to locate public buckets and trigger a Lambda function via SNS for remediation.",
"option_d": "Enable S3 Block Public Access at the account level and use AWS Organizations to enforce an SCP that prevents modifications.",
"correct_answers": ["D"],
"explanation_detailed": "Enabling S3 Block Public Access for the account and enforcing it via an SCP ensures that no S3 bucket or object can be made public and that the setting cannot be disabled by IAM users.",
"incorrect_explanations": {
"A": "GuardDuty focuses on threat detection and does not enforce or lock public access settings for S3 buckets.",
"B": "Trusted Advisor provides visibility into public buckets but depends on manual remediation and does not prevent future misconfigurations.",
"C": "AWS Resource Access Manager does not provide functionality for detecting or remediating public S3 bucket configurations."
}
},
{
"id": "saa-c03-domain-193",
"certification_id": "SAA-C03",
"domain": "DESIGN_COST_OPTIMIZED_ARCHITECTURES",
"difficulty": "easy",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "A development team needs to host a website that will be accessed by other teams. The website consists of HTML, CSS, client-side JavaScript, and images. Which method is the MOST cost-effective?",
"option_a": "Containerize the website and host it on AWS Fargate.",
"option_b": "Create an S3 bucket and host the website there.",
"option_c": "Deploy a web server on an EC2 instance to host the website.",
"option_d": "Configure an Application Load Balancer with a Lambda target using Express.js.",
"correct_answers": ["B"],
"explanation_detailed": "S3 static website hosting is ideal for serving static content with minimal cost and no need to manage compute resources.",
"incorrect_explanations": {
"A": "Fargate adds unnecessary container orchestration and compute cost for a simple static website.",
"C": "An EC2 instance must be maintained and paid for continuously, even when traffic is low or zero.",
"D": "Combining an ALB with Lambda for static content introduces extra moving parts and costs compared to S3."
}
},
{
"id": "saa-c03-domain-194",
"certification_id": "SAA-C03",
"domain": "DESIGN_RESILIENT_ARCHITECTURES",
"difficulty": "medium",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "A solutions architect is designing a workflow for a new serverless data processing application that must be event-driven and loosely coupled. Which design best meets these requirements?",
"option_a": "Build the workflow using AWS Glue to trigger Lambda functions for each processing step.",
"option_b": "Build the workflow using AWS Step Functions, deploy on EC2, and trigger processing on the EC2 instances.",
"option_c": "Build the workflow using Amazon EventBridge to trigger Lambda functions on a schedule.",
"option_d": "Build the workflow using AWS Step Functions to create a state machine that invokes Lambda functions for each step.",
"correct_answers": ["D"],
"explanation_detailed": "AWS Step Functions orchestrating Lambda functions provides serverless, loosely coupled workflows with clear state management, retries, and branching, which is well suited for event-driven data processing.",
"incorrect_explanations": {
"A": "AWS Glue is tailored for ETL jobs and does not provide the general workflow state modeling capabilities of Step Functions.",
"B": "Running the processing on EC2 goes against the requirement for a fully serverless architecture.",
"C": "Scheduled EventBridge rules are time-based triggers and do not inherently model multi-step workflows or complex dependencies."
}
},
{
"id": "saa-c03-domain-195",
"certification_id": "SAA-C03",
"domain": "DESIGN_SECURE_APPLICATIONS_ARCHITECTURES",
"difficulty": "easy",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "A developer has an application that uses a Lambda function to upload files to S3 and needs the necessary permissions. The developer already has an IAM user with valid S3 credentials. What should be done?",
"option_a": "Add the required IAM permissions to the Lambda function’s resource policy.",
"option_b": "Create a presigned URL using the existing IAM credentials within the Lambda function.",
"option_c": "Create a new IAM user and use its credentials in the Lambda function.",
"option_d": "Create an IAM execution role with the required permissions and attach it to the Lambda function.",
"correct_answers": ["D"],
"explanation_detailed": "The correct approach is to attach an IAM execution role to the Lambda function that grants the required S3 permissions, keeping credentials out of the code and enabling easier rotation and least-privilege access.",
"incorrect_explanations": {
"A": "A resource-based policy controls who can invoke the function, not the S3 operations it can perform.",
"B": "Using embedded IAM user credentials to generate presigned URLs in the function is less secure and more difficult to manage than using roles.",
"C": "Creating another IAM user perpetuates the use of long-term credentials and does not leverage Lambda’s native role assumption model."
}
},
{
"id": "saa-c03-domain-196",
"certification_id": "SAA-C03",
"domain": "DESIGN_RESILIENT_ARCHITECTURES",
"difficulty": "medium",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "A company deployed a serverless application that invokes a Lambda function when new documents are uploaded to an S3 bucket. The Lambda function processes the documents. After a marketing campaign, many documents went unprocessed. What should be done to improve the architecture?",
"option_a": "Increase the Lambda function’s timeout value to 15 minutes.",
"option_b": "Configure an S3 replication policy and prepare documents for later processing.",
"option_c": "Deploy an additional Lambda function and split the processing load between the two.",
"option_d": "Create an Amazon SQS queue, send document upload events to it, and configure it as an event source for Lambda.",
"correct_answers": ["D"],
"explanation_detailed": "Introducing an SQS queue between S3 and Lambda provides durable buffering and allows Lambda to scale consumption based on queue depth, ensuring that all documents are eventually processed.",
"incorrect_explanations": {
"A": "A higher timeout does not ensure reliable event delivery or prevent dropped invocations during peak loads.",
"B": "Replication alone only copies objects and does not guarantee that application logic will process each upload.",
"C": "Adding another Lambda function without a queue still risks losing events when invocation limits are exceeded."
}
},
{
"id": "saa-c03-domain-197",
"certification_id": "SAA-C03",
"domain": "DESIGN_HIGH_PERFORMING_ARCHITECTURES",
"difficulty": "medium",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "A company is migrating its on-premises database to AWS. It must move 20 TB of data in 30 days, but its network is limited to 15 Mbps and cannot exceed 70% utilization. Which solution meets these requirements?",
"option_a": "Use AWS Snowball.",
"option_b": "Use AWS DataSync.",
"option_c": "Use AWS DMS over the public internet.",
"option_d": "Use Amazon S3 Transfer Acceleration.",
"correct_answers": ["A"],
"explanation_detailed": "AWS Snowball is designed for bulk data transfer at the tens of terabytes scale when network bandwidth is constrained, ensuring the migration can complete within the required timeframe.",
"incorrect_explanations": {
"B": "DataSync still relies on the limited network link and cannot efficiently transfer 20 TB without violating the utilization constraint.",
"C": "DMS is intended for database migration and replication, but over a constrained link it will struggle to move this volume of data in time.",
"D": "Transfer Acceleration improves path performance over the internet but cannot overcome the fundamental bandwidth limitation of the local network."
}
},
{
"id": "saa-c03-domain-198",
"certification_id": "SAA-C03",
"domain": "DESIGN_RESILIENT_ARCHITECTURES",
"difficulty": "medium",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "A company’s online education platform stores student records in a PostgreSQL database. The company needs to keep these records available and online across multiple AWS regions at all times. Which solution meets these requirements with the LEAST operational overhead?",
"option_a": "Migrate the PostgreSQL database to a cluster running on EC2 instances.",
"option_b": "Migrate the PostgreSQL database to an Amazon RDS for PostgreSQL instance with Multi-AZ enabled.",
"option_c": "Migrate the PostgreSQL database to an Amazon RDS for PostgreSQL instance and create a read replica in another region.",
"option_d": "Migrate the PostgreSQL database to an Amazon RDS for PostgreSQL instance and configure database snapshots to be copied to another region.",
"correct_answers": ["C"],
"explanation_detailed": "Using RDS for PostgreSQL with a cross-Region read replica provides an online copy of the data in another Region with minimal management, supporting multi-Region availability needs.",
"incorrect_explanations": {
"A": "Running PostgreSQL on EC2 requires manual setup for replication, backups, monitoring, and failover between Regions.",
"B": "Multi-AZ RDS deployments only provide high availability within a single Region, not across multiple Regions.",
"D": "Cross-Region snapshot copies support backup and restore but do not provide continuously available, online read replicas."
}
},
{
"id": "saa-c03-domain-199",
"certification_id": "SAA-C03",
"domain": "DESIGN_SECURE_APPLICATIONS_ARCHITECTURES",
"difficulty": "medium",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "A company hosts its web application on AWS using EC2 instances behind an Application Load Balancer (ALB) and an Aurora database. The security team reports that the application is vulnerable to SQL injection. How should the company address this issue?",
"option_a": "Use AWS WAF in front of the ALB. Attach appropriate web ACLs.",
"option_b": "Create an ALB listener rule that responds to SQL injections with a fixed response.",
"option_c": "Subscribe to AWS Shield Advanced to automatically block all SQL injection attempts.",
"option_d": "Configure Amazon Inspector to automatically block all SQL injection attempts.",
"correct_answers": ["A"],
"explanation_detailed": "Deploying AWS WAF in front of the ALB with rules that detect and block SQL injection patterns is the appropriate way to mitigate SQL injection attacks at the edge.",
"incorrect_explanations": {
"B": "ALB listener rules cannot reliably detect SQL injection payloads and are not designed as a full web application firewall.",
"C": "AWS Shield Advanced provides DDoS protection and does not specifically block application-layer SQL injection attacks.",
"D": "Amazon Inspector performs vulnerability assessments but does not sit inline to block SQL injection attempts in real time."
}
},
{
"id": "saa-c03-domain-200",
"certification_id": "SAA-C03",
"domain": "DESIGN_RESILIENT_ARCHITECTURES",
"difficulty": "medium",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "A company is moving its data management application to AWS and wants to transition to an event-driven architecture. The new architecture should be more distributed and leverage serverless concepts to run various workflow steps while minimizing operational overhead. Which solution meets these requirements?",
"option_a": "Build the workflow using AWS Glue to invoke Lambda functions for each step.",
"option_b": "Build the workflow using AWS Step Functions, deploy the application on EC2, and use Step Functions to invoke steps on the EC2 instances.",
"option_c": "Build the workflow using Amazon EventBridge to invoke Lambda functions on a schedule.",
"option_d": "Build the workflow using AWS Step Functions to create a state machine that invokes Lambda functions for each step.",
"correct_answers": ["D"],
"explanation_detailed": "Using AWS Step Functions with Lambda creates a fully serverless, event-driven workflow where each step is independently scalable and managed without the overhead of running EC2 instances.",
"incorrect_explanations": {
"A": "AWS Glue is targeted at ETL data processing rather than general workflow orchestration for arbitrary application components.",
"B": "Deploying core logic on EC2 contradicts the goal of minimizing operational overhead through serverless services.",
"C": "Scheduled invocations from EventBridge do not inherently provide the stateful, multi-step workflow management that Step Functions offers."
}
},
{
"id": "saa-c03-domain-201",
"certification_id": "SAA-C03",
"domain": "DESIGN_HIGH_PERFORMING_ARCHITECTURES",
"difficulty": "medium",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "A company hosts its web application on AWS. The application uses an RDS for PostgreSQL database and is experiencing connection timeout errors during peak traffic, causing failed transactions. What should a solutions architect recommend to reduce these errors with minimal code changes?",
"option_a": "Reduce the concurrency of the Lambda functions.",
"option_b": "Enable RDS Proxy on the RDS instance.",
"option_c": "Resize the RDS instance to handle more connections.",
"option_d": "Migrate the database to Amazon DynamoDB with on-demand scaling.",
"correct_answers": ["B"],
"explanation_detailed": "Enabling RDS Proxy allows connection pooling and reuse, smoothing out spikes in application connections and reducing timeouts without requiring significant application code changes.",
"incorrect_explanations": {
"A": "Reducing Lambda concurrency lowers throughput and does not directly optimize how database connections are managed.",
"C": "Simply resizing the instance may temporarily increase capacity but does not address inefficient connection usage and pooling.",
"D": "Migrating to DynamoDB changes the data model and requires major application refactoring, which conflicts with the requirement for minimal code changes."
}
},
{
"id": "saa-c03-domain-202",
"certification_id": "SAA-C03",
"domain": "DESIGN_RESILIENT_ARCHITECTURES",
"difficulty": "medium",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "A company is migrating its legacy Windows IIS web application to AWS. The current application runs on a single EC2 instance and stores files on a local NAS. Due to increased remote usage, the file server is nearing capacity. Which solution will provide highly available file storage with minimal changes to the user access pattern?",
"option_a": "Migrate the file server to an EC2 instance in a public subnet. Configure the security group to allow inbound traffic only from employee IPs.",
"option_b": "Migrate the files to Amazon FSx for Windows File Server, integrate it with the on-premises Active Directory, and configure AWS Client VPN.",
"option_c": "Migrate the files to Amazon S3 and create a private VPC endpoint. Generate presigned URLs for secure downloads.",
"option_d": "Migrate the files to Amazon S3 and create a public VPC endpoint. Allow employees to sign in via AWS IAM Identity Center (SSO).",
"correct_answers": ["B"],
"explanation_detailed": "Amazon FSx for Windows File Server provides a fully managed, highly available Windows file system that supports SMB and Active Directory integration, preserving the existing access pattern while scaling for remote users.",
"incorrect_explanations": {
"A": "Running a file server on EC2 still requires capacity management, patching, and HA configuration, and exposes the instance in a public subnet.",
"C": "Migrating to S3 changes the access semantics from SMB shares to object storage and requires application or user workflow changes.",
"D": "Using a public S3 endpoint exposes the bucket to the internet and changes how users access files compared to a traditional Windows file share."
}
},
{
"id": "saa-c03-domain-203",
"certification_id": "SAA-C03",
"domain": "DESIGN_HIGH_PERFORMING_ARCHITECTURES",
"difficulty": "hard",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "A company’s application runs on EC2 instances behind an Application Load Balancer (ALB) in an Auto Scaling group across multiple Availability Zones. The application uses a MySQL 8.0 database hosted on a large EC2 instance. As application load increases, the database performance quickly degrades. The application handles more read requests than writes. The company wants a solution that automatically scales the database to handle unpredictable read workloads while maintaining high availability. Which solution meets these requirements?",
"option_a": "Use Amazon Redshift with a single node for leader and compute.",
"option_b": "Use Amazon RDS with a Single-AZ deployment and add read replicas in another AZ.",
"option_c": "Use Amazon Aurora with a Multi-AZ deployment and configure Aurora Auto Scaling with replicas.",
"option_d": "Use Amazon ElastiCache for Memcached with Spot EC2 instances.",
"correct_answers": ["C"],
"explanation_detailed": "Amazon Aurora with a Multi-AZ cluster and Aurora Replicas supports automatic scaling of read capacity and high availability, making it well suited for unpredictable, read-heavy workloads.",
"incorrect_explanations": {
"A": "Amazon Redshift is a data warehouse solution optimized for analytics, not for serving OLTP application traffic.",
"B": "A Single-AZ RDS deployment is a single point of failure and does not provide the same high availability as a Multi-AZ Aurora cluster with auto-scaling replicas.",
"D": "ElastiCache can offload some reads but does not replace the need for a scalable, highly available database layer, and requires additional cache invalidation logic."
}
},
{
"id": "saa-c03-domain-204",
"certification_id": "SAA-C03",
"domain": "DESIGN_RESILIENT_ARCHITECTURES",
"difficulty": "medium",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "A company runs a serverless application that uses API Gateway and Lambda to process uploaded documents. After a marketing campaign, many documents were not processed. What should be done to improve processing reliability?",
"option_a": "Increase the Lambda function’s timeout to 15 minutes.",
"option_b": "Configure a replication policy on the S3 bucket and process documents later.",
"option_c": "Deploy an additional Lambda function and split the processing load between them.",
"option_d": "Create an Amazon SQS queue, send upload events to it, and configure the queue as an event source for Lambda.",
"correct_answers": ["D"],
"explanation_detailed": "Using SQS between the upload trigger and the processing Lambda adds durable queuing and back-pressure handling, ensuring that every document event is eventually processed even during traffic spikes.",
"incorrect_explanations": {
"A": "A higher timeout does not prevent events from being dropped when concurrency limits are reached.",
"B": "Replication only affects where the objects are stored, not whether the processing logic reliably executes for each upload.",
"C": "Adding another Lambda function without introducing a queue still does not address the need for durable buffering under bursty traffic."
}
},
{
"id": "saa-c03-domain-205",
"certification_id": "SAA-C03",
"domain": "DESIGN_COST_OPTIMIZED_ARCHITECTURES",
"difficulty": "easy",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "A company hosts its objects in S3 Standard. A solutions architect discovers that 75% of the data is rarely accessed after 30 days. The company needs all data to remain immediately accessible with the same high availability and durability, but wants to reduce storage costs. Which storage solution meets these requirements?",
"option_a": "Transition objects to S3 Glacier Deep Archive immediately.",
"option_b": "Transition objects to S3 Standard-Infrequent Access (S3 Standard-IA) after 30 days.",
"option_c": "Transition objects to S3 One Zone-Infrequent Access (S3 One Zone-IA) after 30 days.",
"option_d": "Transition objects to S3 One Zone-Infrequent Access immediately.",
"correct_answers": ["B"],
"explanation_detailed": "Transitioning infrequently accessed objects to S3 Standard-IA after 30 days reduces storage costs while preserving S3 Standard-class durability and multi-AZ availability with millisecond access.",
"incorrect_explanations": {
"A": "S3 Glacier Deep Archive is intended for archival data and does not provide immediate millisecond access.",
"C": "S3 One Zone-IA stores data in a single Availability Zone, reducing availability and resilience compared to S3 Standard-class multi-AZ storage.",
"D": "Transitioning immediately to One Zone-IA violates the requirement to maintain the same level of availability and durability as S3 Standard."
}
}
]









