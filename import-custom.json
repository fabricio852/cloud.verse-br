[
{
"id": "aif-c01-responsible_ai-001",
"certification_id": "AIF-C01",
"domain": "RESPONSIBLE_AI",
"difficulty": "medium",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "An AI practitioner must write a transparency and explainability report for quarterly demand-forecasting ML models. What should be included to meet explainability requirements for stakeholders?",
"option_a": "Training code.",
"option_b": "Partial Dependence Plots (PDPs).",
"option_c": "Sample training data.",
"option_d": "Model convergence tables.",
"correct_answers": ["B"],
"explanation_detailed": "Partial Dependence Plots (PDPs) visualize the marginal effect of one or two input features on a model’s prediction while averaging out the influence of other variables. They provide a feature-to-outcome view that non-technical stakeholders can grasp, supporting transparency and interpretability without disclosing proprietary data or complex internals. PDPs help reveal monotonicity, diminishing returns, and interaction hints. In AWS, you can generate feature-attribution artifacts with Amazon SageMaker Clarify and complement them with PDP-like analyses in notebooks. Together with documentation of data lineage and evaluation metrics, PDPs make it clear how specific drivers (price, seasonality, promotions) move the forecast, which is central to responsible reporting and model risk management.",
"incorrect_explanations": {
"A": "Publishing raw training code alone does not explain how individual features influence predictions. It aids reproducibility but offers limited stakeholder-level interpretability without model-behavior visualizations.",
"C": "Sample data shows inputs but not their causal or marginal contribution to outcomes. It can raise privacy concerns and still leaves stakeholders unsure how the model uses features.",
"D": "Convergence tables focus on optimization progress or loss curves, not on feature impact. They help engineers, but they do not translate into stakeholder-friendly explanations."
}
},
{
"id": "aif-c01-ai_services-002",
"certification_id": "AIF-C01",
"domain": "AI_SERVICES",
"difficulty": "medium",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "A law firm wants an AI application using large language models to read legal documents and extract key points. Which solution fits?",
"option_a": "Build a named entity recognition (NER) automation only.",
"option_b": "Create a content recommendation engine.",
"option_c": "Develop a summarization chatbot.",
"option_d": "Develop a multilingual translation system.",
"correct_answers": ["C"],
"explanation_detailed": "A summarization chatbot powered by an LLM is designed to condense long, technical texts into concise, salient points while preserving legal nuance. Using Amazon Bedrock, you can invoke foundation models to perform extractive or abstractive summarization and layer retrieval over case repositories if needed. Guardrails can enforce tone and policy, while Amazon Kendra or a Bedrock knowledge base can improve grounding on firm documents. Compared to a pure NER pipeline, summarization covers broader insights such as issue spotting, holdings, and obligations, not just entities. This aligns with the firm’s need to read documents and surface key points interactively, with governance and logging via CloudWatch and Bedrock invocation logging.",
"incorrect_explanations": {
"A": "NER extracts entities (names, dates, statutes) but misses higher-level key points, arguments, or obligations that summarization captures.",
"B": "Recommendation engines rank or suggest content; they do not read a document and extract its salient points.",
"D": "Translation changes language, not content abstraction. It does not identify key legal ideas in the original document."
}
},
{
"id": "aif-c01-ai_fundamentals-003",
"certification_id": "AIF-C01",
"domain": "AI_FUNDAMENTALS",
"difficulty": "medium",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "A company must classify human genes into 20 categories and wants to document how internal model mechanics affect outputs. Which algorithm best fits?",
"option_a": "Decision Trees.",
"option_b": "Linear Regression.",
"option_c": "Logistic Regression.",
"option_d": "Neural Networks.",
"correct_answers": ["A"],
"explanation_detailed": "Decision trees provide native interpretability by exposing the hierarchical splits that lead to a prediction. Each node’s condition shows which gene features drive the decision boundary, enabling straightforward, auditable explanations suitable for regulated or scientific contexts. Compared to neural networks, trees require no post-hoc explanation to understand mechanics. They also scale to multi-class problems naturally. On AWS, you can train tree-based models with Amazon SageMaker (e.g., XGBoost or built-in algorithms) and use SageMaker Clarify for feature importance and bias checks. While logistic regression offers interpretability, it assumes linear decision boundaries; tree models capture non-linear interactions between gene features while staying transparent.",
"incorrect_explanations": {
"B": "Linear regression targets continuous outcomes and assumes linearity, which is unsuitable for multi-class categorical outputs and complex interactions.",
"C": "Logistic regression can be interpretable but becomes less practical with 20 classes and non-linear feature interactions common in genomics.",
"D": "Neural networks can be accurate but are opaque by default; documenting internal mechanics requires extra tooling and expertise."
}
},
{
"id": "aif-c01-ai_fundamentals-004",
"certification_id": "AIF-C01",
"domain": "AI_FUNDAMENTALS",
"difficulty": "easy",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "A plant-disease image classifier needs a metric showing how many images were classified correctly. Which metric should be used?",
"option_a": "R-squared score.",
"option_b": "Accuracy.",
"option_c": "Root Mean Squared Error (RMSE).",
"option_d": "Learning rate.",
"correct_answers": ["B"],
"explanation_detailed": "Accuracy measures the proportion of correct predictions over all predictions, which directly answers “how many images were classified correctly.” For balanced, single-label multi-class problems, accuracy is an intuitive primary metric. In AWS, you can compute it during SageMaker training and track it in CloudWatch or SageMaker Experiments. If classes are imbalanced, supplement accuracy with precision, recall, and F1, and inspect confusion matrices to understand class-specific errors. Metrics such as RMSE or R-squared are regression-oriented, while the learning rate is a hyperparameter, not an evaluation metric. Thus, accuracy is the cleanest top-level indicator for a plant leaf disease classifier’s correctness.",
"incorrect_explanations": {
"A": "R-squared quantifies variance explained for regression, not classification correctness across discrete labels.",
"C": "RMSE is a regression error metric; it does not represent categorical hit/miss outcomes.",
"D": "Learning rate is a training hyperparameter, not a performance metric."
}
},
{
"id": "aif-c01-ai_services-005",
"certification_id": "AIF-C01",
"domain": "AI_SERVICES",
"difficulty": "easy",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "A company uses a pretrained LLM for a product-recommendation chatbot. They need short outputs in a specific language. What aligns the LLM responses with expectations?",
"option_a": "Prompt tuning and instruction refinement.",
"option_b": "Choose a different LLM size.",
"option_c": "Increase temperature.",
"option_d": "Increase Top-K.",
"correct_answers": ["A"],
"explanation_detailed": "Clear, constraint-driven prompts steer generation style: target language, maximum tokens, bullet format, tone, and examples of desired replies. In Amazon Bedrock, you can specify system and user messages to enforce brevity and language, add few-shot examples, and set max output tokens. Lower temperature and decoding limits can further reduce verbosity, but the biggest lever is instruction quality. Model size changes capacity, not adherence to style. Top-K and temperature control randomness, not high-level policy. Combine explicit instructions with guardrails and invocation parameters to reliably produce concise, on-brand, language-specific responses without retraining the underlying model.",
"incorrect_explanations": {
"B": "Model size affects capability and latency, not adherence to stylistic constraints you can enforce with clear instructions.",
"C": "Higher temperature increases randomness and verbosity risk; it works against consistency and brevity.",
"D": "Top-K adjusts sampling diversity, not overall style control or required language constraints."
}
},
{
"id": "aif-c01-ai_services-006",
"certification_id": "AIF-C01",
"domain": "AI_SERVICES",
"difficulty": "medium",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "Using Amazon SageMaker in production, inputs can be up to 1 GB and processing up to 1 hour, with near real-time latency required. Which inference option fits?",
"option_a": "Real-time inference.",
"option_b": "Serverless inference.",
"option_c": "Asynchronous inference.",
"option_d": "Batch transform.",
"correct_answers": ["C"],
"explanation_detailed": "SageMaker Asynchronous Inference handles large payloads and long processing times without holding an open HTTP connection. Clients submit requests and later retrieve results from an output S3 location, achieving near real-time responsiveness at scale while decoupling front-end latency from back-end processing. Real-time endpoints are better for sub-second payloads. Serverless inference targets spiky, low-latency traffic with smaller payloads. Batch transform is for offline, deferred processing of large datasets with no immediacy requirement. Asynchronous Inference provides the best balance for big inputs, long compute windows, queueing, and near real-time delivery.",
"incorrect_explanations": {
"A": "Real-time endpoints keep connections open and are not optimized for very large payloads or hour-long processing.",
"B": "Serverless inference suits low-latency, bursty workloads with smaller payloads, not 1 GB and hour-long runs.",
"D": "Batch transform is offline and not designed for near real-time responsiveness."
}
},
{
"id": "aif-c01-ai_fundamentals-007",
"certification_id": "AIF-C01",
"domain": "AI_FUNDAMENTALS",
"difficulty": "easy",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "A team wants to avoid training from scratch and adapt pretrained, domain-specific models for related tasks. Which strategy should they use?",
"option_a": "Increase the number of epochs.",
"option_b": "Transfer learning.",
"option_c": "Decrease the number of epochs.",
"option_d": "Unsupervised learning.",
"correct_answers": ["B"],
"explanation_detailed": "Transfer learning starts from a pretrained model’s learned representations and fine-tunes them on a related downstream task, dramatically reducing data, compute, and time. This is effective when source and target domains share structure (e.g., legal to contract summaries, medical images to related modalities). On AWS, use Amazon SageMaker training or Amazon Bedrock fine-tuning endpoints to adapt foundation models, and leverage MLOps features—Model Registry, Pipelines—for repeatability. Adjust learning rates, unfreeze layers selectively, and monitor overfitting. Transfer learning typically outperforms training from scratch when labeled data is limited and accelerates deployment without sacrificing accuracy.",
"incorrect_explanations": {
"A": "More epochs on a randomly initialized model increases cost and overfitting risk without leveraging prior knowledge.",
"C": "Fewer epochs does not enable knowledge reuse; it simply truncates training time.",
"D": "Unsupervised learning finds structure without labels and does not directly adapt a pretrained supervised model to a new labeled task."
}
},
{
"id": "aif-c01-ai_services-008",
"certification_id": "AIF-C01",
"domain": "AI_SERVICES",
"difficulty": "medium",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "A solution must generate images for protective goggles with high annotation accuracy and minimal mislabeling risk. What should be used?",
"option_a": "Human-in-the-loop validation with Amazon SageMaker Ground Truth Plus.",
"option_b": "Data augmentation using an Amazon Bedrock knowledge base.",
"option_c": "Image recognition with Amazon Rekognition.",
"option_d": "Data summarization using Amazon QuickSight Q.",
"correct_answers": ["A"],
"explanation_detailed": "For high-stakes computer vision datasets, human-in-the-loop (HITL) annotation workflows reduce errors. Amazon SageMaker Ground Truth Plus provides managed labeling with expert workforces, multi-stage review, and built-in quality controls. You can define label taxonomies, consensus strategies, and auditing processes, which is crucial when safety equipment images require precise bounding boxes or segmentation. Rekognition offers prebuilt detection but does not solve dataset creation quality. A Bedrock knowledge base augments LLM context, not labels for images. QuickSight Q supports BI Q&A, not vision annotation. Ground Truth Plus delivers rigorous accuracy controls and metrics needed to minimize mislabeling risk.",
"incorrect_explanations": {
"B": "A Bedrock knowledge base augments LLM context for retrieval, not image labeling accuracy or dataset creation.",
"C": "Rekognition detects objects and scenes but does not manage bespoke annotation workflows or label quality assurance.",
"D": "QuickSight Q is a business intelligence natural-language feature, unrelated to image dataset annotation."
}
},
{
"id": "aif-c01-ai_services-009",
"certification_id": "AIF-C01",
"domain": "AI_SERVICES",
"difficulty": "medium",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "A Bedrock foundation model must access encrypted objects in Amazon S3 that use SSE-S3. The model fails to access the bucket. What should be ensured?",
"option_a": "The role assumed by Amazon Bedrock has permissions to decrypt using the correct encryption key.",
"option_b": "Set S3 buckets to public and enable internet access.",
"option_c": "Use prompt engineering to instruct the model to look into S3.",
"option_d": "Ensure the S3 data has no sensitive information.",
"correct_answers": ["A"],
"explanation_detailed": "Accessing encrypted S3 data requires the invoking service role to have appropriate permissions to read and decrypt the objects. With AWS services such as Amazon Bedrock integrating with S3, the IAM role must include the necessary S3 actions (for example, s3:GetObject) and decryption permissions consistent with the bucket’s encryption settings. In tightly controlled environments, pair this with VPC endpoints and restrictive bucket policies scoped to the role. While encryption mode details differ (KMS vs. S3-managed), the governing principle remains: grant the service role the least-privilege rights to read and decrypt. Do not make buckets public or rely on prompts to bypass authorization.",
"incorrect_explanations": {
"B": "Public access weakens security posture and violates least privilege without fixing missing role permissions.",
"C": "Prompt instructions cannot override IAM or S3 authorization; access control is enforced by AWS, not the model.",
"D": "Sanitizing content does not address the authorization failure when reading encrypted objects."
}
},
{
"id": "aif-c01-ai_services-010",
"certification_id": "AIF-C01",
"domain": "AI_SERVICES",
"difficulty": "medium",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "A company will build an ML model on Amazon SageMaker and needs to share and manage features across multiple teams during development. What should they use?",
"option_a": "Amazon SageMaker Feature Store",
"option_b": "Amazon SageMaker Data Wrangler",
"option_c": "Amazon SageMaker Clarify",
"option_d": "Amazon SageMaker Model Cards",
"correct_answers": ["A"],
"explanation_detailed": "Amazon SageMaker Feature Store provides a centralized, versioned repository for online and offline features. It enables consistent feature definitions across teams, reduces training/serving skew, and supports low-latency online retrieval for inference. Offline stores back experiments and batch training. Integration with SageMaker Pipelines and Model Registry supports MLOps governance. Data Wrangler focuses on preparation and transformation, Clarify on bias/explainability, and Model Cards on documentation. Using Feature Store ensures that data scientists and engineers reuse vetted features, improve reproducibility, and accelerate model delivery while maintaining lineage and access control through IAM and encryption at rest.",
"incorrect_explanations": {
"B": "Data Wrangler streamlines data prep and visualization; it does not provide a shared, versioned repository for cross-team feature reuse.",
"C": "Clarify explains models and detects bias; it does not manage feature storage and online serving.",
"D": "Model Cards document model details for governance; they are not a feature repository."
}
},
{
"id": "aif-c01-ai_services-011",
"certification_id": "AIF-C01",
"domain": "AI_SERVICES",
"difficulty": "easy",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "A company wants generative AI to boost developer productivity and will use Amazon Q Developer. What capability is aligned?",
"option_a": "Create code snippets, trace references, and manage open-source license obligations.",
"option_b": "Run an application without managing servers.",
"option_c": "Enable voice commands for coding and natural-language search.",
"option_d": "Convert audio files to text documents with ML models.",
"correct_answers": ["A"],
"explanation_detailed": "Amazon Q Developer is a generative AI assistant that accelerates coding tasks: drafting functions, unit tests, and refactors, while surfacing references and license considerations from dependencies. It integrates into IDEs and AWS services to recommend code and explanations in context. Serverless execution is an AWS compute pattern (e.g., Lambda), not a developer assistant function. Speech-to-text belongs to Amazon Transcribe, and voice assistants are separate capabilities. Q Developer’s value is targeted productivity in authoring, reviewing, and understanding code, while preserving compliance by tracking OSS licenses and using context from repositories and tickets.",
"incorrect_explanations": {
"B": "Serverless compute (e.g., AWS Lambda) addresses runtime management, not code generation and developer assistance.",
"C": "Voice control is not the core capability; Q Developer focuses on code generation, explanations, and integration with developer tools.",
"D": "Transcribing audio is Amazon Transcribe’s role, not Q Developer’s coding productivity function."
}
},
{
"id": "aif-c01-ai_services-012",
"certification_id": "AIF-C01",
"domain": "AI_SERVICES",
"difficulty": "medium",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "A financial institution uses Amazon Bedrock inside a VPC with no internet egress due to compliance. Which option enables private service connectivity?",
"option_a": "AWS PrivateLink",
"option_b": "Amazon Macie",
"option_c": "Amazon CloudFront",
"option_d": "Internet gateway",
"correct_answers": ["A"],
"explanation_detailed": "AWS PrivateLink provides private connectivity between VPCs and supported AWS services without traversing the public internet. It establishes interface VPC endpoints for Bedrock where available, keeping traffic within the AWS network to meet compliance requirements. Macie is for data-privacy discovery in S3. CloudFront is a CDN that relies on public endpoints. An Internet Gateway would break the “no internet egress” policy. With PrivateLink and tight IAM, you maintain least-privilege access while logging via CloudWatch and CloudTrail. Combine with VPC endpoint policies and S3 gateway endpoints for a fully private data-and-model interaction path.",
"incorrect_explanations": {
"B": "Macie classifies sensitive data in S3; it does not provide private network connectivity.",
"C": "CloudFront accelerates content delivery over the internet and is not a private link mechanism.",
"D": "An Internet Gateway enables internet access, violating the no-egress constraint."
}
},
{
"id": "aif-c01-ai_fundamentals-013",
"certification_id": "AIF-C01",
"domain": "AI_FUNDAMENTALS",
"difficulty": "easy",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "An educational game asks probability questions like drawing a green ball from a jar with known counts. What is the simplest approach?",
"option_a": "Use supervised learning to train a regression model.",
"option_b": "Use reinforcement learning to return the probability.",
"option_c": "Compute the probability with straightforward rules and calculations.",
"option_d": "Use unsupervised learning to estimate density.",
"correct_answers": ["C"],
"explanation_detailed": "This is a closed-form probability problem: P(green) = count(green)/count(total). Coding the rule is simpler, cheaper, and perfectly accurate, avoiding any ML overhead. In AWS, such logic can run in AWS Lambda or within an application backend without model training or hosting. ML is useful when relationships are unknown or complex; here, the relationship is explicit and deterministic. Training a model to approximate a trivial formula adds cost, latency, and risk of error. Keep ML for problems where it adds value, and apply direct computation for elementary combinatorial or probabilistic tasks.",
"incorrect_explanations": {
"A": "Regression would learn to approximate a known formula, adding training cost and possible error for a trivial task.",
"B": "Reinforcement learning is for sequential decision problems with rewards, not static closed-form probability calculations.",
"D": "Unsupervised density estimation is unnecessary when counts are explicit and formulaic."
}
},
{
"id": "aif-c01-ai_fundamentals-014",
"certification_id": "AIF-C01",
"domain": "AI_FUNDAMENTALS",
"difficulty": "easy",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "Which metric evaluates runtime efficiency of AI models?",
"option_a": "Customer satisfaction (CSAT).",
"option_b": "Training time per epoch.",
"option_c": "Average response time.",
"option_d": "Number of training instances.",
"correct_answers": ["C"],
"explanation_detailed": "Average response time (or latency) measures how quickly a model serves predictions, reflecting runtime efficiency. It is tracked at inference endpoints (for example, Amazon SageMaker real-time or serverless inference) and visualized in CloudWatch. Training time per epoch is a training metric, not serving efficiency. Instance count describes capacity, not efficiency. CSAT is a business outcome metric and may correlate with latency but does not measure it. Optimizing latency involves model compression, better hardware, batch sizing, and autoscaling policies, all observable through endpoint metrics and logs.",
"incorrect_explanations": {
"A": "CSAT is a perception metric and does not directly quantify serving latency.",
"B": "Epoch time relates to training speed, not production inference efficiency.",
"D": "Instance count indicates capacity; efficiency requires latency and throughput measurements."
}
},
{
"id": "aif-c01-ai_services-015",
"certification_id": "AIF-C01",
"domain": "AI_SERVICES",
"difficulty": "easy",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "A contact-center application needs insights from customer conversations by analyzing and extracting key information from call audio. Which AWS service should be used first?",
"option_a": "Amazon Lex to build a conversational bot.",
"option_b": "Amazon Transcribe to transcribe call recordings.",
"option_c": "Amazon SageMaker Model Monitor to extract information.",
"option_d": "Amazon Comprehend to create classification labels.",
"correct_answers": ["B"],
"explanation_detailed": "Start by converting audio to text using Amazon Transcribe, which provides accurate speech-to-text with timestamps, channel separation, and PII redaction. Once transcripts are available, you can apply Amazon Comprehend for sentiment, key phrases, entities, and custom classification. For generative summaries, invoke foundation models via Amazon Bedrock. Lex is used to build interactive bots, not offline analysis of recorded audio. Model Monitor tracks data/quality drift for deployed models, not basic transcription. This pipeline—Transcribe then NLP/LLM—unlocks structured insights for QA, compliance, and coaching.",
"incorrect_explanations": {
"A": "Lex is for building live conversational interfaces; it does not process historic audio recordings into text.",
"C": "Model Monitor detects drift in deployed models; it does not extract information from audio.",
"D": "Comprehend analyzes text; you first need transcripts created by Transcribe."
}
},
{
"id": "aif-c01-ai_fundamentals-016",
"certification_id": "AIF-C01",
"domain": "AI_FUNDAMENTALS",
"difficulty": "easy",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "A company has petabytes of unlabeled customer data to segment for marketing. Which learning approach fits?",
"option_a": "Supervised learning",
"option_b": "Unsupervised learning",
"option_c": "Reinforcement learning",
"option_d": "Reinforcement learning with human feedback (RLHF)",
"correct_answers": ["B"],
"explanation_detailed": "Unsupervised learning discovers patterns without labels, making it ideal for customer segmentation when only raw attributes exist. Clustering (e.g., k-means) or dimensionality reduction can group similar customers for targeted campaigns. On AWS, use SageMaker to train unsupervised algorithms and evaluate segments with downstream KPIs. Supervised methods require labeled targets, RL addresses sequential decision-making, and RLHF shapes reward functions for LLMs. Starting with unsupervised segmentation enables hypothesis generation that later can be validated with A/B testing and uplift modeling.",
"incorrect_explanations": {
"A": "Supervised learning requires labeled targets (e.g., churn yes/no), which are absent here.",
"C": "Reinforcement learning is for sequential decisions with rewards, not static segmentation.",
"D": "RLHF is a specialized technique for aligning generative models, not customer segmentation."
}
},
{
"id": "aif-c01-ai_services-017",
"certification_id": "AIF-C01",
"domain": "AI_SERVICES",
"difficulty": "medium",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "An AI search app must handle queries containing both text and images. What model type should be used?",
"option_a": "Multimodal embedding model",
"option_b": "Text-only embedding model",
"option_c": "Multimodal generation model",
"option_d": "Image generation model",
"correct_answers": ["A"],
"explanation_detailed": "Multimodal embedding models map heterogeneous inputs—text and images—into a shared vector space, enabling similarity search across modalities. This supports use cases such as “find images like this caption” or “match this photo to related documents.” With Amazon OpenSearch Service’s k-NN and vector indices, you can store embeddings and query nearest neighbors efficiently. Text-only embeddings miss image semantics, generation models synthesize outputs rather than index similarity. Multimodal embeddings, coupled with Bedrock-hosted encoders and vector databases, underpin robust cross-modal retrieval and RAG pipelines.",
"incorrect_explanations": {
"B": "Text-only embeddings cannot encode images; they break cross-modal retrieval requirements.",
"C": "Generation focuses on creating content, not producing comparable vectors for retrieval.",
"D": "Image generators synthesize pictures; they are not built for similarity search across text and images."
}
},
{
"id": "aif-c01-ai_services-018",
"certification_id": "AIF-C01",
"domain": "AI_SERVICES",
"difficulty": "medium",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "A Bedrock foundation model powering search must be made more accurate using company data. What should be provided for fine-tuning?",
"option_a": "Labeled data with prompt and completion fields",
"option_b": "A .txt file containing multiple lines in .csv format",
"option_c": "Provisioned throughput for Amazon Bedrock",
"option_d": "Train on newspapers and textbooks",
"correct_answers": ["A"],
"explanation_detailed": "For supervised fine-tuning of a foundation model, provide aligned prompt–completion pairs that reflect your domain and the desired outputs. This teaches the model to map enterprise-style queries to precise answers, formats, and constraints. On Amazon Bedrock, supported models expose fine-tuning APIs and expect structured JSONL with fields like instruction, input, and output. Provisioned throughput impacts capacity, not quality. Generic public corpora dilute domain specificity. Proper labeling, validation splits, and evaluation with business-relevant metrics ensure improved accuracy aligned to enterprise needs.",
"incorrect_explanations": {
"B": "Malformed files or arbitrary text do not supply the supervised signal needed for fine-tuning.",
"C": "Throughput affects scalability and latency, not accuracy of outputs.",
"D": "Training on generic data ignores company context, reducing domain alignment."
}
},
{
"id": "aif-c01-ai_fundamentals-019",
"certification_id": "AIF-C01",
"domain": "AI_FUNDAMENTALS",
"difficulty": "medium",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "An AI solution must verify whether an IP address is suspicious to protect an application. Which approach fits?",
"option_a": "Build a speech recognition system.",
"option_b": "Create a named entity recognition (NLP) system.",
"option_c": "Develop an anomaly detection system.",
"option_d": "Create a fraud prediction system.",
"correct_answers": ["C"],
"explanation_detailed": "Anomaly detection identifies patterns that deviate from normal traffic baselines, flagging suspicious IPs exhibiting abnormal request rates, geolocation shifts, or behavior. This can be done with unsupervised algorithms (e.g., Isolation Forest) when labeled attack data is limited. On AWS, use SageMaker to train and deploy anomaly detectors; pair with AWS WAF, CloudFront, and GuardDuty findings for defense-in-depth. Fraud models are typically supervised on labeled transactions. NER and speech recognition do not apply to network telemetry. Anomaly detection offers early warning and generalizes to novel threat signatures.",
"incorrect_explanations": {
"A": "Speech recognition transforms audio to text and is unrelated to network threat detection.",
"B": "NER extracts entities from text; it does not evaluate IP behavioral anomalies.",
"D": "Fraud prediction targets transactional patterns; IP reputation often benefits from unsupervised anomaly baselines."
}
},
{
"id": "aif-c01-ai_services-020",
"certification_id": "AIF-C01",
"domain": "AI_SERVICES",
"difficulty": "medium",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "Which Amazon OpenSearch Service capability enables vector database applications?",
"option_a": "Integration with Amazon S3 object storage",
"option_b": "Geospatial indexing and queries",
"option_c": "Scalable index management and k-NN nearest-neighbor search",
"option_d": "Real-time analytics on streaming data",
"correct_answers": ["C"],
"explanation_detailed": "Amazon OpenSearch Service supports k-NN vector indices that store high-dimensional embeddings and perform approximate nearest-neighbor search, enabling semantic similarity applications such as RAG, cross-modal search, and recommendation. You can scale shards and replicas, tune recall/latency trade-offs, and combine vector and keyword filters. S3 integration handles snapshots, geospatial is orthogonal to vectors, and streaming analytics use cases rely on OpenSearch ingest—not vector similarity. With Bedrock for embeddings and OpenSearch k-NN, you can build robust vector databases for production workloads.",
"incorrect_explanations": {
"A": "S3 snapshot integration is for backups and restores, not vector similarity search.",
"B": "Geospatial features target location queries, not high-dimensional embedding search.",
"D": "Streaming analytics is valuable but unrelated to nearest-neighbor vector retrieval."
}
},
{
"id": "aif-c01-ai_fundamentals-021",
"certification_id": "AIF-C01",
"domain": "AI_FUNDAMENTALS",
"difficulty": "easy",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "Which is a valid use case for generative AI models?",
"option_a": "Improve network security using intrusion detection systems.",
"option_b": "Create photorealistic images from textual descriptions for digital marketing.",
"option_c": "Improve database performance using optimized indexing.",
"option_d": "Analyze financial data to forecast stock market trends with ARIMA.",
"correct_answers": ["B"],
"explanation_detailed": "Generative models synthesize content—images, text, audio, or code—from prompts. For marketing, text-to-image generation can produce product visuals, lifestyle scenes, and variants quickly. With Amazon Bedrock, you can invoke image generation models through managed APIs and add guardrails to enforce brand policies. While AI aids security, database tuning, or time-series forecasting, those are not primarily generative tasks. Generative pipelines can be combined with retrieval for brand assets and SageMaker endpoints for approval workflows. This accelerates creative cycles while maintaining governance and cost controls.",
"incorrect_explanations": {
"A": "Intrusion detection focuses on anomaly or signature detection, not content generation.",
"C": "Index optimization is a database engineering task, not generative synthesis.",
"D": "Stock forecasting is predictive modeling; generative models target content creation."
}
},
{
"id": "aif-c01-ai_fundamentals-022",
"certification_id": "AIF-C01",
"domain": "AI_FUNDAMENTALS",
"difficulty": "easy",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "When choosing a foundation model in Amazon Bedrock, which concept determines how much information can fit in a single prompt?",
"option_a": "Temperature",
"option_b": "Context window",
"option_c": "Batch size",
"option_d": "Model size",
"correct_answers": ["B"],
"explanation_detailed": "The context window (context length) determines the maximum number of tokens the model can attend to across input and output. Larger windows allow longer prompts, more retrieved context, or multi-document conversations. In Bedrock, select models with sufficient context for your grounding strategy (e.g., RAG). Temperature controls randomness, not capacity. Batch size applies to throughput during training/inference, not per-request context capacity. Model size correlates with capability but does not guarantee a larger context. Align window size with your retrieval chunking and token budgets to avoid truncation.",
"incorrect_explanations": {
"A": "Temperature changes output randomness; it does not increase how many tokens fit in the prompt.",
"C": "Batch size affects parallelism/throughput, not prompt length capacity.",
"D": "Larger models may or may not have larger context windows; context length is a distinct parameter."
}
},
{
"id": "aif-c01-ai_services-023",
"certification_id": "AIF-C01",
"domain": "AI_SERVICES",
"difficulty": "easy",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "A company is building a customer-support chatbot with a foundation model. The bot must answer in the company’s tone. What should the team do?",
"option_a": "Set a very low token limit for outputs.",
"option_b": "Use batch inference to process detailed responses.",
"option_c": "Iteratively experiment and refine the prompt until the FM produces the desired style.",
"option_d": "Increase the temperature parameter.",
"correct_answers": ["C"],
"explanation_detailed": "Style alignment is primarily achieved through strong prompting: a system prompt that specifies tone, persona, brand vocabulary, and format, plus few-shot examples showing correct and incorrect answers. In Amazon Bedrock, you can enforce this with guardrails and max token settings to keep outputs concise. Temperature controls randomness, not brand tone. Batch inference is about throughput, not style. Token limits prevent verbosity but do not establish voice. Combine prompt iteration with evaluation and, if needed, fine-tuning on brand-approved conversation transcripts for best consistency.",
"incorrect_explanations": {
"A": "Lowering token limits reduces length, not tone alignment or stylistic consistency.",
"B": "Batch inference optimizes throughput; it does not shape style or tone.",
"D": "Higher temperature increases randomness and can drift from the target tone."
}
},
{
"id": "aif-c01-ai_services-024",
"certification_id": "AIF-C01",
"domain": "AI_SERVICES",
"difficulty": "easy",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "Using an LLM on Amazon Bedrock for sentiment analysis, how should the prompt be structured to classify snippets as positive or negative?",
"option_a": "Provide examples labeled positive or negative, followed by the new snippet to classify.",
"option_b": "Provide a detailed explanation of sentiment analysis and how LLMs work.",
"option_c": "Provide only the new snippet with no context.",
"option_d": "Provide the new snippet plus examples of unrelated tasks like summarization.",
"correct_answers": ["A"],
"explanation_detailed": "Few-shot prompting with labeled examples establishes the task, format, and decision criteria. Showing positive and negative samples teaches the model the boundary in your domain and improves consistency. In Bedrock, combine this with a clear instruction, output schema, and low temperature for determinism. Explanations of ML theory add no signal. Uncontextualized prompts yield variable outputs. Unrelated tasks confuse the instruction space. For higher accuracy, evaluate with labeled datasets and consider fine-tuning or using Amazon Comprehend for ready-made sentiment when your needs are standard.",
"incorrect_explanations": {
"B": "Theory descriptions do not provide task structure or decision boundaries for the model.",
"C": "Zero-context prompts lead to ambiguous, inconsistent classifications.",
"D": "Irrelevant examples pollute the prompt and degrade task adherence."
}
},
{
"id": "aif-c01-ai_services-025",
"certification_id": "AIF-C01",
"domain": "AI_SERVICES",
"difficulty": "medium",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "A security company runs FMs on Amazon Bedrock and must ensure only authorized users invoke models. They need to identify unauthorized access attempts to refine future IAM policies and roles. Which AWS service should be used?",
"option_a": "AWS Security Hub",
"option_b": "AWS CloudTrail",
"option_c": "Amazon GuardDuty",
"option_d": "AWS Trusted Advisor",
"correct_answers": ["B"],
"explanation_detailed": "AWS CloudTrail records API activity across AWS accounts, including Bedrock InvokeModel calls, failures, and access-denied events. Reviewing CloudTrail logs helps identify which principals attempted unauthorized actions and why (missing permissions, wrong roles, blocked resource policies). You can route events to CloudWatch Logs and create alarms or EventBridge rules for alerts. GuardDuty detects threats at the account/network level, Security Hub aggregates findings, and Trusted Advisor offers best-practice checks. For auditing access attempts to Bedrock specifically, CloudTrail is the authoritative source of evidence.",
"incorrect_explanations": {
"A": "Security Hub aggregates findings; it does not capture raw API access attempts for forensic analysis.",
"C": "GuardDuty flags suspicious behavior but is not the canonical record of Bedrock API calls and denials.",
"D": "Trusted Advisor provides guidance and checks, not detailed per-call access logs."
}
},
{
"id": "aif-c01-ai_services-026",
"certification_id": "AIF-C01",
"domain": "AI_SERVICES",
"difficulty": "medium",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "A company built an image-classification model and wants production predictions for a web app without managing infrastructure. What should they choose?",
"option_a": "Host the model in AWS Lambda.",
"option_b": "Use Amazon SageMaker Serverless Inference.",
"option_c": "Use Amazon API Gateway to host the model.",
"option_d": "Use AWS Fargate to host the model.",
"correct_answers": ["B"],
"explanation_detailed": "Amazon SageMaker Serverless Inference serves models without provisioning instances, automatically scaling based on traffic while charging for compute used during inference. It simplifies deployment for spiky or unpredictable loads and integrates with model registry, CI/CD, and observability. Lambda has short runtime and memory limits and is not ideal for typical ML frameworks. API Gateway fronts APIs; it does not run models. Fargate runs containers but requires you to manage model packaging, autoscaling, and GPU choices. Serverless Inference minimizes ops while preserving ML-specific deployment features.",
"incorrect_explanations": {
"A": "Lambda’s short timeouts and memory constraints limit many ML workloads and model sizes.",
"C": "API Gateway is a routing layer; it cannot execute ML models by itself.",
"D": "Fargate removes server management but still requires container ops and scaling logic for ML serving."
}
},
{
"id": "aif-c01-ai_services-027",
"certification_id": "AIF-C01",
"domain": "AI_SERVICES",
"difficulty": "medium",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "An AI company uses independent software vendors for periodic assessments and needs email notifications when new ISV compliance reports are available. Which AWS service should they use?",
"option_a": "AWS Control Tower",
"option_b": "AWS Artifact",
"option_c": "AWS Trusted Advisor",
"option_d": "AWS Security Hub",
"correct_answers": ["B"],
"explanation_detailed": "AWS Artifact is the portal for on-demand access to compliance reports and agreements from AWS and select ISVs. Teams can retrieve SOC, ISO, PCI, and other attestations centrally. While Artifact itself is a repository, you can integrate notifications via automation (e.g., polling with EventBridge Scheduler/Lambda) to alert stakeholders when new documents appear. Control Tower governs multi-account setups, Trusted Advisor provides optimization and best-practice checks, and Security Hub aggregates security findings. For compliance report access and distribution, Artifact is the primary source of truth.",
"incorrect_explanations": {
"A": "Control Tower manages account baselining and guardrails, not compliance document retrieval.",
"C": "Trusted Advisor focuses on cost, performance, and security checks, not compliance report delivery.",
"D": "Security Hub aggregates security findings; it does not host ISV compliance reports."
}
},
{
"id": "aif-c01-responsible_ai-028",
"certification_id": "AIF-C01",
"domain": "RESPONSIBLE_AI",
"difficulty": "medium",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "A company is building a conversational agent with an LLM and wants to reduce prompt-engineering attacks that coerce harmful or sensitive actions. What helps reduce this risk?",
"option_a": "Create a prompt template that trains the LLM to detect attack patterns.",
"option_b": "Increase the temperature parameter.",
"option_c": "Avoid LLMs not listed in Amazon SageMaker.",
"option_d": "Reduce the number of input tokens.",
"correct_answers": ["A"],
"explanation_detailed": "Defense-in-depth includes robust system prompts with explicit refusal policies, separators between instructions and user content, input/output validation, and attack-pattern detection (e.g., jailbreak cues). In Amazon Bedrock, combine templates with Guardrails for Bedrock to filter unsafe content and restrict tool actions. Temperature and token count do not address adversarial inputs. Model source listing does not guarantee safety against prompt injection. Incorporate retrieval whitelists, schema-validated outputs, and allow-list tool calls through Agents for Bedrock to further limit damage if instructions are subverted.",
"incorrect_explanations": {
"B": "Higher temperature increases randomness and can worsen adherence to safety policies.",
"C": "Model listing does not prevent prompt injection; safety comes from controls around the model.",
"D": "Shorter prompts alone do not neutralize adversarial content embedded by attackers."
}
},
{
"id": "aif-c01-responsible_ai-029",
"certification_id": "AIF-C01",
"domain": "RESPONSIBLE_AI",
"difficulty": "easy",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "Using a Generative AI Security Scope Matrix, which solution scope gives the company the greatest security responsibility?",
"option_a": "Use a third-party app with embedded generative AI features.",
"option_b": "Build an app using a third-party generative AI model (FM).",
"option_c": "Refine a third-party generative AI model (FM) with business data.",
"option_d": "Build and train a generative AI model from scratch with customer data.",
"correct_answers": ["D"],
"explanation_detailed": "Training a model from scratch transfers maximum responsibility to the company: data collection/consent, labeling quality, secure pipelines, model architecture, training infrastructure hardening, evaluation, red-teaming, and deployment controls. In AWS, this spans S3 data governance, SageMaker training security, KMS encryption, VPC isolation, ECR image scanning, and IAM least privilege. Using or fine-tuning third-party FMs shifts many controls to the provider. Embedded AI in SaaS leaves the least responsibility. The more you customize, the more you own the security surface and compliance obligations.",
"incorrect_explanations": {
"A": "Embedded AI in SaaS offloads the most responsibility to the vendor, not the company.",
"B": "Building on a third-party FM reduces responsibility versus training your own model stack.",
"C": "Fine-tuning adds responsibility but still leverages provider controls more than training from scratch."
}
},
{
"id": "aif-c01-ai_services-030",
"certification_id": "AIF-C01",
"domain": "AI_SERVICES",
"difficulty": "easy",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "An AI practitioner has a large photo database of animals and wants to automatically identify and categorize animals in images without manual effort. What strategy fits?",
"option_a": "Object detection",
"option_b": "Anomaly detection",
"option_c": "Named entity recognition",
"option_d": "Inpainting",
"correct_answers": ["A"],
"explanation_detailed": "Object detection localizes and classifies objects within images using bounding boxes or masks. Pretrained models adapted via transfer learning can recognize animal species and count occurrences. On AWS, Amazon Rekognition provides prebuilt labels, while SageMaker supports custom detectors (e.g., YOLO, Detectron2). Anomaly detection targets outliers, NER extracts entities from text, and inpainting fills missing image regions. For automatic categorization with minimal manual intervention, object detection and image classification are the appropriate computer vision approaches.",
"incorrect_explanations": {
"B": "Anomaly detection finds unusual patterns; it does not label known object classes in images.",
"C": "NER is an NLP technique applied to text, not images.",
"D": "Inpainting edits images by filling regions; it does not perform categorization."
}
},
{
"id": "aif-c01-ai_services-031",
"certification_id": "AIF-C01",
"domain": "AI_SERVICES",
"difficulty": "easy",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "A company wants to build with Amazon Bedrock on a limited budget and with flexibility, avoiding long-term commitments. Which pricing model fits?",
"option_a": "On-demand",
"option_b": "Model customization",
"option_c": "Provisioned throughput",
"option_d": "Spot instance",
"correct_answers": ["A"],
"explanation_detailed": "On-demand pricing in Amazon Bedrock charges per request/token without capacity reservations, ideal for experimentation and variable usage. Provisioned throughput reserves capacity for consistent, high-volume workloads at a committed rate. Model customization refers to fine-tuning cost, not a pricing plan for inference. Spot instances are EC2 compute pricing, not applicable to fully managed Bedrock model endpoints. Start on-demand, measure cost and latency, then consider provisioned throughput when traffic stabilizes and SLAs require predictable performance.",
"incorrect_explanations": {
"B": "Customization is an activity (fine-tuning), not a flexible pay-as-you-go inference pricing model.",
"C": "Provisioned throughput requires commitment and is optimized for steady high usage, not budget flexibility.",
"D": "Spot pricing applies to EC2 compute, not Bedrock’s managed model invocation."
}
},
{
"id": "aif-c01-ai_services-032",
"certification_id": "AIF-C01",
"domain": "AI_SERVICES",
"difficulty": "medium",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "Which AWS service helps a development team quickly deploy and consume a foundation model within its VPC?",
"option_a": "Amazon Personalize",
"option_b": "Amazon SageMaker JumpStart",
"option_c": "PartyRock (Amazon Bedrock playground)",
"option_d": "Amazon SageMaker endpoints",
"correct_answers": ["B"],
"explanation_detailed": "Amazon SageMaker JumpStart provides curated model catalogs, solution templates, and 1-click deployments into your account and VPC. Teams can stand up endpoints for foundation models or fine-tune quickly with integrated MLOps patterns. Personalize is a managed recommendation service, PartyRock is a public sandbox not tied to your VPC, and generic SageMaker endpoints require you to bring and configure the model. JumpStart accelerates secure, production-oriented deployments with guardrails, IAM, and observability baked in.",
"incorrect_explanations": {
"A": "Personalize targets recommendation use cases; it does not deploy general FMs into your VPC.",
"C": "PartyRock is a playground for experimentation, not a production VPC deployment path.",
"D": "Plain endpoints need a model and configuration; JumpStart streamlines selection and deployment."
}
},
{
"id": "aif-c01-responsible_ai-033",
"certification_id": "AIF-C01",
"domain": "RESPONSIBLE_AI",
"difficulty": "easy",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "How can companies use LLMs safely on Amazon Bedrock?",
"option_a": "Craft clear prompts and configure IAM roles/policies with least privilege.",
"option_b": "Enable AWS Audit Manager for automatic model assessments.",
"option_c": "Enable automatic model evaluation in Amazon Bedrock.",
"option_d": "Use CloudWatch Logs to make models explainable and monitor bias.",
"correct_answers": ["A"],
"explanation_detailed": "Secure LLM usage combines strong instruction design, least-privilege IAM, encryption, private connectivity (e.g., PrivateLink), and content controls (Guardrails for Bedrock). Clear prompts reduce ambiguity and misuse, while IAM restricts data and tool access to what’s necessary. Audit Manager helps evidence compliance but does not secure the model itself. Bedrock does not auto-assess all risks by default. CloudWatch captures logs and metrics but does not provide explainability or bias controls alone. Defense-in-depth includes prompt policies, retrieval allow-lists, schema-validated outputs, and continuous monitoring.",
"incorrect_explanations": {
"B": "Audit Manager assists with compliance evidence; it is not a direct control for LLM safety or access.",
"C": "There is no single toggle that ensures safe LLM use; you must implement multiple controls.",
"D": "Logs aid observability but do not enforce explainability or fairness guarantees."
}
},
{
"id": "aif-c01-ai_services-034",
"certification_id": "AIF-C01",
"domain": "AI_SERVICES",
"difficulty": "medium",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "A company has terabytes of data and wants an AI app that turns natural-language inputs into SQL for non-technical staff. What fits best?",
"option_a": "Generative Pre-trained Transformers (GPT)",
"option_b": "Residual neural network",
"option_c": "Support vector machine",
"option_d": "WaveNet",
"correct_answers": ["A"],
"explanation_detailed": "GPT-style LLMs excel at code synthesis from natural language, including SQL generation with schema hints and examples. On Amazon Bedrock, you can use foundation models to translate questions into SQL, validate against a schema, and execute in a controlled environment with guardrails. ResNets focus on vision tasks, SVMs are classical ML classifiers, and WaveNet targets audio. Add schema-aware validation, parameterized queries, and role-based access to prevent unsafe operations, and log prompts/results with CloudWatch for auditing.",
"incorrect_explanations": {
"B": "Residual networks are primarily used in computer vision and are not designed for text-to-SQL tasks.",
"C": "SVMs classify feature vectors; they do not generate executable SQL from natural language.",
"D": "WaveNet is an audio waveform generator; it is not a text-to-code model."
}
},
{
"id": "aif-c01-ai_fundamentals-035",
"certification_id": "AIF-C01",
"domain": "AI_FUNDAMENTALS",
"difficulty": "easy",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "A deep-learning object detector is deployed. When the model analyzes a new image to identify objects, what process is occurring?",
"option_a": "Training",
"option_b": "Inference",
"option_c": "Model deployment",
"option_d": "Bias mitigation",
"correct_answers": ["B"],
"explanation_detailed": "Inference is the application phase where a trained model processes new inputs to produce predictions. In production on Amazon SageMaker (real-time, serverless, or async endpoints), the model loads weights, executes forward passes, and returns detections (classes, boxes, scores). Training adjusts weights; deployment provisions serving infrastructure; bias mitigation addresses fairness during data/model prep. Monitoring latency, throughput, and accuracy drift with CloudWatch and Model Monitor ensures inference remains healthy over time.",
"incorrect_explanations": {
"A": "Training updates weights using labeled data; it does not describe using the model on unseen images.",
"C": "Deployment sets up the endpoint; it is not the act of predicting.",
"D": "Bias mitigation is a governance activity, not runtime prediction."
}
},
{
"id": "aif-c01-responsible_ai-036",
"certification_id": "AIF-C01",
"domain": "RESPONSIBLE_AI",
"difficulty": "medium",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "An AI practitioner generates images of people in different professions but finds biased outputs due to skewed input data. Which technique helps?",
"option_a": "Data augmentation for underrepresented classes",
"option_b": "Model monitoring for class distribution",
"option_c": "Retrieval-augmented generation (RAG)",
"option_d": "Watermark detection for images",
"correct_answers": ["A"],
"explanation_detailed": "Bias often stems from underrepresentation. Augmenting minority classes—through additional curated samples or transformations—helps balance the training distribution. Pair this with targeted collection and reweighting to correct skews. In SageMaker, use Clarify to measure pre-/post-training bias and track fairness metrics, then retrain with augmented data. Monitoring class distribution is diagnostic but not corrective, RAG affects text grounding, and watermark detection is unrelated. Balanced data is foundational to fairer generative outputs.",
"incorrect_explanations": {
"B": "Monitoring reveals skew but does not fix it; you must change the data or training objective.",
"C": "RAG improves factual grounding for language models, not visual class balance.",
"D": "Watermark checks provenance, not demographic or occupational representation in outputs."
}
},
{
"id": "aif-c01-ai_services-037",
"certification_id": "AIF-C01",
"domain": "AI_SERVICES",
"difficulty": "medium",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "A company is implementing Amazon Titan (FM) on Amazon Bedrock and must complement the model with relevant private data. What should they do?",
"option_a": "Use a different FM.",
"option_b": "Lower the temperature.",
"option_c": "Create a Bedrock knowledge base.",
"option_d": "Enable model invocation logging.",
"correct_answers": ["C"],
"explanation_detailed": "A Bedrock knowledge base indexes private documents, generates embeddings, and retrieves the most relevant chunks to ground model responses. This improves factuality without fine-tuning and keeps data under your control. Temperature affects randomness, not knowledge. Invocation logging aids observability. Switching models does not inject your domain content. Combine knowledge bases with guardrails, access control (IAM, VPC endpoints), and evaluation to keep answers accurate, contextual, and compliant.",
"incorrect_explanations": {
"A": "Changing models does not add your private knowledge; you need retrieval or fine-tuning.",
"B": "Lower temperature reduces randomness but does not supply domain facts.",
"D": "Logging is helpful for auditing but does not improve content grounding."
}
},
{
"id": "aif-c01-responsible_ai-038",
"certification_id": "AIF-C01",
"domain": "RESPONSIBLE_AI",
"difficulty": "medium",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "A medical company customizes a foundation model for diagnostics and must meet regulatory transparency. What should they use?",
"option_a": "Amazon Inspector for security and compliance setup",
"option_b": "Amazon SageMaker Clarify for metrics, reports, and example-based explanations",
"option_c": "Amazon Macie to encrypt and protect training data",
"option_d": "Collect more data and use Amazon Rekognition Custom Labels",
"correct_answers": ["B"],
"explanation_detailed": "Amazon SageMaker Clarify provides explainability (feature attributions such as SHAP), bias metrics, and reports at training and inference time. These artifacts support model cards and regulatory documentation, showing how inputs influence predictions and whether disparate impact exists. Inspector checks vulnerabilities in resources, Macie discovers sensitive data but does not explain models, and Rekognition relates to vision labeling. In regulated healthcare, combine Clarify with SageMaker Model Cards, data lineage, and human-in-the-loop review for a defensible transparency posture.",
"incorrect_explanations": {
"A": "Inspector targets vulnerability assessments; it does not generate model explanations or fairness metrics.",
"C": "Macie identifies sensitive data exposure; it is not an explainability tool.",
"D": "More data or different services do not replace formal explainability and bias reporting."
}
},
{
"id": "aif-c01-responsible_ai-039",
"certification_id": "AIF-C01",
"domain": "RESPONSIBLE_AI",
"difficulty": "medium",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "A company will deploy a conversational bot (fine-tuned via SageMaker JumpStart) and must demonstrate compliance with multiple regulatory frameworks. Which capabilities help? (Choose two.)",
"option_a": "Auto-scaling inference endpoints",
"option_b": "Threat detection",
"option_c": "Data protection",
"option_d": "Cost optimization",
"option_e": "Loosely coupled microservices",
"correct_answers": ["B", "C"],
"explanation_detailed": "Compliance narratives hinge on security controls: threat detection and data protection. AWS services such as GuardDuty, Security Hub, and CloudTrail help detect and aggregate suspicious activity and provide audit trails. Data protection covers encryption at rest/in transit (KMS, TLS), IAM least privilege, private networking (PrivateLink), and data minimization. Auto-scaling and microservices aid resilience and maintainability; cost optimization affects spend. For regulated chatbots, evidence strong detective controls and robust data-handling policies.",
"incorrect_explanations": {
"A": "Auto-scaling improves availability but does not directly address regulatory controls.",
"D": "Cost efficiency is valuable, but it is not a compliance control.",
"E": "Architecture style helps maintainability; compliance focuses on security and privacy controls."
}
},
{
"id": "aif-c01-ai_fundamentals-040",
"certification_id": "AIF-C01",
"domain": "AI_FUNDAMENTALS",
"difficulty": "easy",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "A team wants to raise a foundation model’s accuracy to an acceptable threshold. What action should they take?",
"option_a": "Reduce batch size.",
"option_b": "Increase the number of epochs.",
"option_c": "Decrease the number of epochs.",
"option_d": "Increase the temperature.",
"correct_answers": ["B"],
"explanation_detailed": "Increasing epochs gives the model more passes over the training set, often improving accuracy until overfitting begins. Monitor validation loss/metrics and apply early stopping. On SageMaker, track experiments and compare runs. Batch size affects optimization dynamics, not necessarily final accuracy. Temperature is an inference-time sampling parameter for generative models and unrelated to training. If gains stall, consider more data, transfer learning, regularization, or hyperparameter tuning via SageMaker Automatic Model Tuning.",
"incorrect_explanations": {
"A": "Batch size influences training dynamics but isn’t a guaranteed lever for accuracy.",
"C": "Fewer epochs reduce learning opportunity and often lower accuracy.",
"D": "Temperature changes output randomness at inference, not training quality."
}
},
{
"id": "aif-c01-ai_services-041",
"certification_id": "AIF-C01",
"domain": "AI_SERVICES",
"difficulty": "easy",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "A call-center wants to reduce the number of actions agents take to answer customer questions by deploying an LLM chatbot. Which business KPI best measures impact?",
"option_a": "Website engagement rate",
"option_b": "Average handle time (AHT)",
"option_c": "Corporate social responsibility",
"option_d": "Regulatory compliance",
"correct_answers": ["B"],
"explanation_detailed": "Average handle time reflects how efficiently customer issues are resolved. An effective chatbot that retrieves answers or automates steps should reduce AHT by pre-resolving or accelerating agent workflows. Website engagement is a marketing metric, CSR relates to ethics/sustainability, and compliance is a governance outcome. Pair AHT with containment rate, first-contact resolution, CSAT, and deflection metrics for a full picture. Use Amazon Bedrock for the LLM and integrate with enterprise systems via Agents for Bedrock.",
"incorrect_explanations": {
"A": "Site engagement is not directly tied to call-center efficiency or agent task reduction.",
"C": "CSR does not measure operational efficiency in support workflows.",
"D": "Compliance is necessary but does not quantify productivity gains from a chatbot."
}
},
{
"id": "aif-c01-ai_services-042",
"certification_id": "AIF-C01",
"domain": "AI_SERVICES",
"difficulty": "easy",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "What capability does Amazon SageMaker Clarify provide?",
"option_a": "Integrates a Retrieval-Augmented Generation (RAG) workflow",
"option_b": "Monitors production ML quality",
"option_c": "Documents critical details about ML models",
"option_d": "Identifies potential bias during data preparation",
"correct_answers": ["D"],
"explanation_detailed": "SageMaker Clarify measures bias in datasets and models and provides explainability via feature attributions (e.g., SHAP) at training and inference time. Reports help teams assess fairness and understand feature influence, supporting responsible AI. Model Monitor handles production drift/quality, not Clarify. Model Cards document models. RAG is a separate retrieval pipeline, often built with Bedrock and vector stores. Use Clarify in preprocessing, training, and post-deployment audits to track equity metrics and explain predictions.",
"incorrect_explanations": {
"A": "RAG involves retrieval and generation; Clarify focuses on bias and explainability, not retrieval.",
"B": "SageMaker Model Monitor tracks drift and quality; Clarify is for bias/explainability.",
"C": "Model Cards provide documentation; Clarify generates bias and explanation artifacts."
}
},
{
"id": "aif-c01-ai_fundamentals-043",
"certification_id": "AIF-C01",
"domain": "AI_FUNDAMENTALS",
"difficulty": "medium",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "A model predicts item prices well on training data but degrades significantly in production. What should the company do?",
"option_a": "Reduce the amount of training data.",
"option_b": "Add more hyperparameters to the model.",
"option_c": "Increase the amount of training data.",
"option_d": "Increase training time.",
"correct_answers": ["C"],
"explanation_detailed": "Generalization improves with more diverse, representative data that matches production conditions. Collect additional samples, reduce label noise, and refresh for seasonality or covariate shift. In SageMaker, retrain with new data and evaluate using holdout sets and backtesting. Adding hyperparameters or blindly training longer rarely fixes overfitting or data drift. Combine with feature engineering, regularization, and Model Monitor to detect drift early and trigger retraining pipelines.",
"incorrect_explanations": {
"A": "Less data often worsens generalization and increases overfitting risk.",
"B": "More hyperparameters increase complexity and may exacerbate overfitting without data improvements.",
"D": "Longer training can overfit further if data is not representative."
}
},
{
"id": "aif-c01-ai_services-044",
"certification_id": "AIF-C01",
"domain": "AI_SERVICES",
"difficulty": "medium",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "An e-commerce company wants to determine sentiment from written product reviews. Which AWS services satisfy this? (Choose two.)",
"option_a": "Amazon Lex",
"option_b": "Amazon Comprehend",
"option_c": "Amazon Polly",
"option_d": "Amazon Bedrock",
"option_e": "Amazon Rekognition",
"correct_answers": ["B", "D"],
"explanation_detailed": "Amazon Comprehend provides out-of-the-box sentiment analysis for text with APIs that return positive/negative/neutral/mixed labels and confidence. For customized tone, domain jargon, or multilingual nuance, an LLM on Amazon Bedrock can be prompted or fine-tuned to classify sentiment or extract rationales. Lex builds chatbots, Polly converts text to speech, and Rekognition analyzes images/video. Combine Comprehend for scale with targeted Bedrock prompts or fine-tuning for brand-specific sentiment rules.",
"incorrect_explanations": {
"A": "Lex orchestrates conversations; it does not analyze the sentiment of text corpora.",
"C": "Polly synthesizes speech from text, unrelated to sentiment classification.",
"E": "Rekognition is for vision tasks, not text sentiment."
}
},
{
"id": "aif-c01-ai_services-045",
"certification_id": "AIF-C01",
"domain": "AI_SERVICES",
"difficulty": "medium",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "A company will build an LLM chat interface for product manuals stored as PDFs. What is the most cost-effective approach?",
"option_a": "Add a single PDF to each user prompt via prompt engineering.",
"option_b": "Add all PDFs to each prompt via prompt engineering.",
"option_c": "Fine-tune a model on all PDFs and use it for prompts.",
"option_d": "Load the PDFs into a Bedrock knowledge base and use it for retrieval at query time.",
"correct_answers": ["D"],
"explanation_detailed": "A Bedrock knowledge base indexes PDFs into embeddings and retrieves only the relevant chunks per query, minimizing token usage and avoiding expensive fine-tuning. Stuffing entire documents into prompts is costly and often exceeds context limits. Fine-tuning for static manuals is unnecessary; retrieval keeps content fresh without retraining. This approach pairs well with guardrails, output schemas, and evaluation, delivering accurate, controlled answers at lower cost.",
"incorrect_explanations": {
"A": "Injecting a whole PDF into each prompt wastes tokens and risks truncation.",
"B": "Stuffing all PDFs is prohibitively costly and exceeds context windows.",
"C": "Fine-tuning is costly and inflexible for documents that change; retrieval is cheaper and fresher."
}
},
{
"id": "aif-c01-responsible_ai-046",
"certification_id": "AIF-C01",
"domain": "RESPONSIBLE_AI",
"difficulty": "medium",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "A social media company will use an LLM for content moderation and wants to evaluate outputs for bias and discrimination with minimal admin effort. Which data source should be used?",
"option_a": "User-generated content",
"option_b": "Moderation logs",
"option_c": "Content moderation guidelines",
"option_d": "Benchmark datasets",
"correct_answers": ["D"],
"explanation_detailed": "Benchmark datasets are pre-curated, standardized, and labeled for fairness/bias testing, enabling rapid, reproducible evaluation without heavy internal labeling. They help compare models and prompts consistently. User content and logs are valuable but require significant cleaning, labeling, and privacy handling. Guidelines inform policy but are not labeled test sets. Use benchmarks first, then validate with masked internal data to ensure alignment with platform norms and legal constraints.",
"incorrect_explanations": {
"A": "Raw user content demands heavy labeling and privacy work before fairness evaluation.",
"B": "Logs reflect past actions, not standardized test sets for bias measurement.",
"C": "Guidelines are policies, not datasets with labels for evaluation."
}
},
{
"id": "aif-c01-ai_services-047",
"certification_id": "AIF-C01",
"domain": "AI_SERVICES",
"difficulty": "easy",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "A marketing team will use a pretrained generative model to create campaign content and must ensure outputs align with brand voice and communication requirements. What should they do?",
"option_a": "Optimize architecture and hyperparameters of the base model.",
"option_b": "Add more layers to increase complexity.",
"option_c": "Craft effective prompts with clear instructions and contextual constraints.",
"option_d": "Pretrain a new generative model on a broad, diverse corpus.",
"correct_answers": ["C"],
"explanation_detailed": "Prompting defines persona, tone, do/don’t lists, style guides, and examples to constrain outputs to brand standards. In Bedrock, use system prompts, few-shot examples, and guardrails to filter disallowed themes. Architecture changes or pretraining are costly and unnecessary for style control. Start with prompts, then consider lightweight fine-tuning on approved brand assets for consistency at scale. Track outputs with human review loops for high-impact campaigns.",
"incorrect_explanations": {
"A": "Tuning base architecture is expensive and not required to enforce style guidelines.",
"B": "Adding layers raises complexity and cost without guaranteeing brand adherence.",
"D": "Pretraining a new model is overkill for brand voice; prompting and fine-tuning are sufficient."
}
},
{
"id": "aif-c01-responsible_ai-048",
"certification_id": "AIF-C01",
"domain": "RESPONSIBLE_AI",
"difficulty": "medium",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "A lending company is building a generative AI solution to offer discounts to new applicants and wants to minimize harmful bias. Which actions should they take? (Choose two.)",
"option_a": "Detect imbalances or disparities in the data.",
"option_b": "Ensure the model runs frequently.",
"option_c": "Evaluate model behavior to provide transparency to stakeholders.",
"option_d": "Use ROUGE to guarantee 100% accuracy.",
"option_e": "Ensure inference time is within accepted limits.",
"correct_answers": ["A", "C"],
"explanation_detailed": "Bias control starts with data audits: identify representation gaps and disparate impact across protected groups. Apply rebalancing, reweighting, or targeted collection. Evaluate behavior with fairness metrics and transparency artifacts (SageMaker Clarify, Model Cards). Frequency of runs, ROUGE (a summarization metric), and latency do not address fairness. Document governance, implement adverse-action explanations where applicable, and maintain monitoring to detect drift in both data and fairness metrics.",
"incorrect_explanations": {
"B": "Running often does not mitigate bias; data quality and evaluation do.",
"D": "ROUGE measures summary overlap; it does not assess fairness or guarantee accuracy.",
"E": "Latency is a performance metric, unrelated to bias control."
}
},
{
"id": "aif-c01-ai_services-049",
"certification_id": "AIF-C01",
"domain": "AI_SERVICES",
"difficulty": "medium",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "A company uses a Bedrock base model to summarize documents but trained a custom model to improve quality. What must they do to use the custom model via Bedrock?",
"option_a": "Purchase provisioned throughput for the custom model.",
"option_b": "Deploy the custom model to an Amazon SageMaker real-time endpoint.",
"option_c": "Register the model in SageMaker Model Registry only.",
"option_d": "Grant access to the custom model directly in Bedrock.",
"correct_answers": ["B"],
"explanation_detailed": "Custom models you train are typically hosted on SageMaker endpoints. Your application can orchestrate calls to Bedrock for base models and to SageMaker for your custom summarizer behind a single API layer. Provisioned throughput applies to Bedrock-managed models. Model Registry aids versioning/governance but does not serve traffic alone. Bedrock cannot automatically host your independently trained model unless integrated through your architecture. Use IAM and VPC endpoints to secure both paths and log inference with CloudWatch.",
"incorrect_explanations": {
"A": "Provisioned throughput is for Bedrock models; it does not host your custom model.",
"C": "Model Registry records versions; it does not create an inference endpoint.",
"D": "Bedrock does not directly host arbitrary external custom models without your deployment."
}
},
{
"id": "aif-c01-ai_services-050",
"certification_id": "AIF-C01",
"domain": "AI_SERVICES",
"difficulty": "medium",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "A company must select a Bedrock model for internal use and identify one that generates responses in employees’ preferred style. What should they do?",
"option_a": "Evaluate models using built-in prompt datasets only.",
"option_b": "Evaluate models with a human workforce and custom prompt datasets.",
"option_c": "Use public model rankings to pick the model.",
"option_d": "Use CloudWatch latency metrics while testing models.",
"correct_answers": ["B"],
"explanation_detailed": "Style alignment is subjective and domain-specific. Build a representative prompt set from real internal use cases and have human reviewers score outputs for tone, clarity, and policy compliance. Automate evaluation where possible, but keep human judgment central. Public leaderboards and latency metrics don’t capture style preferences. In Bedrock, test multiple FMs with the same prompts, apply guardrails, and consider lightweight fine-tuning on internal style guides or examples after selecting a candidate.",
"incorrect_explanations": {
"A": "Built-in prompts are generic and may not reflect your organization’s style needs.",
"C": "Public rankings rarely measure company-specific tone and constraints.",
"D": "Latency measures speed, not stylistic fit or quality."
}
},

{
"id": "aif-c01-ai_services-051",
"certification_id": "AIF-C01",
"domain": "AI_SERVICES",
"difficulty": "medium",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "An enterprise must pick an Amazon Bedrock model for internal use and identify a model that produces responses in the style preferred by employees. What should the company do?",
"option_a": "Evaluate models using built-in prompt datasets.",
"option_b": "Evaluate models with a human workforce and custom prompt datasets.",
"option_c": "Use public model leaderboards to choose the model.",
"option_d": "Use Amazon CloudWatch model invocation latency metrics during tests.",
"correct_answers": ["B"],
"explanation_detailed": "Style, tone, and task fit are subjective and domain-specific. The most reliable way to select an Amazon Bedrock foundation model for an organization’s preferred style is to run a structured human evaluation on prompts and references that reflect real internal use cases. Build a tailored prompt set from production data (sanitized) and ask a representative group of employees to rate outputs on tone, clarity, faithfulness, and policy adherence. Combine side-by-side comparisons and Likert ratings, then analyze inter-rater agreement. This method reveals preference alignment and failure modes that automated metrics and public leaderboards miss. On AWS, you can orchestrate evaluations with Bedrock model endpoints, store results in Amazon S3, and analyze in Amazon QuickSight or SageMaker notebooks.",
"incorrect_explanations": {
"A": "Built-in or generic prompt sets rarely mirror your company’s voice, domain terminology, or compliance constraints. They can screen gross quality but won’t measure alignment to your specific style preferences.",
"C": "Leaderboards optimize for public benchmarks, not your in-house voice, data, or safety policies. A top model on public tasks can still misalign with your brand style or compliance needs.",
"D": "Latency is an operational metric. It helps size and cost the solution but says nothing about stylistic alignment or response quality for your employees."
}
},
{
"id": "aif-c01-responsible_ai-052",
"certification_id": "AIF-C01",
"domain": "RESPONSIBLE_AI",
"difficulty": "easy",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "A college student copies generative-AI content to write essays. Which responsible AI challenge does this represent?",
"option_a": "Toxicity",
"option_b": "Hallucinations",
"option_c": "Plagiarism",
"option_d": "Privacy",
"correct_answers": ["C"],
"explanation_detailed": "Using model-generated text without attribution constitutes plagiarism. Responsible AI practices require clear provenance, citation, and policies that prevent misrepresenting generated content as original work. Educational and enterprise settings should set guidelines that require disclosure when AI tools assisted, and—where applicable—cite sources retrieved via augmented generation. On AWS, Guardrails for Amazon Bedrock can be configured to prompt disclosures and discourage misuse, and Amazon Bedrock logging can record prompts and outputs for audit. Content authenticity solutions and watermark detection (when available) can complement policy. The core issue here is not toxicity or privacy, but misattribution of authorship and failing to maintain academic integrity.",
"incorrect_explanations": {
"A": "Toxicity concerns harmful or offensive output. The scenario focuses on misattribution of authorship, not harmful language.",
"B": "Hallucinations are confident but incorrect outputs. Even if the text were accurate, copying it without attribution is still plagiarism.",
"D": "Privacy issues involve exposure of personal or sensitive data. The central problem here is improper attribution and authorship."
}
},
{
"id": "aif-c01-ai_services-053",
"certification_id": "AIF-C01",
"domain": "AI_SERVICES",
"difficulty": "medium",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "A company must train its own LLM using only private data and is concerned about environmental impact. Which Amazon EC2 instance type has the lowest environmental impact for training LLMs?",
"option_a": "EC2 C-series",
"option_b": "EC2 G-series",
"option_c": "EC2 P-series",
"option_d": "EC2 Trn-series",
"correct_answers": ["D"],
"explanation_detailed": "EC2 Trn instances (Trn1/Trn1n), powered by AWS Trainium, are purpose-built for energy- and cost-efficient training of deep learning models at scale. Relative to general GPU instances, Trainium can deliver higher performance per watt and lower total energy for large transformer workloads by providing higher throughput, mixed-precision acceleration, and optimized collective communications. Trainium integrates with the AWS Neuron SDK, PyTorch/XLA, and popular LLM stacks to reduce code changes while increasing hardware utilization. Using Trn can therefore reduce carbon and cost footprints for long-running LLM training compared to P-series GPUs, while C/G instances are not designed for large-scale training. Combine with managed storage (Amazon S3) and spot where appropriate to further optimize sustainability.",
"incorrect_explanations": {
"A": "C-series are compute-optimized CPUs, suitable for general compute but not efficient for large-scale deep learning training.",
"B": "G-series targets inference/graphics with smaller GPUs and is not optimized for cost- or energy-efficient LLM training.",
"C": "P-series GPUs are strong for training but typically consume more power per equivalent throughput than Trainium for transformer workloads."
}
},
{
"id": "aif-c01-ai_services-054",
"certification_id": "AIF-C01",
"domain": "AI_SERVICES",
"difficulty": "medium",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "A company will build a kid-friendly story generator on Amazon Bedrock and must ensure outputs and topics remain appropriate for children. Which AWS capability satisfies this?",
"option_a": "Amazon Rekognition",
"option_b": "Amazon Bedrock playgrounds",
"option_c": "Guardrails for Amazon Bedrock",
"option_d": "Agents for Amazon Bedrock",
"correct_answers": ["C"],
"explanation_detailed": "Guardrails for Amazon Bedrock provide policy controls to shape and constrain model behavior at inference time. You can define safety categories, disallowed topics, PII handling, and custom blocked word/phrase lists, then apply the guardrail to multiple Bedrock models consistently. This is ideal for a children’s storytelling app where you must filter violent, sexual, or age-inappropriate content and prevent the model from answering prompts outside acceptable scope. Guardrails also support response transformation (e.g., refusal or redaction) and logging for audits. Rekognition solves vision tasks, playgrounds are for experimentation, and Agents orchestrate tools/knowledge but do not enforce safety policies by themselves.",
"incorrect_explanations": {
"A": "Rekognition detects objects, text, and faces in images and video; it does not filter LLM text for age-appropriateness.",
"B": "Playgrounds are for ad-hoc testing; they don’t enforce production safety policies across endpoints.",
"D": "Agents orchestrate actions and tools. Without guardrails, they won’t inherently block unsafe or age-inappropriate content."
}
},
{
"id": "aif-c01-ai_fundamentals-055",
"certification_id": "AIF-C01",
"domain": "AI_FUNDAMENTALS",
"difficulty": "easy",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "A company must generate synthetic data based on existing data. Which model type fits?",
"option_a": "Generative Adversarial Network (GAN)",
"option_b": "XGBoost",
"option_c": "Residual neural network",
"option_d": "WaveNet",
"correct_answers": ["A"],
"explanation_detailed": "GANs pair a generator and a discriminator in a minimax game: the generator produces candidates and the discriminator distinguishes real from synthetic. Over training, the generator learns to produce realistic samples that match the distribution of real data. GANs are widely used to create images, tabular data, and even time series under privacy and imbalance constraints. On AWS, you can prototype in Amazon SageMaker using frameworks such as PyTorch or TensorFlow, track experiments with SageMaker Experiments, and store datasets in Amazon S3. XGBoost is a gradient-boosted decision tree algorithm for supervised tasks, residual networks are discriminative deep nets, and WaveNet targets audio generation, not general synthetic tabular/image data generation.",
"incorrect_explanations": {
"B": "XGBoost excels at classification/regression but does not generate new data distributions.",
"C": "ResNets are primarily discriminative architectures for vision; they are not generative models.",
"D": "WaveNet specializes in audio waveform generation; it’s inappropriate for generic synthetic data across modalities."
}
},
{
"id": "aif-c01-ai_services-056",
"certification_id": "AIF-C01",
"domain": "AI_SERVICES",
"difficulty": "medium",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "A device company wants demand forecasting for memory hardware, has no coding skills, and must analyze internal and external datasets. Which AWS solution fits?",
"option_a": "Store in Amazon S3 and train with built-in SageMaker algorithms from S3.",
"option_b": "Use SageMaker Data Wrangler and built-in algorithms directly.",
"option_c": "Use SageMaker Data Wrangler with Amazon Personalize Trending-Now.",
"option_d": "Use Amazon SageMaker Canvas to build models and forecasts no-code.",
"correct_answers": ["D"],
"explanation_detailed": "Amazon SageMaker Canvas provides a no-code interface for business users to build, evaluate, and deploy ML models using automated feature engineering and algorithm selection. It connects to data sources such as Amazon S3, Redshift, and third-party feeds, allowing users to create demand forecasts and what-if analyses without writing code. Canvas leverages SageMaker Autopilot under the hood and can hand off models to SageMaker Studio for experts to review. Data Wrangler is powerful for data prep but assumes practitioner skills; Personalize is a recommendation service, not a general forecasting engine; and training built-ins directly requires ML expertise and scripting. Canvas best fits the non-developer, multi-source forecasting need described.",
"incorrect_explanations": {
"A": "Training built-in algorithms from S3 needs coding and ML know-how, which the company lacks.",
"B": "Data Wrangler focuses on data preparation, not end-to-end no-code model building for non-experts.",
"C": "Amazon Personalize is for recommendation systems, not general demand forecasting across heterogeneous data."
}
},
{
"id": "aif-c01-responsible_ai-057",
"certification_id": "AIF-C01",
"domain": "RESPONSIBLE_AI",
"difficulty": "medium",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "A security camera model flags shoplifting disproportionately for one ethnic group. Which bias is most likely?",
"option_a": "Measurement bias",
"option_b": "Sampling bias",
"option_c": "Observer bias",
"option_d": "Confirmation bias",
"correct_answers": ["B"],
"explanation_detailed": "Sampling bias occurs when the training data under- or over-represents groups, contexts, or conditions, leading the model to learn skewed decision boundaries. In vision systems, if certain demographics are underrepresented or captured in different lighting, angles, or camera quality, the model may generalize poorly and yield disparate false positives. Responsible AI mitigation includes auditing dataset composition, rebalancing with targeted collection or augmentation, and evaluating using group-wise metrics (e.g., equalized odds, demographic parity). On AWS, you can use Amazon SageMaker Clarify to compute bias metrics pre- and post-training and to generate explainability artifacts. Measurement bias centers on faulty labels/sensors; observer and confirmation bias refer to human judgment and hypothesis-driven selection, respectively.",
"incorrect_explanations": {
"A": "Measurement bias concerns miscalibrated sensors or inconsistent labels, not group under-representation in the dataset.",
"C": "Observer bias arises from human annotators’ subjective judgments, not necessarily from dataset representativeness.",
"D": "Confirmation bias is selecting evidence to confirm a hypothesis. The scenario points to representation imbalance."
}
},
{
"id": "aif-c01-ai_fundamentals-058",
"certification_id": "AIF-C01",
"domain": "AI_FUNDAMENTALS",
"difficulty": "medium",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "A customer-service chatbot should improve by learning from past interactions and online resources. Which learning strategy enables continuous self-improvement?",
"option_a": "Supervised learning with a hand-curated set of good and bad replies",
"option_b": "Reinforcement learning with rewards for positive customer feedback",
"option_c": "Unsupervised learning to cluster similar customer queries",
"option_d": "Supervised learning with a continuously updated FAQ database",
"correct_answers": ["B"],
"explanation_detailed": "Reinforcement learning (RL) optimizes behavior by maximizing cumulative reward. For chatbots, you can design a reward signal from explicit ratings, resolution outcomes, or proxy signals (e.g., reduced handle time). An RL agent explores response strategies, receives feedback, and updates policies to improve future answers. On AWS, you can combine Amazon Bedrock for base LLMs, collect interaction telemetry in Amazon Kinesis/Firehose, and compute rewards with AWS Lambda or SageMaker RL stacks. Supervised fine-tuning on labeled pairs helps initial quality but doesn’t inherently optimize for long-term outcomes. Clustering organizes data but doesn’t optimize actions. Updating FAQs improves knowledge access, yet without a reward framework the model won’t systematically learn policies.",
"incorrect_explanations": {
"A": "Supervised datasets help initial accuracy but don’t optimize long-term conversational strategies via rewards.",
"C": "Clustering helps discover patterns but doesn’t adapt behavior based on feedback or outcomes.",
"D": "Feeding FAQs adds knowledge; it doesn’t define a reward mechanism to drive behavior optimization."
}
},
{
"id": "aif-c01-ai_fundamentals-059",
"certification_id": "AIF-C01",
"domain": "AI_FUNDAMENTALS",
"difficulty": "easy",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "An ML practitioner built a deep learning classifier for material types in images and wants to measure performance. Which metric helps?",
"option_a": "Confusion matrix",
"option_b": "Correlation matrix",
"option_c": "R² score",
"option_d": "Mean squared error (MSE)",
"correct_answers": ["A"],
"explanation_detailed": "A confusion matrix tabulates counts of true positives, false positives, true negatives, and false negatives per class. From it you derive accuracy, precision, recall, F1, and per-class error analysis. This is essential for multi-class vision tasks where aggregate accuracy can hide minority class failures. A correlation matrix measures feature correlations, not classification performance. R² and MSE are regression metrics and do not apply to discrete labels. On AWS, you can compute confusion matrices in Amazon SageMaker notebooks using scikit-learn, store artifacts in Amazon S3, and visualize class-wise errors to guide data collection and augmentation, threshold tuning, or class rebalancing.",
"incorrect_explanations": {
"B": "Correlation matrices reveal relationships among variables, not predictive classification outcomes.",
"C": "R² evaluates regression fit; it is not defined for categorical predictions.",
"D": "MSE is a regression loss and doesn’t directly describe classification error by class."
}
},
{
"id": "aif-c01-ai_services-060",
"certification_id": "AIF-C01",
"domain": "AI_SERVICES",
"difficulty": "medium",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "A chatbot answers natural-language questions with images. The company must ensure it never returns inappropriate images. What solution fits?",
"option_a": "Implement moderation APIs",
"option_b": "Retrain the model on a broad public dataset",
"option_c": "Perform model validation",
"option_d": "Automate user-feedback integration",
"correct_answers": ["A"],
"explanation_detailed": "Content moderation must happen at inference time to enforce policy reliably. Implement moderation APIs for both text prompts (to block unsafe requests) and image outputs (to analyze and filter generated or retrieved media). On AWS, combine Guardrails for Amazon Bedrock for prompt/output policies with Amazon Rekognition’s moderation APIs for image safety classes. You can also maintain deny-lists and add human-in-the-loop review for edge cases using Amazon A2I. Retraining may reduce risk but cannot guarantee compliance. Validation and feedback loops are valuable, yet without runtime moderation you risk policy violations slipping through.",
"incorrect_explanations": {
"B": "Retraining improves averages but cannot guarantee that no unsafe image will be produced or returned.",
"C": "Validation is pre-deployment testing; it does not enforce safety in real-time responses.",
"D": "Feedback improves future behavior but cannot reliably block unsafe content in the moment."
}
},
{
"id": "aif-c01-ai_services-061",
"certification_id": "AIF-C01",
"domain": "AI_SERVICES",
"difficulty": "medium",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "An AI practitioner uses an Amazon Bedrock base model to summarize customer-support chats and wants to store invocation logs to monitor inputs and outputs. What should they do?",
"option_a": "Configure AWS CloudTrail as the model’s log destination.",
"option_b": "Enable model invocation logging in Amazon Bedrock.",
"option_c": "Use AWS Audit Manager as the log destination for the model.",
"option_d": "Configure model invocation logging in Amazon EventBridge.",
"correct_answers": ["B"],
"explanation_detailed": "Amazon Bedrock provides native model invocation logging to capture prompts, responses, and metadata (subject to your retention and privacy policies). You can direct logs to Amazon S3 and integrate with AWS CloudTrail for control-plane auditing, but CloudTrail does not capture payloads. With Bedrock logging, teams can audit usage, debug failure cases, and build evaluation datasets. You should also apply data redaction policies, encryption with AWS KMS, and access controls via IAM. EventBridge is for event routing, not payload logging, and Audit Manager is for audit workflows and evidence collection rather than capturing model input/output content.",
"incorrect_explanations": {
"A": "CloudTrail records API calls and identities, not full model input/output payloads needed for quality monitoring.",
"C": "Audit Manager manages audit programs and evidence; it’s not used to capture inference payloads.",
"D": "EventBridge routes events; it is not a native mechanism to log Bedrock prompts and responses."
}
},
{
"id": "aif-c01-ai_services-062",
"certification_id": "AIF-C01",
"domain": "AI_SERVICES",
"difficulty": "medium",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "A company must run inference over multi-GB archived datasets and does not need immediate predictions. Which SageMaker inference option fits?",
"option_a": "Batch Transform",
"option_b": "Real-time inference",
"option_c": "Serverless inference",
"option_d": "Asynchronous inference",
"correct_answers": ["A"],
"explanation_detailed": "SageMaker Batch Transform processes large datasets asynchronously by reading from Amazon S3, running inference at scale, and writing outputs back to S3. It’s ideal when latency is not critical, instances can be right-sized for throughput, and you avoid managing request/response endpoints. Real-time and serverless endpoints target millisecond-second latencies and request/response workloads. Asynchronous inference is better for large individual payloads that still return per-request results via callback, not full dataset jobs. For multi-GB archives and non-interactive SLAs, Batch Transform minimizes operational overhead and cost.",
"incorrect_explanations": {
"B": "Real-time endpoints are optimized for low-latency per-request inference, not bulk processing of archives.",
"C": "Serverless inference suits sporadic, small payload request/response workloads, not bulk batch scoring.",
"D": "Asynchronous inference handles large single requests but is not optimized for full-dataset batch jobs."
}
},
{
"id": "aif-c01-ai_fundamentals-063",
"certification_id": "AIF-C01",
"domain": "AI_FUNDAMENTALS",
"difficulty": "easy",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "Which term describes numerical representations of real-world objects and concepts that help NLP/AI models understand text?",
"option_a": "Embeddings",
"option_b": "Tokens",
"option_c": "Models",
"option_d": "Binaries",
"correct_answers": ["A"],
"explanation_detailed": "Embeddings map discrete symbols (words, phrases, images) to dense vectors such that semantic similarity corresponds to geometric proximity. They power retrieval, clustering, classification, and semantic search. In LLM applications, text embeddings enable RAG pipelines using vector databases (e.g., OpenSearch k-NN) to find contextually relevant passages. Tokens are the atomic input/output units; models are the architectures/parameters; binaries are compiled artifacts. On AWS, you can compute embeddings with Amazon Bedrock text embedding models and store them in Amazon OpenSearch Service or Aurora PostgreSQL with pgvector to support similarity search at scale.",
"incorrect_explanations": {
"B": "Tokens are units of input/output, not semantic vector representations for similarity computations.",
"C": "A model is the learned function; embeddings are data representations the model uses.",
"D": "Binaries are compiled executables, unrelated to semantic representations."
}
},
{
"id": "aif-c01-ml_development-064",
"certification_id": "AIF-C01",
"domain": "ML_DEVELOPMENT",
"difficulty": "medium",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "A research company used an Amazon Bedrock FM for a Q&A chatbot over scientific articles. Prompt engineering failed due to complex terminology. How can they improve performance?",
"option_a": "Use few-shot prompting to define answer style.",
"option_b": "Apply domain-adaptation fine-tuning to teach scientific terminology.",
"option_c": "Change inference parameters.",
"option_d": "Clean articles to remove complex terms.",
"correct_answers": ["B"],
"explanation_detailed": "Domain adaptation fine-tunes a base FM on domain-specific corpora so the model internalizes specialized vocabulary, syntax, and discourse patterns. For scientific Q&A, fine-tuning on curated articles, glossaries, and expert-authored Q&A pairs can materially improve comprehension and factual grounding. On Amazon Bedrock, use model customization (where supported) with labeled prompt-completion pairs and evaluate via held-out domain benchmarks. Few-shot prompting and parameter tweaks can help style and determinism but rarely close knowledge gaps for highly technical language. Removing complex terms destroys needed signal. Combining fine-tuning with retrieval (Bedrock Knowledge Bases or OpenSearch) yields further gains.",
"incorrect_explanations": {
"A": "Few-shot improves formatting and task induction but cannot teach deep domain vocabulary comprehensively.",
"C": "Temperature, top-p, etc., affect diversity and determinism, not domain knowledge acquisition.",
"D": "Deleting complex terms removes essential information and undermines answer quality."
}
},
{
"id": "aif-c01-ai_fundamentals-065",
"certification_id": "AIF-C01",
"domain": "AI_FUNDAMENTALS",
"difficulty": "medium",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "To make an LLM on Amazon Bedrock produce more consistent answers to the same prompt, which inference parameter change helps most?",
"option_a": "Lower the temperature",
"option_b": "Increase the temperature",
"option_c": "Reduce maximum output tokens",
"option_d": "Increase maximum generation length",
"correct_answers": ["A"],
"explanation_detailed": "Temperature controls sampling randomness. Lower values bias the distribution toward higher-probability tokens, increasing determinism and repeatability of outputs for the same input—useful for classification, extraction, and policy-bound tasks. On Bedrock, pair low temperature with fixed seeds (where available) and constrained decoding when possible. Changing max tokens affects length, not variability; increasing length can even introduce more variation late in generation. Top-p/top-k adjustments also influence randomness, but temperature is the principal knob for global sampling sharpness. Combine with robust prompts and, if needed, guardrails for format enforcement.",
"incorrect_explanations": {
"B": "Higher temperature increases randomness and diversity—opposite of consistency.",
"C": "Limiting tokens truncates outputs but does not fundamentally reduce sampling variance.",
"D": "Longer outputs can introduce additional variability and drift without improving consistency."
}
},
{
"id": "aif-c01-ai_services-066",
"certification_id": "AIF-C01",
"domain": "AI_SERVICES",
"difficulty": "medium",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "A company builds an LLM app on Amazon Bedrock with customer data in S3. Security policy: each team can access only its own customers’ data. What should they do?",
"option_a": "Create a dedicated Amazon Bedrock service role per team scoped only to that team’s S3 data.",
"option_b": "Create a single service role with S3 access and rely on teams to pass a customer name in each request.",
"option_c": "Redact PII in S3 and open S3 access broadly for teams.",
"option_d": "Give Bedrock full S3 access and restrict via team IAM roles to folders.",
"correct_answers": ["A"],
"explanation_detailed": "Use least-privilege IAM with a distinct Amazon Bedrock service role per team. Scope each role’s S3 permissions (resource ARNs and condition keys like aws:PrincipalTag or s3:prefix) to that team’s buckets/prefixes. This ensures Bedrock invocations on behalf of a team can only access that team’s customer data. Passing a customer name is not access control. Broad S3 access violates policy even if data is redacted. Granting Bedrock full S3 access undermines isolation and complicates auditing. Combine with KMS encryption, S3 bucket policies, and CloudTrail for access auditing.",
"incorrect_explanations": {
"B": "Relying on user input is not an authorization boundary and is error-prone.",
"C": "PII redaction does not enforce tenant isolation; broad access still violates policy.",
"D": "Full S3 access to Bedrock breaks least privilege and increases blast radius."
}
},
{
"id": "aif-c01-ai_services-067",
"certification_id": "AIF-C01",
"domain": "AI_SERVICES",
"difficulty": "medium",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "A medical company deployed a disease-detection model on Amazon Bedrock. It must prevent patient PII from appearing in responses and receive notifications on policy violations. What fits?",
"option_a": "Use Amazon Macie to scan outputs and alert on sensitive data.",
"option_b": "Use AWS CloudTrail to monitor responses and alert on PII.",
"option_c": "Use Guardrails for Amazon Bedrock to filter content and CloudWatch alarms for violation notifications.",
"option_d": "Use SageMaker Model Monitor for data drift and quality alerts.",
"correct_answers": ["C"],
"explanation_detailed": "Guardrails for Amazon Bedrock can detect and redact PII or block responses that violate content policies, preventing sensitive data from being returned. Configure guardrails with PII detection, custom deny lists, and safe-response transformations. Stream logs to CloudWatch and set metric filters/alarms for violation events to notify teams. Macie discovers sensitive data at rest, not dynamically in generated outputs. CloudTrail tracks API calls, not response payloads. Model Monitor focuses on drift and data quality for SageMaker endpoints, not Bedrock content filtering. Guardrails + CloudWatch is the correct runtime control and alerting combination.",
"incorrect_explanations": {
"A": "Macie classifies data at rest in S3; it doesn’t filter or redact model outputs in real time.",
"B": "CloudTrail logs control-plane events and lacks output payload inspection for PII.",
"D": "Model Monitor detects drift/quality on SageMaker endpoints, not Bedrock content policy enforcement."
}
},
{
"id": "aif-c01-ai_services-068",
"certification_id": "AIF-C01",
"domain": "AI_SERVICES",
"difficulty": "easy",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "The company must convert many PDF resumes to plain text for automated processing. Which AWS service fits?",
"option_a": "Amazon Textract",
"option_b": "Amazon Personalize",
"option_c": "Amazon Lex",
"option_d": "Amazon Transcribe",
"correct_answers": ["A"],
"explanation_detailed": "Amazon Textract extracts text, forms, tables, and key-value pairs from scanned PDFs and images. It handles variable layouts and returns structured JSON for downstream parsing. For resumes, you can push documents from S3 to Textract via asynchronous jobs, store results back in S3, and index them in OpenSearch or a database. Transcribe converts speech to text, Lex builds chatbots, and Personalize is for recommendations. Textract is purpose-built for document OCR and structure extraction needed in resume processing pipelines.",
"incorrect_explanations": {
"B": "Personalize builds recommendation systems, unrelated to OCR and PDF text extraction.",
"C": "Lex is a conversational interface service; it does not parse documents.",
"D": "Transcribe handles audio-to-text, not image/PDF OCR."
}
},
{
"id": "aif-c01-ai_services-069",
"certification_id": "AIF-C01",
"domain": "AI_SERVICES",
"difficulty": "medium",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "An education provider builds a Q&A app with a generative model and wants the response style to adapt automatically to the user’s age group (provided to the model). What is the lowest-effort solution?",
"option_a": "Fine-tune the model on datasets spanning all supported ages.",
"option_b": "Add a role/system description in the prompt instructing the model on the target age group.",
"option_c": "Use chain-of-thought reasoning to infer the correct style.",
"option_d": "Post-summarize responses shorter for younger users.",
"correct_answers": ["B"],
"explanation_detailed": "Prompt conditioning is the lightest-weight method to adapt tone and complexity. Include role instructions such as: “You are a tutor. Target audience: {age group}. Use vocabulary and examples appropriate for this age.” Provide style exemplars (few-shot) if needed and enforce length constraints. This approach avoids training costs and is easy to iterate. Fine-tuning can help but is excessive for simple style control. Chain-of-thought affects reasoning steps, not guaranteed stylistic adaptation, and may leak internal text. Post-summarization may shorten text but won’t reliably adjust vocabulary, structure, or conceptual load to the user’s age.",
"incorrect_explanations": {
"A": "Fine-tuning is costly and slower to iterate; overkill when simple prompt control suffices.",
"C": "Reasoning traces do not inherently tailor tone or vocabulary to age groups.",
"D": "Shortening alone does not ensure age-appropriate vocabulary or pedagogy."
}
},
{
"id": "aif-c01-ml_development-070",
"certification_id": "AIF-C01",
"domain": "ML_DEVELOPMENT",
"difficulty": "medium",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "How should you assess the accuracy of a foundation model used for image classification?",
"option_a": "Compute total resource cost used by the model.",
"option_b": "Measure accuracy against a predefined benchmark dataset.",
"option_c": "Count the number of layers in the neural network.",
"option_d": "Evaluate color fidelity of processed images.",
"correct_answers": ["B"],
"explanation_detailed": "To evaluate a classifier, use a held-out benchmark dataset with ground truth labels and compute accuracy and class-wise metrics (precision, recall, F1). Benchmarks enable apples-to-apples comparisons across models and versions. For robust evaluation, stratify by sub-populations and conditions (lighting, occlusion) and analyze confusion matrices to find systematic errors. On AWS, store datasets in S3, run evaluations in SageMaker notebooks, and visualize metrics in Amazon QuickSight. Resource cost and architecture size don’t guarantee predictive quality; color fidelity is irrelevant to label accuracy unless the task is explicitly about color reproduction.",
"incorrect_explanations": {
"A": "Cost indicates efficiency, not the model’s predictive correctness on labeled images.",
"C": "Depth alone does not predict accuracy; training, data, and regularization matter.",
"D": "Color fidelity is unrelated unless color is the target label; classification accuracy needs labeled benchmarks."
}
},
{
"id": "aif-c01-responsible_ai-071",
"certification_id": "AIF-C01",
"domain": "RESPONSIBLE_AI",
"difficulty": "medium",
"tier": "FREE",
"required_selection_count": 2,
"active": true,
"question_text": "An accounting firm will deploy an LLM to automate document processing and must proceed responsibly to avoid harm. What should they do? (Choose two.)",
"option_a": "Include fairness metrics in model evaluation.",
"option_b": "Tune the model’s temperature parameter.",
"option_c": "Modify training data to mitigate biases.",
"option_d": "Avoid overfitting on training data.",
"option_e": "Apply prompt-engineering techniques.",
"correct_answers": ["A", "C"],
"explanation_detailed": "Responsible AI requires measuring and mitigating harms. Incorporate fairness metrics (e.g., demographic parity, equalized odds) and group-wise evaluations to detect disparate performance across document types, client segments, or languages. If disparities appear, adjust data sampling, reweight records, or collect targeted data to reduce bias. Temperature tuning and prompt engineering affect style and randomness but do not address structural bias. General overfitting control is good practice but is not specific to responsible-AI risk. On AWS, use SageMaker Clarify for bias analysis and explainability, Bedrock Guardrails for policy enforcement and PII controls, and CloudWatch plus Bedrock logging for auditability.",
"incorrect_explanations": {
"B": "Temperature controls randomness; it does not measure or correct systemic bias.",
"D": "Avoiding overfitting is standard ML hygiene, not a specific fairness/bias mitigation step.",
"E": "Prompting can guide style and format but won’t correct structural data bias by itself."
}
},
{
"id": "aif-c01-ml_development-072",
"certification_id": "AIF-C01",
"domain": "ML_DEVELOPMENT",
"difficulty": "easy",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "A company collected new data, computed a correlation matrix, basic statistics, and visualizations. Which ML pipeline stage is this?",
"option_a": "Data preprocessing",
"option_b": "Feature engineering",
"option_c": "Exploratory Data Analysis (EDA)",
"option_d": "Hyperparameter tuning",
"correct_answers": ["C"],
"explanation_detailed": "Exploratory Data Analysis surfaces structure, distributions, correlations, outliers, and missingness before modeling decisions. EDA informs preprocessing (imputation, scaling), feature engineering (domain-informed transforms), and model selection. On AWS, analysts typically run EDA in SageMaker Studio notebooks with Pandas/Seaborn/Matplotlib, reading data from S3, and may publish summaries to QuickSight for stakeholders. Preprocessing and feature engineering are follow-on steps guided by EDA findings; hyperparameter tuning comes after you define a modeling approach and features.",
"incorrect_explanations": {
"A": "Preprocessing applies transformations; EDA precedes and informs those choices.",
"B": "Feature engineering creates/Transforms variables after understanding data via EDA.",
"D": "Tuning occurs post-model selection; not at the data exploration stage."
}
},
{
"id": "aif-c01-ai_fundamentals-073",
"certification_id": "AIF-C01",
"domain": "AI_FUNDAMENTALS",
"difficulty": "medium",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "Some documents lost words due to a database error. The company wants a model to suggest likely words to fill the gaps. Which model type fits?",
"option_a": "Topic modeling",
"option_b": "Clustering models",
"option_c": "Prescriptive ML models",
"option_d": "BERT-based models",
"correct_answers": ["D"],
"explanation_detailed": "BERT and similar masked-language models are trained to reconstruct missing tokens given both left and right context. This bidirectional conditioning yields strong performance on fill-in-the-blank tasks. For production, you can deploy a BERT variant on SageMaker or use a Bedrock FM with masking prompts. Topic models and clustering are unsupervised and don’t predict specific missing words. Prescriptive ML refers to decision optimization, not token reconstruction. Ensure privacy by stripping PII before inference and log predictions to S3 with versioning to audit edits.",
"incorrect_explanations": {
"A": "Topic modeling uncovers latent themes, not precise token prediction for gaps.",
"B": "Clustering groups similar items; it doesn’t infer exact missing words.",
"C": "Prescriptive analytics optimize actions, not text completion."
}
},
{
"id": "aif-c01-ai_services-074",
"certification_id": "AIF-C01",
"domain": "AI_SERVICES",
"difficulty": "medium",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "A company wants automated charts of total sales for top products across retail locations for the last 12 months. Which AWS solution should they use?",
"option_a": "Amazon Q on Amazon EC2",
"option_b": "Amazon Q Developer",
"option_c": "Amazon Q in Amazon QuickSight",
"option_d": "Amazon Q in AWS Chatbot",
"correct_answers": ["C"],
"explanation_detailed": "Amazon QuickSight with Amazon Q provides natural-language-driven BI. Analysts can ask, “Show last 12 months’ sales for top products by location,” and Q builds visuals from governed semantic models. Schedule dashboards, set row-level security, and share insights. Q Developer focuses on code and software tasks; EC2 is unnecessary for managed BI; AWS Chatbot integrates chat with AWS ops, not BI visualizations. For sales analytics across stores, QuickSight + Q is the correct managed analytics solution on AWS.",
"incorrect_explanations": {
"A": "Running Q on EC2 is unnecessary; QuickSight is the managed BI service with Q built in.",
"B": "Q Developer targets developer workflows, not BI chart generation for business users.",
"D": "AWS Chatbot bridges chat tools with AWS events; it does not build BI dashboards."
}
},
{
"id": "aif-c01-ml_development-075",
"certification_id": "AIF-C01",
"domain": "ML_DEVELOPMENT",
"difficulty": "medium",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "A company uses an Amazon Bedrock LLM for intent detection in a chatbot and wants to apply few-shot learning. What additional data do they need?",
"option_a": "Chatbot replies paired with users’ correct intents",
"option_b": "User messages paired with correct chatbot replies",
"option_c": "User messages paired with correct user intents",
"option_d": "User intents paired with correct chatbot replies",
"correct_answers": ["C"],
"explanation_detailed": "Few-shot prompting teaches the model by example. For intent classification, provide user utterances labeled with the correct intent names. The LLM then generalizes from the examples to new utterances. Replies are irrelevant to training the intent labeler and may confound the task. On AWS, store labeled examples in S3, load them into your Bedrock prompt templates, and evaluate performance on a held-out labeled set. If volume grows, consider fine-tuning or a small supervised classifier served on SageMaker.",
"incorrect_explanations": {
"A": "Responses do not define the true intent; you need utterance→intent pairs.",
"B": "Pairing with replies frames a generative task, not intent classification labels.",
"D": "Intent→reply pairs are useful for response selection, not intent detection."
}
},
{
"id": "aif-c01-ai_services-076",
"certification_id": "AIF-C01",
"domain": "AI_SERVICES",
"difficulty": "medium",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "A Bedrock base model uses 10 few-shot examples in the prompt, is invoked once per day, performs well, and the company wants to reduce monthly cost. What should they do?",
"option_a": "Customize the model via fine-tuning.",
"option_b": "Reduce the number of tokens in the prompt.",
"option_c": "Increase the number of tokens in the prompt.",
"option_d": "Use Provisioned Throughput.",
"correct_answers": ["B"],
"explanation_detailed": "Bedrock pricing is token-based. Reducing prompt tokens—by compressing instructions, abbreviating examples, or replacing some with a compact schema—lowers cost without changing infra. One daily invocation doesn’t justify Provisioned Throughput. Fine-tuning may reduce prompt length but adds training cost and operational complexity. Increasing tokens raises cost. Maintain quality by selecting the most informative few-shot examples and enforcing concise formatting.",
"incorrect_explanations": {
"A": "Fine-tuning incurs training cost and overhead; unnecessary for a single daily call.",
"C": "More tokens directly increase cost without proven quality gains here.",
"D": "Provisioned Throughput is for sustained, high-volume workloads—not one call/day."
}
},
{
"id": "aif-c01-ai_fundamentals-077",
"certification_id": "AIF-C01",
"domain": "AI_FUNDAMENTALS",
"difficulty": "easy",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "An LLM generates plausible but factually incorrect marketing copy. What problem is this?",
"option_a": "Data leakage",
"option_b": "Hallucination",
"option_c": "Overfitting",
"option_d": "Underfitting",
"correct_answers": ["B"],
"explanation_detailed": "Hallucination is when a model produces confident but false statements. It arises from gaps in training data, distribution shift, or unconstrained generation. Mitigation: add retrieval-augmented generation (Amazon Bedrock Knowledge Bases or OpenSearch), lower temperature for deterministic tasks, constrain outputs with schemas, and add post-hoc verification. Overfitting/underfitting refer to training dynamics, not inference factuality; data leakage is unintended train/test contamination. Logging prompts/outputs for audits and adding guardrails improve reliability in production.",
"incorrect_explanations": {
"A": "Data leakage concerns training with test or sensitive data, not plausible but wrong outputs.",
"C": "Overfitting is a training issue; it doesn’t directly explain fabricated facts at inference.",
"D": "Underfitting is under-learning patterns, not fabricating believable details."
}
},
{
"id": "aif-c01-responsible_ai-078",
"certification_id": "AIF-C01",
"domain": "RESPONSIBLE_AI",
"difficulty": "medium",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "A practitioner fine-tuned a Bedrock model with sensitive data and wants to ensure inferences don’t expose those sensitive elements. How should they prevent exposure?",
"option_a": "Delete the model, remove sensitive data from training, and retrain.",
"option_b": "Dynamically mask sensitive data in inference responses.",
"option_c": "Encrypt sensitive data in inference responses with SageMaker.",
"option_d": "Encrypt sensitive data inside the model with AWS KMS.",
"correct_answers": ["B"],
"explanation_detailed": "Apply runtime PII detection and redaction to outputs. With Guardrails for Amazon Bedrock, configure PII policies to redact names, addresses, IDs, and custom patterns. You can also integrate Amazon Comprehend PII detection or custom regex to mask outputs before returning them to clients. This prevents accidental disclosure without retraining. Encryption protects data at rest/in transit but doesn’t stop the model from generating sensitive content. Full retraining may be ideal but is costly; runtime masking provides immediate protection and auditability.",
"incorrect_explanations": {
"A": "Retraining may help but is expensive and slow; runtime controls protect immediately.",
"C": "Encrypting responses still returns sensitive content to the caller once decrypted; it doesn’t prevent disclosure.",
"D": "KMS secures stored artifacts, not the semantic content the model chooses to emit."
}
},
{
"id": "aif-c01-ai_fundamentals-079",
"certification_id": "AIF-C01",
"domain": "AI_FUNDAMENTALS",
"difficulty": "medium",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "A company translates training manuals from English to other languages with LLMs and wants to evaluate translation accuracy. Which metric fits?",
"option_a": "BLEU (Bilingual Evaluation Understudy)",
"option_b": "RMSE",
"option_c": "ROUGE",
"option_d": "F1 score",
"correct_answers": ["A"],
"explanation_detailed": "BLEU measures n-gram overlap between a candidate translation and one or more human references, capturing precision of translated segments. It’s a standard automatic metric for machine translation evaluation. ROUGE targets summarization recall, RMSE is a regression error metric, and F1 is for classification. For robust assessment, pair BLEU with human evaluation for fluency and adequacy, and track domain terms with custom glossaries. On AWS, store references in S3, compute BLEU in SageMaker notebooks, and visualize trends in QuickSight.",
"incorrect_explanations": {
"B": "RMSE evaluates numeric predictions, not language translation quality.",
"C": "ROUGE focuses on recall overlap for summarization, not translation fidelity.",
"D": "F1 applies to classification tasks; translation needs sequence-level metrics like BLEU."
}
},
{
"id": "aif-c01-ai_services-080",
"certification_id": "AIF-C01",
"domain": "AI_SERVICES",
"difficulty": "medium",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "A large retailer receives thousands of product support queries daily and wants to use Agents for Amazon Bedrock. What key benefit helps here?",
"option_a": "Automatically generate custom FMs to predict customer needs",
"option_b": "Automate repetitive tasks and orchestrate complex workflows",
"option_c": "Automatically call multiple FMs and consolidate outputs",
"option_d": "Select the best FM by predefined metrics",
"correct_answers": ["B"],
"explanation_detailed": "Agents for Amazon Bedrock break tasks into steps, call tools/APIs, maintain memory, and orchestrate multi-step workflows—ideal for high-volume support use cases. An agent can authenticate users, look up order status via APIs, generate summaries, and escalate when needed, reducing handle time. They don’t create custom FMs or select FMs automatically, though they can route requests per your logic. For moderation and safety, pair agents with Guardrails; log traces to CloudWatch and S3 for observability.",
"incorrect_explanations": {
"A": "Agents orchestrate workflows; they do not train or auto-generate custom foundation models.",
"C": "Agents can call tools/models, but the core value is workflow orchestration, not multi-FM consolidation alone.",
"D": "Model selection is up to your routing logic; agents don’t autonomously choose the best FM by metrics."
}
},
{
"id": "aif-c01-ml_development-081",
"certification_id": "AIF-C01",
"domain": "ML_DEVELOPMENT",
"difficulty": "medium",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "What is a benefit of continual pre-training when adapting a foundation model?",
"option_a": "Decreases model architectural complexity",
"option_b": "Improves model performance over time",
"option_c": "Reduces training time requirement",
"option_d": "Optimizes model inference latency",
"correct_answers": ["B"],
"explanation_detailed": "Continual pre-training exposes the model to new corpora so it internalizes fresh terminology, styles, and facts, reducing distribution shift over time. This can improve downstream task performance, especially when paired with periodic evaluation and safety audits. It does not inherently simplify the architecture, shorten training time, or speed up inference. On AWS, you can run scheduled training on Trn or P-series instances, store datasets in S3 with versioning, track experiments in SageMaker, and validate safety with Clarify and Guardrails before promotion.",
"incorrect_explanations": {
"A": "Continual pre-training changes parameters, not architecture complexity.",
"C": "It usually adds training time since you train on more data over time.",
"D": "Inference latency depends on deployment hardware/architecture, not on continual pre-training."
}
},
{
"id": "aif-c01-ai_fundamentals-082",
"certification_id": "AIF-C01",
"domain": "AI_FUNDAMENTALS",
"difficulty": "easy",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "In generative AI, what are tokens?",
"option_a": "Basic input/output units the model operates on—words, subwords, or other linguistic units",
"option_b": "Mathematical representations of words used by models",
"option_c": "Pretrained weights adjusted for specific tasks",
"option_d": "Prompts or instructions provided to the model",
"correct_answers": ["A"],
"explanation_detailed": "Tokens are the atomic units processed by language models. Tokenization splits text into words, subwords, or characters, allowing models to handle diverse vocabularies efficiently. Costs, context windows, and rate limits are measured in tokens. Embeddings (vectors) can represent tokens semantically, but tokens themselves are the discrete symbols. Weights are model parameters. Prompts are token sequences provided as input. On AWS with Amazon Bedrock, pricing and limits are token-based; understanding tokenization helps control cost and ensure prompts fit within context windows.",
"incorrect_explanations": {
"B": "That describes embeddings; tokens are the discrete units before embedding.",
"C": "Weights are parameters, not the input/output units of text processing.",
"D": "Prompts are composed of tokens but are not tokens themselves."
}
},
{
"id": "aif-c01-ai_services-083",
"certification_id": "AIF-C01",
"domain": "AI_SERVICES",
"difficulty": "easy",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "A company wants to estimate inference costs when using Amazon Bedrock to build generative applications. Which factor most influences the cost?",
"option_a": "Number of tokens consumed",
"option_b": "Temperature value",
"option_c": "Amount of data used to train the LLM",
"option_d": "Total training time",
"correct_answers": ["A"],
"explanation_detailed": "Bedrock inference pricing is primarily based on tokens processed—both prompt and completion. Reducing prompt verbosity, pruning few-shot examples, and capping max tokens can substantially cut cost. Temperature affects randomness, not pricing. Training data size and time apply to model training, not pay-as-you-go inference with managed FMs. Monitor token usage with Bedrock logging and CloudWatch metrics, and batch requests or cache results when possible to optimize spend.",
"incorrect_explanations": {
"B": "Temperature shapes output diversity, not billing.",
"C": "Training data volume is irrelevant to on-demand inference pricing.",
"D": "Training time affects custom training cost, not Bedrock model usage fees."
}
},
{
"id": "aif-c01-ai_services-084",
"certification_id": "AIF-C01",
"domain": "AI_SERVICES",
"difficulty": "medium",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "A team uses Amazon SageMaker Studio notebooks with data in S3 and must manage data flow securely from S3 to Studio. What should they configure?",
"option_a": "Amazon Inspector to monitor SageMaker Studio",
"option_b": "Amazon Macie to monitor SageMaker Studio",
"option_c": "SageMaker in a VPC with an S3 VPC endpoint",
"option_d": "SageMaker with S3 Glacier Deep Archive",
"correct_answers": ["C"],
"explanation_detailed": "Place SageMaker Studio in a VPC and configure an S3 Gateway (or Interface) VPC Endpoint to route traffic privately to S3 without traversing the public internet. Add security groups, IAM roles with least privilege, and KMS encryption for data at rest. Inspector and Macie address vulnerability and sensitive data discovery respectively, not network path control. Glacier Deep Archive is cold storage and unrelated to Studio data access. VPC endpoints enforce secure, controlled, and auditable data movement for notebooks.",
"incorrect_explanations": {
"A": "Inspector scans for vulnerabilities; it does not control Studio↔S3 network paths.",
"B": "Macie helps discover sensitive data; it doesn’t provide private connectivity.",
"D": "Glacier Deep Archive is archival storage, not a connectivity/security configuration."
}
},
{
"id": "aif-c01-ai_fundamentals-086",
"certification_id": "AIF-C01",
"domain": "AI_FUNDAMENTALS",
"difficulty": "easy",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "What is the primary difference between AI and ML?",
"option_a": "AI is a subset of ML",
"option_b": "ML is a subset of AI",
"option_c": "They are completely unrelated fields",
"option_d": "AI and ML are the same thing",
"correct_answers": ["B"],
"explanation_detailed": "Artificial Intelligence is the broad field of building systems that perform tasks requiring human-like intelligence. Machine Learning is a subfield of AI that learns patterns from data to improve task performance without explicit rule programming. DL (deep learning) is a further subset using neural networks with many layers. On AWS, Amazon SageMaker provides tools to develop, train, and deploy ML; Bedrock exposes foundation models (an ML application) for generative AI; higher-level services like Comprehend, Rekognition, and Transcribe are AI services powered by ML under the hood.",
"incorrect_explanations": {
"A": "It is the inverse: ML is a subset of AI, not AI a subset of ML.",
"C": "They are tightly related; ML is a core approach within AI.",
"D": "AI is broader; ML is one approach, so they are not identical."
}
},
{
"id": "aif-c01-ai_fundamentals-087",
"certification_id": "AIF-C01",
"domain": "AI_FUNDAMENTALS",
"difficulty": "easy",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "Which is NOT a standard type of machine learning?",
"option_a": "Supervised learning",
"option_b": "Unsupervised learning",
"option_c": "Reinforcement learning",
"option_d": "Diagnostic learning",
"correct_answers": ["D"],
"explanation_detailed": "The canonical categories are supervised, unsupervised, and reinforcement learning. Supervised uses labeled examples; unsupervised finds structure in unlabeled data; reinforcement learns via rewards in an environment. “Diagnostic learning” is not a standard ML category, though models can assist diagnosis. On AWS, SageMaker supports all three through built-ins, frameworks, and RL toolkits.",
"incorrect_explanations": {
"A": "Supervised learning is a core ML paradigm using labeled data.",
"B": "Unsupervised learning is standard for clustering, dimensionality reduction, and discovery.",
"C": "Reinforcement learning is a major paradigm for sequential decision-making."
}
},
{
"id": "aif-c01-ai_fundamentals-088",
"certification_id": "AIF-C01",
"domain": "AI_FUNDAMENTALS",
"difficulty": "easy",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "Which data type best suits training a computer vision model?",
"option_a": "Tabular data",
"option_b": "Time-series data",
"option_c": "Image data",
"option_d": "Text data",
"correct_answers": ["C"],
"explanation_detailed": "Computer vision models learn from images or video frames. They require pixel arrays with labels for tasks like classification, detection, or segmentation. Time-series and tabular data power forecasting or structured ML tasks, and text suits NLP. On AWS, use SageMaker built-ins (e.g., image classification) or frameworks (PyTorch, TensorFlow), store images in S3, and optimize pipelines with SageMaker Processing and Training jobs.",
"incorrect_explanations": {
"A": "Tabular data is for structured ML, not visual perception tasks.",
"B": "Time-series is suited to forecasting and signal analysis, not image vision directly.",
"D": "Text is for NLP tasks; computer vision consumes images/videos."
}
},
{
"id": "aif-c01-ai_services-089",
"certification_id": "AIF-C01",
"domain": "AI_SERVICES",
"difficulty": "easy",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "Which AWS service is best for NLP tasks like sentiment, entity extraction, and key phrases?",
"option_a": "Amazon SageMaker",
"option_b": "Amazon Comprehend",
"option_c": "Amazon Polly",
"option_d": "Amazon Transcribe",
"correct_answers": ["B"],
"explanation_detailed": "Amazon Comprehend is a managed NLP service for sentiment analysis, entity recognition, key phrases, and topic modeling out of the box. It supports custom classification and entity models. SageMaker is a general ML platform; Polly converts text to speech; Transcribe converts speech to text. Comprehend speeds time to value for common NLP tasks without building custom models.",
"incorrect_explanations": {
"A": "SageMaker can build NLP models but requires ML engineering; Comprehend is turnkey.",
"C": "Polly is TTS, not text understanding.",
"D": "Transcribe is speech-to-text, not NLP analysis."
}
},
{
"id": "aif-c01-ml_development-090",
"certification_id": "AIF-C01",
"domain": "ML_DEVELOPMENT",
"difficulty": "easy",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "What is the primary goal of Exploratory Data Analysis (EDA) in ML development?",
"option_a": "Train the model",
"option_b": "Deploy the model",
"option_c": "Understand the data’s characteristics",
"option_d": "Monitor the model in production",
"correct_answers": ["C"],
"explanation_detailed": "EDA helps you understand distributions, correlations, missingness, outliers, and potential data leakage before modeling. Insights from EDA inform preprocessing, feature engineering, and model choice. On AWS, run EDA in SageMaker Studio with Pandas/Matplotlib, storing datasets in S3, and communicate findings via QuickSight dashboards.",
"incorrect_explanations": {
"A": "Model training happens after you understand and prepare data via EDA.",
"B": "Deployment occurs after training and evaluation, not during EDA.",
"D": "Monitoring is a production concern, not an analysis-stage task."
}
},
{
"id": "aif-c01-ml_development-091",
"certification_id": "AIF-C01",
"domain": "ML_DEVELOPMENT",
"difficulty": "easy",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "Which is NOT a typical step in an ML pipeline?",
"option_a": "Data collection",
"option_b": "Feature engineering",
"option_c": "Model training",
"option_d": "Customer acquisition",
"correct_answers": ["D"],
"explanation_detailed": "ML pipelines focus on data operations, modeling, evaluation, and deployment. Customer acquisition is a business process, not a technical pipeline step. On AWS, pipelines may use S3 for data, SageMaker Processing/Training, Model Registry for versions, and CI/CD via SageMaker Pipelines or CodePipeline.",
"incorrect_explanations": {
"A": "Collecting data is fundamental to ML pipelines.",
"B": "Feature engineering shapes inputs and is a core step.",
"C": "Training is the central modeling stage."
}
},
{
"id": "aif-c01-ai_fundamentals-092",
"certification_id": "AIF-C01",
"domain": "AI_FUNDAMENTALS",
"difficulty": "easy",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "In model performance metrics, what does AUC stand for?",
"option_a": "Average User Cost",
"option_b": "Area Under the Curve",
"option_c": "Automated Universal Calculation",
"option_d": "Augmented Use Case",
"correct_answers": ["B"],
"explanation_detailed": "AUC—Area Under the ROC Curve—measures a classifier’s ability to rank positives above negatives across thresholds. It summarizes trade-offs between true-positive and false-positive rates. AUC is threshold-independent and useful for imbalanced datasets. On AWS, compute AUC in SageMaker notebooks (scikit-learn) and track it in Model Registry alongside precision/recall and calibration plots.",
"incorrect_explanations": {
"A": "Not a standard ML metric acronym.",
"C": "Not an ML performance term.",
"D": "Not related to classification performance."
}
},
{
"id": "aif-c01-ai_fundamentals-093",
"certification_id": "AIF-C01",
"domain": "AI_FUNDAMENTALS",
"difficulty": "easy",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "Which learning type is most appropriate when you have a large labeled dataset?",
"option_a": "Unsupervised learning",
"option_b": "Reinforcement learning",
"option_c": "Supervised learning",
"option_d": "Semi-supervised learning",
"correct_answers": ["C"],
"explanation_detailed": "Supervised learning maps inputs to outputs using labeled examples and performs best when abundant labeled data exists. It underpins classification and regression tasks. Unsupervised learning discovers structure without labels; reinforcement learning optimizes actions via rewards; semi-supervised uses limited labels plus unlabeled data. On AWS, train supervised models with SageMaker built-ins (XGBoost) or frameworks.",
"incorrect_explanations": {
"A": "Unsupervised lacks labels and won’t exploit your labeled dataset.",
"B": "RL is for sequential decisions and rewards, not fixed input-label pairs.",
"D": "Semi-supervised is for scarce labels; you already have many."
}
},
{
"id": "aif-c01-ai_fundamentals-094",
"certification_id": "AIF-C01",
"domain": "AI_FUNDAMENTALS",
"difficulty": "easy",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "What is the main advantage of using pre-trained models?",
"option_a": "They always outperform custom models",
"option_b": "They require fewer compute resources to train",
"option_c": "They are always more accurate",
"option_d": "They can be used immediately without additional training",
"correct_answers": ["D"],
"explanation_detailed": "Pre-trained models already encode linguistic or visual knowledge learned from large corpora, enabling zero-shot and few-shot use and rapid prototyping. You can often deploy them directly or lightly adapt them via prompt engineering or fine-tuning. They don’t guarantee higher accuracy than tailored models, but they reduce time-to-value. On AWS, use Amazon Bedrock for managed access to pre-trained FMs and SageMaker JumpStart for pre-built models and notebooks.",
"incorrect_explanations": {
"A": "They don’t always outperform domain-specific custom models.",
"B": "They still require compute if you fine-tune; the advantage is starting from a trained baseline.",
"C": "Accuracy depends on domain and data; not guaranteed."
}
},
{
"id": "aif-c01-ai_services-095",
"certification_id": "AIF-C01",
"domain": "AI_SERVICES",
"difficulty": "easy",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "Which AWS service best automates finding strong hyperparameters for a model?",
"option_a": "Amazon SageMaker Autopilot",
"option_b": "Amazon Comprehend",
"option_c": "Amazon Polly",
"option_d": "Amazon Transcribe",
"correct_answers": ["A"],
"explanation_detailed": "SageMaker Autopilot automates feature preprocessing, algorithm selection, and hyperparameter tuning to produce strong baselines with minimal code. It leverages Bayesian or grid/random search under the hood (and you can also use SageMaker Hyperparameter Tuning Jobs directly). Comprehend is NLP, Polly is TTS, Transcribe is STT. Autopilot reduces manual iteration and logs candidates for inspection and deployment via Model Registry.",
"incorrect_explanations": {
"B": "Comprehend performs NLP analysis; it doesn’t tune arbitrary models.",
"C": "Polly synthesizes speech, unrelated to tuning ML models.",
"D": "Transcribe converts audio to text, not model optimization."
}
},
{
"id": "aif-c01-ml_development-096",
"certification_id": "AIF-C01",
"domain": "ML_DEVELOPMENT",
"difficulty": "easy",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "What does MLOps stand for?",
"option_a": "Machine Learning Operations",
"option_b": "Multiple Learning Optimizations",
"option_c": "Model Learning Objectives",
"option_d": "Managed Learning Outputs",
"correct_answers": ["A"],
"explanation_detailed": "MLOps is the practice of reliably building, deploying, monitoring, and governing ML systems in production. It covers CI/CD for models, data/version management, automated testing, bias/safety checks, and observability. On AWS, use SageMaker Pipelines, Model Registry, Clarify, Model Monitor, and CI/CD via CodePipeline/CodeBuild to implement MLOps at scale.",
"incorrect_explanations": {
"B": "Not a standard expansion; MLOps refers to operational practices, not optimizations only.",
"C": "Objectives are part of ML design, not the definition of MLOps.",
"D": "Outputs management is a subset; MLOps is broader lifecycle operations."
}
},
{
"id": "aif-c01-ml_development-097",
"certification_id": "AIF-C01",
"domain": "ML_DEVELOPMENT",
"difficulty": "easy",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "Which is NOT a typical business metric to evaluate ML systems?",
"option_a": "Cost per user",
"option_b": "Development costs",
"option_c": "Customer feedback",
"option_d": "F1 score",
"correct_answers": ["D"],
"explanation_detailed": "F1 is a technical metric reflecting precision-recall balance. Business metrics gauge commercial impact, costs, and user sentiment. Aligning technical metrics with business KPIs is crucial for value delivery. On AWS, tie CloudWatch model metrics to QuickSight dashboards that include cost and customer outcomes.",
"incorrect_explanations": {
"A": "Unit economics like cost per user are core business KPIs.",
"B": "Development costs affect ROI and are tracked by product teams.",
"C": "User feedback measures satisfaction and value, key to success."
}
},
{
"id": "aif-c01-ai_fundamentals-098",
"certification_id": "AIF-C01",
"domain": "AI_FUNDAMENTALS",
"difficulty": "easy",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "Which learning paradigm is best when an agent must learn from interactions with an environment?",
"option_a": "Supervised learning",
"option_b": "Unsupervised learning",
"option_c": "Reinforcement learning",
"option_d": "Transfer learning",
"correct_answers": ["C"],
"explanation_detailed": "Reinforcement learning learns policies that maximize cumulative reward via trial and error. It suits control, recommendation with long-term value, and dialogue management. Supervised learning requires labeled input/output pairs; unsupervised finds structure; transfer adapts knowledge across tasks. On AWS, you can implement RL in SageMaker or leverage simulation on AWS RoboMaker for robotics scenarios.",
"incorrect_explanations": {
"A": "Supervised learning uses static labels, not interactive feedback loops.",
"B": "Unsupervised learning discovers patterns without rewards or actions.",
"D": "Transfer learning reuses knowledge but is orthogonal to interaction-driven learning."
}
},
{
"id": "aif-c01-ai_services-099",
"certification_id": "AIF-C01",
"domain": "AI_SERVICES",
"difficulty": "easy",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "Which AWS service converts text to lifelike speech?",
"option_a": "Amazon Comprehend",
"option_b": "Amazon Translate",
"option_c": "Amazon Transcribe",
"option_d": "Amazon Polly",
"correct_answers": ["D"],
"explanation_detailed": "Amazon Polly is a neural text-to-speech service providing natural voices and SSML control over prosody, pronunciation, and emphasis. It supports multiple languages and can stream audio for low-latency applications. Comprehend performs NLP analysis, Translate handles language translation, and Transcribe converts speech to text. Polly is the correct TTS solution on AWS.",
"incorrect_explanations": {
"A": "Comprehend analyzes text but does not synthesize speech.",
"B": "Translate converts text between languages, not to audio.",
"C": "Transcribe is speech-to-text, the inverse of TTS."
}
},
{
"id": "aif-c01-ml_development-100",
"certification_id": "AIF-C01",
"domain": "ML_DEVELOPMENT",
"difficulty": "easy",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "What is the primary objective of feature engineering in ML development?",
"option_a": "Collect more data",
"option_b": "Create or transform features to improve model performance",
"option_c": "Evaluate model performance",
"option_d": "Deploy the model to production",
"correct_answers": ["B"],
"explanation_detailed": "Feature engineering extracts domain signal from raw data—creating, transforming, and selecting variables that improve model capacity to generalize. Examples include scaling, encoding categoricals, aggregations, domain-specific ratios, and time-window features. On AWS, use SageMaker Processing or Feature Store to build, document, and share features consistently across training and inference. Good features reduce complexity requirements and model brittleness.",
"incorrect_explanations": {
"A": "More data can help but doesn’t replace engineering informative features.",
"C": "Evaluation measures performance; feature engineering changes inputs to affect it.",
"D": "Deployment is an operational step, not feature creation."
}
},




{
"id": "aif-c01-ai_fundamentals-101",
"certification_id": "AIF-C01",
"domain": "AI_FUNDAMENTALS",
"difficulty": "easy",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "Qual das seguintes opções é um exemplo de aprendizagem não supervisionada?",
"option_a": "Detecção de spam",
"option_b": "Classificação de imagens",
"option_c": "Agrupamento de segmentos de clientes",
"option_d": "Previsão de preços de casas",
"correct_answers": ["C"],
"explanation_detailed": "Aprendizagem não supervisionada busca padrões sem rótulos humanos. O exemplo clássico é o agrupamento (clustering), que organiza amostras em grupos (clusters) com alta similaridade interna e baixa similaridade externa. Isso é útil para segmentação de clientes quando você não tem classes pré-definidas, descoberta de temas e detecção de estruturas latentes. Em um contexto AWS, você pode experimentar clustering com algoritmos disponíveis no Amazon SageMaker (por exemplo, K-Means), executando treinamentos gerenciados, tuning automático e inferência em lote. Essa abordagem difere da aprendizagem supervisionada, que usa dados rotulados para treinar um classificador ou regressor. O clustering ajuda a revelar segmentos ocultos para campanhas, preços e personalização.",
"incorrect_explanations": {
"A": "Detecção de spam é tipicamente um problema supervisionado: você treina um classificador com exemplos rotulados de mensagens ‘spam’ e ‘não spam’. O modelo aprende fronteiras a partir desses rótulos.",
"B": "Classificação de imagens usa rótulos das classes (por exemplo, ‘gato’, ‘cão’). Sem rótulos, o modelo não consegue aprender limites discriminativos típicos da classificação supervisionada.",
"D": "Previsão de preços de casas é regressão supervisionada: o modelo aprende a partir de pares recurso-alvo rotulados (características da casa → preço) para estimar valores contínuos."
}
},
{
"id": "aif-c01-ml_development-102",
"certification_id": "AIF-C01",
"domain": "ML_DEVELOPMENT",
"difficulty": "medium",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "Qual é a principal diferença entre inferência em lote e inferência em tempo real?",
"option_a": "A inferência em lote é sempre mais precisa",
"option_b": "A inferência em tempo real só pode ser feita em pequenos conjuntos de dados",
"option_c": "A inferência em lote processa várias entradas de uma vez, enquanto a inferência em tempo real processa entradas individuais à medida que chegam",
"option_d": "A inferência em tempo real é sempre mais rápida do que a inferência em lote",
"correct_answers": ["C"],
"explanation_detailed": "Inferência em lote agrega muitas entradas e as processa periodicamente, otimizando custo e throughput quando a latência não é crítica (ex.: scoring noturno de milhões de registros). Em tempo real, cada requisição é processada sob baixa latência para alimentar experiências online, como recomendações ou chatbots. Na AWS, o Amazon SageMaker oferece Batch Transform para lotes e Endpoints em tempo real (incluindo Serverless Inference) para baixa latência. Serviços de IA gerenciados, como Amazon Comprehend ou Rekognition, também expõem APIs síncronas (tempo real) e, em alguns cenários, mecanismos assíncronos para processar grandes coleções. A escolha depende de SLA de latência, volume, custo e necessidade operacional.",
"incorrect_explanations": {
"A": "Precisão não é função direta do modo de inferência. O mesmo modelo pode ser aplicado em lote ou tempo real; a diferença central está em latência e processamento agregado.",
"B": "Tempo real não é limitado a ‘pequenos’ dados; ele processa requisições unitárias sob baixa latência. O limite prático vem de SLA, escalabilidade do endpoint e custo.",
"D": "Tempo real visa baixa latência por requisição, mas ‘sempre mais rápido’ é falso. Em cenários massivos, lotes podem concluir o conjunto total mais rapidamente e com menor custo."
}
},
{
"id": "aif-c01-ai_services-103",
"certification_id": "AIF-C01",
"domain": "AI_SERVICES",
"difficulty": "easy",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "Qual serviço da AWS é mais adequado para gerenciar todo o ciclo de vida de aprendizado de máquina?",
"option_a": "Amazon Comprehend",
"option_b": "Amazon SageMaker",
"option_c": "Amazon Polly",
"option_d": "Amazon Translate",
"correct_answers": ["B"],
"explanation_detailed": "O Amazon SageMaker é a plataforma completa de ML da AWS para preparar dados, treinar, ajustar, implantar e monitorar modelos em escala. Ele provê notebooks gerenciados, pipelines de MLOps (SageMaker Pipelines), experimentos, tuning automático (HPO), além de endpoints para inferência em tempo real, assíncrona e em lote. Recursos como SageMaker Model Monitor detectam desvio de dados/modelo, e SageMaker JumpStart fornece modelos pré-treinados e soluções. Comparativamente, Amazon Comprehend, Polly e Translate são serviços de alto nível para NLP, síntese de fala e tradução, respectivamente, consumidos via API, sem gerenciar todo o ciclo de vida. SageMaker centraliza o ML de ponta a ponta, promovendo governança e repetibilidade.",
"incorrect_explanations": {
"A": "Amazon Comprehend resolve tarefas de NLP (tópicos, entidades, sentimento) via API. Não é uma plataforma de ciclo de vida completo para criar e operar modelos arbitrários.",
"C": "Amazon Polly realiza Text-to-Speech. Ele não cobre preparação de dados, experimentos, treinamento personalizado, deployment e monitoramento de modelos gerais.",
"D": "Amazon Translate foca tradução automática. Apesar de poderoso, não é uma solução de gestão de ML abrangente para qualquer domínio e pipeline de MLOps."
}
},
{
"id": "aif-c01-ml_development-104",
"certification_id": "AIF-C01",
"domain": "ML_DEVELOPMENT",
"difficulty": "medium",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "Qual é o objetivo principal do monitoramento de modelo em produção?",
"option_a": "Treinar novos modelos",
"option_b": "Coletar mais dados",
"option_c": "Detectar problemas como desvio de modelo ou desvio de dados",
"option_d": "Realizar engenharia de atributos",
"correct_answers": ["C"],
"explanation_detailed": "Após o deploy, o ambiente de produção muda: a distribuição dos dados de entrada e a relação entre recursos e rótulos podem evoluir. O monitoramento detecta desvio de dados (data drift), desvio de conceito/modelo (concept drift), degradação de métricas e problemas operacionais. Na AWS, o SageMaker Model Monitor coleta amostras do tráfego, compara estatísticas com baselines e aciona alertas (por exemplo, via CloudWatch) quando há violações. Essa telemetria orienta ações como retreinamento programado, reengenharia de features e revisão de prompts para LLMs. Sem monitoramento, erros silenciosos crescem, corroendo KPIs de negócio e confiança do usuário, especialmente em sistemas de recomendação e decisão.",
"incorrect_explanations": {
"A": "Treinar novos modelos pode ser uma consequência, não o objetivo primário. O alvo é detectar anomalias e degradação para decidir se treinar novamente faz sentido.",
"B": "Coletar dados auxilia, porém o foco é analisar qualidade/distribuição e métricas em produção. Coleta sem análise não garante detecção de desvio ou regressões.",
"D": "Engenharia de atributos melhora representações, mas ocorre antes ou durante retrain. O monitoramento avalia comportamento pós-deploy e aciona correções quando necessário."
}
},
{
"id": "aif-c01-ai_fundamentals-105",
"certification_id": "AIF-C01",
"domain": "AI_FUNDAMENTALS",
"difficulty": "easy",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "Qual das seguintes opções NÃO é um caso de uso típico para IA/AM?",
"option_a": "Detecção de fraudes",
"option_b": "Sistemas de recomendação",
"option_c": "Entrada manual de dados",
"option_d": "Reconhecimento de fala",
"correct_answers": ["C"],
"explanation_detailed": "IA/ML automatizam tarefas repetitivas, tomam decisões probabilísticas e extraem padrões de dados em escala. Detecção de fraude usa modelos supervisionados e detecção de anomalias; recomendações empregam filtragem colaborativa e modelos como Amazon Personalize; reconhecimento de fala utiliza redes acústicas, disponível via Amazon Transcribe. ‘Entrada manual de dados’ é justamente o oposto: processos manuais suscetíveis a erros e baixa escala que tendem a ser reduzidos pela automação. Na AWS, serviços gerenciados aceleram a adoção sem exigir equipes de pesquisa, e quando necessário, o SageMaker permite criar modelos personalizados e pipelines para incorporar MLOps e governança.",
"incorrect_explanations": {
"A": "Detecção de fraude é aplicação consolidada de IA/ML com classificação e anomalias. Bancos e e-commerce usam há anos para reduzir perdas e revisar alertas.",
"B": "Sistemas de recomendação estão no núcleo de muitos produtos digitais. A AWS oferece Amazon Personalize para acelerar essa capacidade sem infraestrutura pesada.",
"D": "Reconhecimento de fala é amplamente atendido pelo Amazon Transcribe, permitindo transcrever áudio/vídeo e alimentar análise de sentimento, buscas e relatórios."
}
},
{
"id": "aif-c01-ai_fundamentals-106",
"certification_id": "AIF-C01",
"domain": "AI_FUNDAMENTALS",
"difficulty": "easy",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "O que é um token no contexto da IA generativa?",
"option_a": "Uma funcionalidade de segurança",
"option_b": "Uma unidade de texto processada pelo modelo",
"option_c": "Um tipo de rede neural",
"option_d": "Uma métrica de avaliação de modelo",
"correct_answers": ["B"],
"explanation_detailed": "Tokens são unidades atômicas que modelos de linguagem consomem e produzem. Dependendo da tokenização, um token pode ser uma palavra, subpalavra ou caractere. O custo e a latência em serviços de LLM costumam ser proporcionais ao número de tokens de entrada e saída. Em Amazon Bedrock, a maioria dos provedores de modelos precifica por 1K tokens processados, impactando diretamente a conta. Compreender tokens ajuda a projetar prompts concisos, aplicar chunking para longos documentos e prever limites de contexto. Ferramentas como contadores de tokens e embeddings orientam sizing de janelas de contexto e custos de inferência em aplicações de geração e RAG.",
"incorrect_explanations": {
"A": "‘Token’ aqui não se refere a credenciais de segurança ou JWT. É uma unidade textual usada internamente pelo modelo para representar entradas/saídas.",
"C": "Token não é uma arquitetura. Redes como Transformers processam sequências de tokens, mas token é o insumo, não o tipo de rede em si.",
"D": "Não é métrica de avaliação. Métricas como BLEU, ROUGE ou BERTScore avaliam qualidade; tokens medem quantidade de texto e custos."
}
},
{
"id": "aif-c01-ai_fundamentals-107",
"certification_id": "AIF-C01",
"domain": "AI_FUNDAMENTALS",
"difficulty": "easy",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "Qual das seguintes opções NÃO é um caso de uso típico para modelos de IA generativa?",
"option_a": "Geração de imagem",
"option_b": "Sumarização",
"option_c": "Criptografia de dados",
"option_d": "Geração de código",
"correct_answers": ["C"],
"explanation_detailed": "Modelos generativos criam novos conteúdos a partir de padrões aprendidos: texto, imagens, áudio e código. Sumarização condensa textos mantendo significado; geração de imagem produz visuais originais; geração de código auxilia desenvolvimento. Criptografia, por sua vez, é domínio de segurança com algoritmos matemáticos (AES, RSA) e protocolos. Embora IA possa auxiliar na detecção de vulnerabilidades ou geração de exemplos, ‘criptografar’ dados não é função típica de um modelo generativo. Na AWS, capacidades generativas aparecem em Amazon Bedrock (LLMs e difusão), enquanto controles de segurança e criptografia são providos por serviços como KMS, não por LLMs.",
"incorrect_explanations": {
"A": "Geração de imagem é um uso central de modelos de difusão, disponível em provedores do Amazon Bedrock e frameworks de visão.",
"B": "Sumarização é caso clássico de LLMs, inclusive via APIs gerenciadas e exemplos em Bedrock ou SageMaker JumpStart.",
"D": "Geração de código é amplamente suportada por LLMs code-centric. A saída não substitui revisão humana, mas é um caso de uso genuíno."
}
},
{
"id": "aif-c01-ai_fundamentals-108",
"certification_id": "AIF-C01",
"domain": "AI_FUNDAMENTALS",
"difficulty": "easy",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "Qual é a principal vantagem da adaptabilidade da IA generativa?",
"option_a": "Ela só pode trabalhar com dados estruturados",
"option_b": "Ela pode lidar com uma ampla gama de tarefas e domínios",
"option_c": "Ela sempre produz resultados perfeitos",
"option_d": "Ela elimina a necessidade de supervisão humana",
"correct_answers": ["B"],
"explanation_detailed": "LLMs e modelos multimodais são pré-treinados em dados diversos e aprendem representações reutilizáveis. Essa base permite adaptação por prompt engineering, few-shot e fine-tuning a múltiplas tarefas: sumarização, extração, geração de código, classificação, imagem e mais. Em Amazon Bedrock, você escolhe FMs adequados e pode realizar ajuste fino gerenciado ou grounding via RAG. Essa versatilidade reduz tempo para valor, sem reescrever pipelines do zero. Entretanto, adaptabilidade não implica perfeição nem substitui governança: supervisão humana, avaliações e monitoramento continuam essenciais para precisão, segurança e conformidade. A força está em generalizar e compor capacidades sob diferentes contextos de negócio.",
"incorrect_explanations": {
"A": "Modelos generativos operam bem com dados não estruturados (texto, imagem, áudio). Eles não se limitam a estruturas tabulares.",
"C": "Resultados ‘perfeitos’ não são garantidos. Há variabilidade estocástica, dependência do prompt e risco de alucinações.",
"D": "Supervisão humana continua relevante para validação, segurança e ética, inclusive revisão de saídas e decisões críticas."
}
},
{
"id": "aif-c01-responsible_ai-109",
"certification_id": "AIF-C01",
"domain": "RESPONSIBLE_AI",
"difficulty": "medium",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "O que é uma alucinação no contexto da IA generativa?",
"option_a": "Uma saída visual produzida pelo modelo",
"option_b": "Um tipo de arquitetura de modelo",
"option_c": "Uma saída incorreta ou fabricada apresentada como fato",
"option_d": "Um método de treinamento de modelo",
"correct_answers": ["C"],
"explanation_detailed": "Alucinação ocorre quando um modelo gera conteúdo plausível, porém falso, incompleto ou fora de fonte. Em contextos críticos, isso mina confiança e pode causar danos. Estratégias de mitigação incluem grounding com RAG (recuperando evidências de bases como Amazon OpenSearch Service, Amazon Kendra ou S3), controles de temperatura/top-p, validações de pós-processamento e avaliação humana. Em Amazon Bedrock, você pode combinar FMs com Guardrails para políticas de segurança e com Agentes para orquestrar chamadas a ferramentas e fontes. Monitorar precisão e rastreabilidade, registrar citações e limitar tarefas de ‘fato duro’ a fluxos com verificação reduzem o risco operacional de alucinações.",
"incorrect_explanations": {
"A": "‘Saída visual’ não caracteriza alucinação por si. Alucinação é sobre incorreção factual, independentemente de modalidade (texto, imagem).",
"B": "Não é arquitetura de modelo. Arquiteturas como Transformers ou difusão podem alucinar se usadas sem grounding/validação.",
"D": "Não é método de treinamento. Técnica como RLHF pode reduzir alucinações, mas a alucinação é um comportamento indesejado na inferência."
}
},
{
"id": "aif-c01-ai_services-110",
"certification_id": "AIF-C01",
"domain": "AI_SERVICES",
"difficulty": "easy",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "Qual serviço da AWS é projetado especificamente para desenvolver aplicações de IA generativa?",
"option_a": "Amazon EC2",
"option_b": "Amazon S3",
"option_c": "Amazon Bedrock",
"option_d": "Amazon RDS",
"correct_answers": ["C"],
"explanation_detailed": "Amazon Bedrock oferece acesso via API a diversos modelos de fundação (FMs) para texto, imagem e multimodal, além de capacidades como ajuste fino, guardrails e orquestração com Agentes. Ele abstrai a infraestrutura, simplificando a integração de LLMs em produtos. Você pode prototipar no console, usar SDKs e combinar com RAG (Kendra/OpenSearch) para respostas baseadas em dados proprietários. Enquanto EC2, S3 e RDS são serviços de computação, armazenamento e banco de dados respectivamente, Bedrock foca diretamente no ciclo de construção de apps generativos, com billing por token e ferramentas para segurança, auditoria e integração empresarial.",
"incorrect_explanations": {
"A": "Amazon EC2 provê instâncias de computação. Útil para hospedar modelos, mas não é o serviço gerenciado de FMs e orquestrações de IA generativa.",
"B": "Amazon S3 armazena objetos. Pode guardar dados de treino, prompts e resultados, porém não fornece modelos generativos via API.",
"D": "Amazon RDS gerencia bancos relacionais. Não entrega diretamente FMs nem recursos de geração/orquestração de conteúdo."
}
},
{
"id": "aif-c01-ai_fundamentals-111",
"certification_id": "AIF-C01",
"domain": "AI_FUNDAMENTALS",
"difficulty": "easy",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "O que é um modelo de fundação em IA generativa?",
"option_a": "Um modelo que só pode gerar texto",
"option_b": "Um modelo grande e pré-treinado que pode ser adaptado para várias tarefas",
"option_c": "Um modelo projetado especificamente para geração de imagens",
"option_d": "Um modelo que não requer dados de treinamento",
"correct_answers": ["B"],
"explanation_detailed": "Modelos de fundação (FMs) são redes de grande escala pré-treinadas em dados extensos e diversos para aprender representações gerais. Eles servem de base para muitas tarefas: QA, sumarização, extração, geração de imagem/código e mais. A adaptação ocorre por prompt engineering, few-shot, fine-tuning ou instrução. Em Amazon Bedrock, você escolhe FMs de diferentes provedores e ajusta ao seu caso, poupando tempo e custos de treinar do zero. Esses modelos não são limitados a texto ou imagem; o termo engloba variantes unimodais e multimodais. Embora extensíveis, ainda requerem dados de treinamento no pré-treino e curadoria ao personalizar.",
"incorrect_explanations": {
"A": "FMs não se limitam a texto; existem modelos para imagem, multimodalidade e código. A chave é o pré-treino amplo e reutilizável.",
"C": "Modelos de imagem (ex.: difusão) são subconjunto possível. ‘Fundação’ não implica obrigatoriamente foco em imagem.",
"D": "Todo modelo exige dados em algum estágio. O pré-treinamento massivo é central ao conceito de FM."
}
},
{
"id": "aif-c01-ai_fundamentals-112",
"certification_id": "AIF-C01",
"domain": "AI_FUNDAMENTALS",
"difficulty": "easy",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "Qual das seguintes opções NÃO é uma etapa no ciclo de vida do modelo de fundação?",
"option_a": "Seleção de dados",
"option_b": "Pré-treinamento",
"option_c": "Implantação",
"option_d": "Marketing",
"correct_answers": ["D"],
"explanation_detailed": "O ciclo de vida técnico de FMs envolve coletar/curar dados, pré-treinar modelos em larga escala, ajustar para tarefas específicas (instrução/fine-tuning), avaliar com métricas adequadas e implantar/monitorar em produção. Governança, segurança e custo entram como considerações transversais. Marketing é atividade de go-to-market e não integra o ciclo de vida técnico do modelo. Na AWS, SageMaker e Bedrock oferecem ferramentas para várias fases: JumpStart e datasets, pipelines de MLOps, endpoints gerenciados, guardrails e monitoramento. A disciplina assegura reprodutibilidade, qualidade e conformidade, reduzindo riscos de desvio e degradando menos em cenários dinâmicos.",
"incorrect_explanations": {
"A": "Seleção/curadoria de dados é essencial para qualidade e cobertura. Dados ruidosos degradam o pré-treino e as adaptações posteriores.",
"B": "Pré-treinamento aprende representações gerais em corpora massivos e é central ao conceito de FM.",
"C": "Implantação garante que o modelo sirva tráfego com SLA, segurança e observabilidade, parte crítica do ciclo operacional."
}
},
{
"id": "aif-c01-ai_services-113",
"certification_id": "AIF-C01",
"domain": "AI_SERVICES",
"difficulty": "easy",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "Qual é a principal vantagem de usar serviços de IA generativa da AWS para construir aplicações?",
"option_a": "Eles são sempre gratuitos",
"option_b": "Eles fornecem uma barreira de entrada mais baixa",
"option_c": "Eles garantem 100% de precisão",
"option_d": "Eles eliminam a necessidade de qualquer codificação",
"correct_answers": ["B"],
"explanation_detailed": "Serviços gerenciados como Amazon Bedrock, Amazon Q e integrações com Kendra/OpenSearch removem grande parte da complexidade de infraestrutura e orquestração de modelos. Você acessa FMs via API, consegue prototipar rapidamente, ajustar por instrução, aplicar guardrails e combinar com RAG. Essa abordagem reduz tempo para valor, esforço de MLOps e custos iniciais, permitindo foco no caso de negócio. Ainda assim, precisão não é garantida e codificação pode ser necessária para integrações, avaliação e monitoramento. O benefício chave é acelerar a entrega com menos carga operacional, mantendo boas práticas de segurança e governança apoiadas pelo ecossistema AWS.",
"incorrect_explanations": {
"A": "Não são ‘sempre gratuitos’. A maioria adota cobrança por tokens, requisições ou recursos computacionais usados.",
"C": "Nenhum serviço garante 100% de acerto. Avaliação, limites e validações continuam obrigatórios.",
"D": "Apesar de low-code/no-code existirem, integrações e lógica de negócio exigem algum nível de codificação."
}
},
{
"id": "aif-c01-ai_fundamentals-114",
"certification_id": "AIF-C01",
"domain": "AI_FUNDAMENTALS",
"difficulty": "medium",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "O que é engenharia de prompt no contexto da IA generativa?",
"option_a": "Um método de otimização de hardware",
"option_b": "Uma técnica para projetar a estrutura física de modelos de IA",
"option_c": "O processo de criar prompts de entrada eficazes para orientar as saídas do modelo",
"option_d": "Uma maneira de reduzir o consumo de energia em sistemas de IA",
"correct_answers": ["C"],
"explanation_detailed": "Engenharia de prompt consiste em estruturar instruções, contexto e exemplos para guiar LLMs a produzir respostas alinhadas à tarefa. Inclui técnicas como delimitação clara, papéis, restrições, few-shot, prompts negativos e validações automáticas. Em aplicações AWS, você pode iterar prompts em Amazon Bedrock, usar Agentes para decompor tarefas e combinar RAG (Kendra/OpenSearch) para grounding, reduzindo alucinações. A prática envolve medir qualidade, custo (tokens) e latência, além de versionar templates e utilizar parâmetros como temperatura e top-p. Bons prompts tornam a aplicação mais previsível e ajudam a cumprir requisitos de negócio e conformidade.",
"incorrect_explanations": {
"A": "Não otimiza hardware. Trata do design textual/estrutural da entrada para orientar o comportamento do modelo.",
"B": "Não projeta a ‘física’ do modelo. É uma camada de aplicação sobre modelos já treinados.",
"D": "Pode influenciar custo/latência indiretamente, mas não é técnica de eficiência energética."
}
},
{
"id": "aif-c01-responsible_ai-115",
"certification_id": "AIF-C01",
"domain": "RESPONSIBLE_AI",
"difficulty": "medium",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "Qual das seguintes opções é uma desvantagem potencial das soluções de IA generativa?",
"option_a": "Adaptabilidade",
"option_b": "Capacidade de resposta",
"option_c": "Imprecisão",
"option_d": "Simplicidade",
"correct_answers": ["C"],
"explanation_detailed": "IA generativa pode produzir saídas incorretas, enviesadas ou não fundamentadas, afetando segurança, reputação e KPIs. Mitigar exige políticas, avaliação humana, grounding (RAG), checagem factual, e guardrails. Na AWS, combinar Bedrock com Amazon Kendra/OpenSearch e regras de conteúdo, além de monitoramento contínuo (CloudWatch, logs, métricas) ajuda a reduzir riscos. Avaliar qualidade com métricas adequadas e A/B tests garante que mudanças em prompts/modelos não degradem resultados de negócio. Adaptabilidade e simplicidade são benefícios, mas devem vir com governança, controles de custo por token e estratégias de revisão humana para domínios críticos como saúde, finanças e jurídico.",
"incorrect_explanations": {
"A": "Adaptabilidade é vantagem: modelos se ajustam a múltiplos contextos via prompt, few-shot e tuning.",
"B": "Capacidade de resposta se refere à interação rápida e natural, geralmente benéfica em assistentes e chat.",
"D": "Simplicidade na integração reduz barreiras. O problema não é ser simples, mas gerir riscos de qualidade e segurança."
}
},
{
"id": "aif-c01-ai_fundamentals-116",
"certification_id": "AIF-C01",
"domain": "AI_FUNDAMENTALS",
"difficulty": "easy",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "O que é um modelo multimodal em IA generativa?",
"option_a": "Um modelo que só pode processar dados de texto",
"option_b": "Um modelo que pode trabalhar com vários tipos de dados (por exemplo, texto, imagens, áudio)",
"option_c": "Um modelo que requer várias GPUs para ser executado",
"option_d": "Um modelo que só pode gerar imagens",
"correct_answers": ["B"],
"explanation_detailed": "Modelos multimodais consomem e geram diferentes modalidades, como texto, imagem, áudio e vídeo. Eles criam representações compartilhadas entre modalidades, permitindo tarefas como descrever imagens, responder perguntas sobre figuras ou gerar ilustrações a partir de texto. Em Amazon Bedrock, há provedores com capacidades multimodais expostas via API, que você pode combinar com armazenamento no S3 e busca vetorial (OpenSearch) para grounding. Essa integração amplia o leque de aplicações, de suporte técnico com screenshots à análise de mídia. Embora flexíveis, precisam de governança igual aos LLMs de texto: avaliação, monitoramento e controle de custos por token e por conteúdo.",
"incorrect_explanations": {
"A": "Texto-apenas descreve modelos unimodais. Multimodalidade vai além de apenas uma forma de dado.",
"C": "Requisito de GPU depende de tamanho e SLA, não é definição de multimodalidade.",
"D": "Gerar apenas imagens é unimodal. Multimodal combina múltiplas modalidades de entrada/saída."
}
},
{
"id": "aif-c01-ai_services-117",
"certification_id": "AIF-C01",
"domain": "AI_SERVICES",
"difficulty": "easy",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "Qual serviço da AWS fornece um playground para experimentar modelos de IA generativa?",
"option_a": "Amazon SageMaker",
"option_b": "Amazon Comprehend",
"option_c": "PartyRock",
"option_d": "Amazon Polly",
"correct_answers": ["C"],
"explanation_detailed": "PartyRock é um playground baseado em Amazon Bedrock que permite montar apps generativos rapidamente, experimentando prompts, componentes de UI e fontes de dados sem gerenciar infraestrutura. Ele facilita aprendizado e prototipação para casos como chat, resumo, extração e geração de imagens. Em paralelo, o console do Bedrock e SDKs permitem testar FMs diretamente e integrar com serviços como Kendra para RAG. Diferentemente de Comprehend ou Polly, que resolvem tarefas específicas de NLP e TTS via API, o PartyRock acelera ideação e validação de casos, reduzindo a barreira inicial para equipes técnicas e não técnicas explorarem soluções generativas.",
"incorrect_explanations": {
"A": "SageMaker é a plataforma completa de ML. Útil para ciclos de vida e tuning, mas não é um ‘playground’ focado em apps generativos prontos.",
"B": "Comprehend fornece NLP pronto (entidades, sentimento). Não é um ambiente de prototipação de apps de IA generativa.",
"D": "Polly faz Text-to-Speech. Não oferece experimentação ampla com LLMs/fluxos generativos."
}
},
{
"id": "aif-c01-ml_development-118",
"certification_id": "AIF-C01",
"domain": "ML_DEVELOPMENT",
"difficulty": "medium",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "Qual é uma consideração chave ao selecionar um modelo de IA generativa apropriado para um problema de negócios?",
"option_a": "A popularidade do modelo nas redes sociais",
"option_b": "Os requisitos de desempenho do modelo",
"option_c": "A data de desenvolvimento do modelo",
"option_d": "O país de origem do modelo",
"correct_answers": ["B"],
"explanation_detailed": "Seleção de modelo deve alinhar requisitos de tarefa a métricas e restrições: qualidade esperada, latência, custo por token, contexto máximo, segurança e suporte a idiomas/modalidades. Em AWS, comparar provedores do Bedrock pelo tamanho de contexto, capacidades de instrução, ferramentas (agentes, function calling), opções de ajuste fino e guardrails é essencial. Para tarefas longas, janelas maiores e custo previsível pesam; para chat seguro, controles e grounding com Kendra/OpenSearch são decisivos. Popularidade não garante adequação. Uma avaliação sistemática com benchmarks e A/B tests reduz risco de escolher modelos inadequados ao SLA e ao ROI desejados.",
"incorrect_explanations": {
"A": "Popularidade não assegura qualidade/ajuste ao seu caso. Métricas e requisitos de negócio devem guiar a escolha.",
"C": "Data de lançamento não implica melhor desempenho para sua tarefa. Avaliação empírica importa mais.",
"D": "Origem não determina adequação técnica. Foco em métricas, políticas de uso e conformidade aplicável."
}
},
{
"id": "aif-c01-ml_development-119",
"certification_id": "AIF-C01",
"domain": "ML_DEVELOPMENT",
"difficulty": "medium",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "Qual das seguintes opções NÃO é uma métrica de negócios típica para avaliar aplicações de IA generativa?",
"option_a": "Taxa de conversão",
"option_b": "Receita média por usuário",
"option_c": "Valor do tempo de vida do cliente",
"option_d": "Contagem de parâmetros do modelo",
"correct_answers": ["D"],
"explanation_detailed": "Métricas de negócio medem impacto em resultados: conversão, retenção, ticket médio, LTV, NPS e tempo economizado. Em aplicações generativas, além de métricas técnicas (latência, custo por 1K tokens), é vital capturar KPIs que provem ROI. Contagem de parâmetros é característica técnica do modelo e não traduz valor para o negócio. Na AWS, combine telemetria de aplicação (CloudWatch, OpenSearch dashboards) e experimentação (A/B via CloudWatch Evidently ou frameworks) para rastrear metas. Mapear métricas a hipóteses e executar ciclos de melhoria contínua evita otimizações de ‘modelo pela modelo’ sem evidência de ganho real.",
"incorrect_explanations": {
"A": "Taxa de conversão reflete impacto direto em objetivos de aquisição/venda. É métrica central de negócio.",
"B": "Receita média por usuário indica monetização por base ativa, útil para avaliar crescimento e upsell.",
"C": "LTV mede valor total projetado do cliente, importante para priorizar investimentos e CAC."
}
},
{
"id": "aif-c01-responsible_ai-120",
"certification_id": "AIF-C01",
"domain": "RESPONSIBLE_AI",
"difficulty": "medium",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "Qual é um benefício chave da infraestrutura da AWS para aplicações de IA generativa?",
"option_a": "Elimina a necessidade de qualquer medida de segurança",
"option_b": "Fornece recursos computacionais gratuitos e ilimitados",
"option_c": "Garante a conformidade com os regulamentos relevantes",
"option_d": "Garante que os modelos de IA nunca cometerão erros",
"correct_answers": ["C"],
"explanation_detailed": "A AWS fornece controles de segurança e conformidade rigorosos (por exemplo, certificações e recursos de criptografia) que facilitam atender requisitos regulatórios setoriais ao hospedar dados e executar inferências. Para IA generativa, isso viabiliza integrar FMs com governança, auditoria e isolamento de dados (VPC endpoints, KMS). Serviços como Bedrock, SageMaker e Kendra operam dentro desse ecossistema, permitindo políticas de acesso, logging e monitoramento. Isso não implica infalibilidade dos modelos, mas cria base confiável para construir soluções em domínios regulados. Além disso, recursos elásticos reduzem esforço operacional sem prometer ‘ilimitado’ gratuito ou ausência de práticas de segurança.",
"incorrect_explanations": {
"A": "Medidas de segurança continuam necessárias: identidade, criptografia, revisão humana e controles de conteúdo.",
"B": "A nuvem é elástica, porém cobrada conforme uso. Não há recursos ‘gratuitos e ilimitados’.",
"D": "Modelos podem errar. Conformidade não elimina a necessidade de avaliação e monitoramento contínuos."
}
},




{
"id": "aif-c01-ml_development-121",
"certification_id": "AIF-C01",
"domain": "ML_DEVELOPMENT",
"difficulty": "medium",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "O que é 'chunking' no contexto da IA generativa?",
"option_a": "Um método de compressão de dados",
"option_b": "Uma técnica para dividir grandes entradas em pedaços menores e gerenciáveis",
"option_c": "Um tipo de arquitetura de modelo",
"option_d": "Uma maneira de aumentar a precisão do modelo",
"correct_answers": ["B"],
"explanation_detailed": "Chunking divide documentos longos em segmentos menores (chunks) para caber no limite de contexto de LLMs e melhorar a recuperação em RAG. Cada chunk recebe metadados (título, fonte, data) e, opcionalmente, sobreposição para manter coerência entre trechos. Na AWS, você pode criar chunks em pipelines no Amazon SageMaker, armazenar embeddings por chunk em Amazon OpenSearch Service ou consultar com Amazon Kendra. Durante a geração (Amazon Bedrock), o sistema busca os chunks mais relevantes (similaridade semântica) e os injeta no prompt. Isso reduz alucinações, melhora precisão factual e controla custo por token, já que apenas partes relevantes do conteúdo são enviadas ao modelo em vez do documento inteiro.",
"incorrect_explanations": {
"A": "Compressão reduz tamanho binário; chunking organiza o texto em blocos semânticos para contornar limites de contexto e habilitar recuperação eficiente.",
"C": "Arquitetura de modelo descreve estruturas como Transformers ou difusão. Chunking é uma estratégia de pré-processamento e recuperação, não uma arquitetura.",
"D": "Pode contribuir indiretamente para melhor precisão em RAG, mas o objetivo central é viabilizar contexto e recuperação, não ‘aumentar precisão’ por si."
}
},
{
"id": "aif-c01-ai_fundamentals-122",
"certification_id": "AIF-C01",
"domain": "AI_FUNDAMENTALS",
"difficulty": "easy",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "Qual das seguintes opções é uma vantagem chave da simplicidade da IA generativa?",
"option_a": "Ela sempre produz resultados perfeitos",
"option_b": "Ela não requer entrada humana",
"option_c": "Pode ser mais fácil de implementar e usar em comparação com métodos tradicionais",
"option_d": "Ela elimina a necessidade de pré-processamento de dados",
"correct_answers": ["C"],
"explanation_detailed": "A simplicidade vem do acesso a modelos de fundação pré-treinados e de interfaces de alto nível. Em vez de construir pipelines do zero, você chama APIs (por exemplo, Amazon Bedrock) e obtém capacidades como sumarização e extração. Isso reduz o tempo para valor e o esforço de MLOps. Serviços como Amazon Q, Amazon Kendra e integrações com Amazon SageMaker JumpStart aceleram a adoção sem exigir profundo conhecimento em treinamento. Apesar de simples de começar, boas práticas permanecem: engenharia de prompt, avaliação, monitoramento e controle de custos por token. A simplicidade reduz fricção inicial, mas não garante perfeição nem elimina a necessidade de curadoria e validação humana.",
"incorrect_explanations": {
"A": "Modelos generativos podem errar ou alucinar. Simplicidade de uso não implica perfeição de resultados.",
"B": "Interação humana segue essencial para definir requisitos, validar saídas e aplicar governança.",
"D": "Pré-processamento (limpeza, chunking, metadados) ainda é útil, especialmente em fluxos com RAG e avaliações automáticas."
}
},
{
"id": "aif-c01-ai_fundamentals-123",
"certification_id": "AIF-C01",
"domain": "AI_FUNDAMENTALS",
"difficulty": "easy",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "O que é um modelo de difusão em IA generativa?",
"option_a": "Um modelo que só funciona com dados textuais",
"option_b": "Um tipo de modelo generativo frequentemente usado para geração de imagens",
"option_c": "Um modelo que não requer dados de treinamento",
"option_d": "Um modelo projetado especificamente para processamento de linguagem natural",
"correct_answers": ["B"],
"explanation_detailed": "Modelos de difusão aprendem a reverter um processo de ruído progressivo até reconstruir uma imagem coerente. Durante o treino, o modelo observa dados com ruído crescente; na geração, remove ruído passo a passo a partir de ruído puro até criar a imagem. Essa abordagem produz resultados de alta qualidade e é base de muitos sistemas modernos de síntese visual. Em AWS, você pode consumir geradores de imagem via Amazon Bedrock (modelos de fundação de provedores parceiros) ou treinar/inferir customizados no Amazon SageMaker. Integrações com Amazon S3 para dados e monitoramento com CloudWatch completam um pipeline de produção.",
"incorrect_explanations": {
"A": "Difusão é majoritariamente aplicada a imagens, embora haja pesquisas multimodais. Não é “texto-apenas”.",
"C": "Todo modelo aprende a partir de dados. Difusão exige datasets de imagens para treinar o processo reverso.",
"D": "Embora exista difusão para áudio e vídeo, não é um modelo projetado ‘apenas’ para NLP."
}
},
{
"id": "aif-c01-ai_services-124",
"certification_id": "AIF-C01",
"domain": "AI_SERVICES",
"difficulty": "easy",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "Qual serviço da AWS é projetado para fornecer recursos de IA conversacional?",
"option_a": "Amazon Bedrock",
"option_b": "Amazon SageMaker",
"option_c": "Amazon Q",
"option_d": "Amazon S3",
"correct_answers": ["C"],
"explanation_detailed": "Amazon Q é um assistente de IA generativa para trabalho, com chat seguro, geração de conteúdo e ações em aplicativos empresariais. Ele pode ser conectado a fontes internas (por exemplo, por meio de conectores) para responder com base nos seus dados, aplicar políticas e registrar auditoria. Pode usar modelos de fundação do Amazon Bedrock e integra-se ao ecossistema AWS para segurança, identidade e monitoramento. Enquanto o Bedrock fornece os FMs e o SageMaker permite construir modelos e pipelines personalizados, o Q entrega a experiência conversacional pronta para uso corporativo, com governança, controle de acesso e recursos de produtividade.",
"incorrect_explanations": {
"A": "Bedrock expõe FMs e orquestração, mas não é por si a experiência conversacional corporativa pronta.",
"B": "SageMaker é plataforma de ML de ponta a ponta, não um produto de chat corporativo pronto.",
"D": "S3 é armazenamento de objetos; não fornece interface ou lógica de conversação."
}
},
{
"id": "aif-c01-ai_services-125",
"certification_id": "AIF-C01",
"domain": "AI_SERVICES",
"difficulty": "medium",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "Qual é uma consideração chave nas compensações de custo dos serviços de IA generativa da AWS?",
"option_a": "O esquema de cores da interface do usuário",
"option_b": "O número de funcionários na empresa",
"option_c": "Preços baseados em token",
"option_d": "A localização física do data center",
"correct_answers": ["C"],
"explanation_detailed": "Muitos provedores no Amazon Bedrock cobram por 1.000 tokens de entrada e saída. Custos crescem com prompts longos, contexto extenso e respostas prolixas. Estratégias de redução incluem chunking eficiente, prompts concisos, RAG focado, cache de resultados e parametrização (temperatura/top-p) para saídas mais curtas quando possível. Em fluxos de alto volume, considere endpoints otimizados e monitoramento de uso via CloudWatch e Cost Explorer. Para modelos próprios no Amazon SageMaker, custos incluem instâncias, armazenamento e tráfego. Em todos os casos, instrumente telemetria por recurso, trace custo por chamada e meça impacto de negócio, evitando otimizar apenas métricas técnicas.",
"incorrect_explanations": {
"A": "Cores da UI não influenciam cobrança por token, latência ou throughput.",
"B": "Número de funcionários não determina diretamente o custo; o volume de chamadas e tokens sim.",
"D": "Região pode afetar preço marginal, mas a principal variável é volume de tokens e tipo de modelo."
}
},
{
"id": "aif-c01-ai_fundamentals-126",
"certification_id": "AIF-C01",
"domain": "AI_FUNDAMENTALS",
"difficulty": "medium",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "Qual é o objetivo principal dos embeddings em IA generativa?",
"option_a": "Comprimir dados para armazenamento",
"option_b": "Representar dados em um espaço de alta dimensão",
"option_c": "Criptografar informações confidenciais",
"option_d": "Gerar números aleatórios",
"correct_answers": ["B"],
"explanation_detailed": "Embeddings transformam itens (palavras, sentenças, imagens) em vetores de alta dimensão em que proximidade reflete semelhança semântica. São base para busca semântica, deduplicação, classificação e RAG. Em AWS, você pode gerar embeddings com modelos disponíveis no Amazon Bedrock ou no Amazon SageMaker, e armazená-los em mecanismos com busca vetorial, como Amazon OpenSearch Service, ou indexá-los com Amazon Kendra. Durante a inferência, consulta-se os vetores mais próximos ao conteúdo da pergunta e injeta-se o contexto recuperado no prompt, reduzindo alucinações e melhorando precisão factual. Embeddings não são criptografia nem compressão; são representações para cálculo de similaridade.",
"incorrect_explanations": {
"A": "Embora possam ser compactos, a finalidade é semântica, não compressão de dados como ZIP.",
"C": "Não fornecem sigilo como criptografia. Vetores podem até vazar informação se mal geridos.",
"D": "Geração de números aleatórios não se relaciona a representações semânticas."
}
},
{
"id": "aif-c01-ai_fundamentals-127",
"certification_id": "AIF-C01",
"domain": "AI_FUNDAMENTALS",
"difficulty": "easy",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "Qual das seguintes opções NÃO é um caso de uso típico para IA generativa no atendimento ao cliente?",
"option_a": "Chatbots",
"option_b": "Respostas automatizadas de e-mail",
"option_c": "Assistentes robôs físicos",
"option_d": "Geração de FAQ",
"correct_answers": ["C"],
"explanation_detailed": "No atendimento, LLMs oferecem respostas contextualizadas, sumarizam históricos, geram e-mails e atualizam FAQs com base em conhecimento corporativo. Chatbots (por exemplo, Amazon Lex integrando-se a FMs via Amazon Bedrock) lidam com solicitações comuns e escalam para humanos quando necessário. Amazon Kendra e embeddings permitem RAG sobre manuais e políticas. ‘Assistentes robôs físicos’ envolvem robótica e hardware, fora do escopo usual de IA generativa aplicada a canais digitais. A prioridade é reduzir tempo de resolução, manter tom adequado e rastrear fontes, com guardrails e auditoria.",
"incorrect_explanations": {
"A": "Chatbots são uso central, especialmente combinando NLU (Lex) e LLMs para respostas ricas.",
"B": "Geração de e-mail com contexto histórico é aplicação comum para acelerar o agente.",
"D": "LLMs geram e atualizam FAQs a partir de bases internas, reduzindo esforço manual."
}
},
{
"id": "aif-c01-ai_services-128",
"certification_id": "AIF-C01",
"domain": "AI_SERVICES",
"difficulty": "easy",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "Qual é uma vantagem chave de usar os serviços de IA generativa da AWS para construir aplicações em termos de velocidade de desenvolvimento?",
"option_a": "Eles escrevem automaticamente todo o código para você",
"option_b": "Eles fornecem um tempo de lançamento no mercado mais rápido",
"option_c": "Eles eliminam a necessidade de testes",
"option_d": "Eles garantem implantação instantânea",
"correct_answers": ["B"],
"explanation_detailed": "Com Amazon Bedrock, PartyRock e integrações com Amazon Kendra, você prototipa rapidamente sem gerenciar infraestrutura de modelos. Modelos de fundação prontos reduzem o trabalho de treinamento, e SDKs simplificam integração. Amazon SageMaker JumpStart fornece exemplos, notebooks e soluções para acelerar POCs e MVPs. O resultado é menor tempo de ideação ao piloto, com pipelines reproduzíveis depois no SageMaker para produção. Ainda é necessário testar, monitorar e governar o ciclo, mas o tempo de chegada ao mercado cai substancialmente.",
"incorrect_explanations": {
"A": "Geração de código pode ajudar, mas integrações e validações ainda exigem desenvolvimento.",
"C": "Testes permanecem essenciais para qualidade, segurança e métricas de negócio.",
"D": "Implantação exige configuração de endpoints, segurança e observabilidade; não é ‘instantânea’."
}
},
{
"id": "aif-c01-ai_fundamentals-129",
"certification_id": "AIF-C01",
"domain": "AI_FUNDAMENTALS",
"difficulty": "easy",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "O que é não determinismo no contexto da IA generativa?",
"option_a": "Um tipo de arquitetura de modelo",
"option_b": "Um método de pré-processamento de dados",
"option_c": "A propriedade de produzir saídas diferentes para a mesma entrada",
"option_d": "Uma técnica para melhorar a precisão do modelo",
"correct_answers": ["C"],
"explanation_detailed": "LLMs frequentemente amostram a próxima token a partir de distribuições probabilísticas; parâmetros como temperatura e top-p controlam essa variabilidade. Assim, a mesma entrada pode gerar saídas distintas entre runs. Para reprodutibilidade, use seed fixa e baixa temperatura; para criatividade, aumente temperatura/top-p. Em ambientes de produção (Amazon Bedrock ou endpoints no Amazon SageMaker), escolha perfis conforme o caso: fluxos de ‘fato duro’ demandam menor aleatoriedade; conteúdo criativo admite mais variação. Logs e versionamento de prompts ajudam a auditar e comparar execuções.",
"incorrect_explanations": {
"A": "Arquitetura (como Transformer) não define sozinha o comportamento estocástico; a amostragem o faz.",
"B": "Pré-processamento prepara dados; não explica saídas diferentes para a mesma entrada.",
"D": "Não determinismo não é técnica de acurácia; é característica de geração estocástica."
}
},
{
"id": "aif-c01-ai_services-130",
"certification_id": "AIF-C01",
"domain": "AI_SERVICES",
"difficulty": "easy",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "Qual serviço da AWS é projetado para ajudar os desenvolvedores a começar rapidamente com modelos pré-treinados para IA generativa?",
"option_a": "Amazon EC2",
"option_b": "Amazon SageMaker JumpStart",
"option_c": "Amazon RDS",
"option_d": "Amazon CloudFront",
"correct_answers": ["B"],
"explanation_detailed": "O Amazon SageMaker JumpStart oferece catálogos de modelos pré-treinados, exemplos de notebooks e soluções de referência para acelerar POCs. Você pode iniciar instâncias de treino/implantação em poucos cliques, comparar modelos e adaptar ao seu caso. Para IA generativa, JumpStart inclui modelos de linguagem, visão e difusão, com pipelines prontos para ajuste fino e inferência. É complementar ao Amazon Bedrock, que fornece acesso via API a FMs gerenciados; JumpStart facilita experimentar e operacionalizar no ecossistema SageMaker.",
"incorrect_explanations": {
"A": "EC2 provê computação bruta. Útil, porém não oferece catálogos e templates prontos de ML.",
"C": "RDS é banco relacional. Não acelera exploração de modelos de IA.",
"D": "CloudFront é CDN para distribuição de conteúdo, não um hub de modelos de ML."
}
},
{
"id": "aif-c01-ml_development-131",
"certification_id": "AIF-C01",
"domain": "ML_DEVELOPMENT",
"difficulty": "medium",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "O que é Recuperação de Geração Aumentada (RAG)?",
"option_a": "Uma técnica para gerar novos dados",
"option_b": "Um método para combinar informações recuperadas com a geração do modelo",
"option_c": "Um tipo de arquitetura de modelo",
"option_d": "Um algoritmo de compressão de dados",
"correct_answers": ["B"],
"explanation_detailed": "RAG busca evidências em fontes externas (documentos, wikis, bases) e injeta o contexto relevante no prompt antes da geração. Isso fundamenta a resposta, reduz alucinações e permite atualização do ‘conhecimento’ sem retreinar o modelo. Em AWS, use Amazon Kendra ou Amazon OpenSearch Service para recuperação, armazene embeddings no índice e chame o LLM via Amazon Bedrock. Registre fontes, implemente guardrails e meça qualidade com avaliações automáticas e humanas. RAG separa ‘memória de conteúdo’ dos parâmetros do modelo, favorecendo governança e custo.",
"incorrect_explanations": {
"A": "RAG não se limita a criar dados novos; integra busca + geração com base em evidências.",
"C": "É um padrão de sistema, não uma arquitetura específica como Transformer.",
"D": "Não envolve compressão; trata de busca semântica e condicionamento do prompt."
}
},
{
"id": "aif-c01-ai_services-132",
"certification_id": "AIF-C01",
"domain": "AI_SERVICES",
"difficulty": "medium",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "Qual serviço da AWS é adequado para armazenar embeddings em um banco de dados vetorial?",
"option_a": "Amazon S3",
"option_b": "Amazon RDS",
"option_c": "Amazon OpenSearch Service",
"option_d": "Amazon EC2",
"correct_answers": ["C"],
"explanation_detailed": "O Amazon OpenSearch Service suporta k-NN (k-nearest neighbors) e índices vetoriais, permitindo armazenar e recuperar embeddings com consultas de similaridade. Isso é ideal para busca semântica e RAG. O pipeline típico gera embeddings (SageMaker ou Bedrock), indexa vetores e metadados no OpenSearch e, na consulta, retorna os mais similares para compor o prompt enviado ao LLM. S3 armazena objetos; RDS é relacional; EC2 é computação geral. Para busca por proximidade em alta dimensão, índices vetoriais do OpenSearch fornecem escalabilidade, filtros por metadados e integração com o ecossistema AWS.",
"incorrect_explanations": {
"A": "S3 armazena blobs; não oferece busca vetorial nativa por similaridade.",
"B": "RDS é relacional; pode guardar vetores, mas não provê nativamente busca k-NN eficiente.",
"D": "EC2 hospeda software, porém não é um serviço de banco vetorial gerenciado."
}
},
{
"id": "aif-c01-ai_fundamentals-133",
"certification_id": "AIF-C01",
"domain": "AI_FUNDAMENTALS",
"difficulty": "medium",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "Qual é o objetivo principal de ajustar o parâmetro de temperatura na inferência?",
"option_a": "Controlar a temperatura física do servidor",
"option_b": "Ajustar a criatividade ou aleatoriedade da saída do modelo",
"option_c": "Aumentar a velocidade de processamento do modelo",
"option_d": "Reduzir o consumo de energia",
"correct_answers": ["B"],
"explanation_detailed": "Temperatura controla a ‘planicidade’ da distribuição de probabilidade sobre os próximos tokens. Valores altos deixam a distribuição mais uniforme (mais criatividade/variação); valores baixos a tornam mais aguda (respostas mais previsíveis). Em fluxos de produção no Amazon Bedrock ou endpoints do SageMaker, defina temperatura baixa para respostas factuais e consistentes; aumente para brainstorming e ideação. Combine com top-p e limites de comprimento para equilibrar custo por token. Logue parâmetros por chamada para reproduzir comportamentos quando necessário.",
"incorrect_explanations": {
"A": "Não tem relação com hardware térmico. É um hiperparâmetro de amostragem na geração.",
"C": "Temperatura não acelera o modelo; influencia diversidade da linguagem gerada.",
"D": "Consumo energético está mais ligado a carga e hardware do que à temperatura de amostragem."
}
},
{
"id": "aif-c01-ai_fundamentals-134",
"certification_id": "AIF-C01",
"domain": "AI_FUNDAMENTALS",
"difficulty": "medium",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "O que é um prompt de cadeia de pensamento (chain-of-thought)?",
"option_a": "Uma corrente física usada em hardware de IA",
"option_b": "Um prompt que encoraja o modelo a mostrar seu processo de raciocínio",
"option_c": "Um método de vincular vários modelos de IA",
"option_d": "Uma técnica para criptografar prompts",
"correct_answers": ["B"],
"explanation_detailed": "Chain-of-thought instrui o modelo a decompor o problema em etapas intermediárias antes da resposta final, melhorando tarefas de raciocínio (cálculo, lógica, planejamento). Em ambientes empresariais, prefira ‘raciocínio estruturado’ com verificações e regras para evitar vazamento de raciocínio sensível. Em AWS, você pode acoplar validações e function calling (Agentes para o Amazon Bedrock) para executar passos verificados e registrar evidências. Avalie impacto em custo (mais tokens) e latência. Para auditoria, registre prompts, saídas e fontes (Kendra/OpenSearch) e defina guardrails para remover conteúdo inadequado.",
"incorrect_explanations": {
"A": "Não é componente de hardware; é técnica de engenharia de prompt.",
"C": "Não liga múltiplos modelos por si; trata de como o modelo organiza a resposta.",
"D": "Não provê sigilo/criptografia; apenas orienta a forma do raciocínio gerado."
}
},
{
"id": "aif-c01-ml_development-135",
"certification_id": "AIF-C01",
"domain": "ML_DEVELOPMENT",
"difficulty": "medium",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "Qual das seguintes opções NÃO é um método típico para ajustar um modelo de fundação?",
"option_a": "Ajuste de instrução",
"option_b": "Aprendizado por transferência",
"option_c": "Ajuste físico",
"option_d": "Pré-treinamento contínuo",
"correct_answers": ["C"],
"explanation_detailed": "FMs são adaptados por ajuste de instrução (treino em pares instrução-resposta), fine-tuning clássico (transfer learning) ou pré-treinamento contínuo em domínios específicos. Essas técnicas refinam pesos para novas tarefas, estilos e vocabulário. Em AWS, o Amazon SageMaker oferece pipelines de fine-tuning e o Amazon Bedrock disponibiliza ajustes gerenciados para certos provedores. ‘Ajuste físico’ não existe: ajustes ocorrem no espaço de parâmetros do modelo. A escolha depende de dados disponíveis, custo, governança e necessidade de retenção do conhecimento geral do FM.",
"incorrect_explanations": {
"A": "Ajuste de instrução é prática comum para ensinar o modelo a seguir comandos com qualidade.",
"B": "Transfer learning é base de adaptação eficiente, aproveitando o pré-treino.",
"D": "Pré-treino contínuo amplia a base do modelo com dados adicionais do domínio."
}
},
{
"id": "aif-c01-ml_development-136",
"certification_id": "AIF-C01",
"domain": "ML_DEVELOPMENT",
"difficulty": "medium",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "Para que serve a métrica ROUGE na avaliação de modelos de fundação?",
"option_a": "Medir a vermelhidão da saída do modelo",
"option_b": "Avaliar a qualidade dos resumos gerados",
"option_c": "Calcular a eficiência energética do modelo",
"option_d": "Determinar a velocidade de processamento do modelo",
"correct_answers": ["B"],
"explanation_detailed": "ROUGE compara sobreposição de n-grams, subsequências e pares entre o resumo gerado e um ou mais resumos de referência. É útil para avaliar sumarização abstrativa e extrativa. Em pipelines na AWS, você pode rodar avaliação automática em jobs do Amazon SageMaker, armazenar métricas no Amazon S3 e visualizar no CloudWatch ou em dashboards. ROUGE não mede entendimento profundo, portanto combine com métricas humanas (adequação, factualidade) e, quando aplicável, RAG com citabilidade (Kendra/OpenSearch) para reduzir alucinações.",
"incorrect_explanations": {
"A": "O nome ‘ROUGE’ é acrônimo; não mede cor. Avalia sobreposições textuais.",
"C": "Eficiência energética requer métricas de hardware/uso; ROUGE avalia qualidade de sumarização.",
"D": "Velocidade é latência/throughput, não capturada por ROUGE."
}
},
{
"id": "aif-c01-ai_services-137",
"certification_id": "AIF-C01",
"domain": "AI_SERVICES",
"difficulty": "medium",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "Qual é o objetivo principal de usar Agentes para o Amazon Bedrock?",
"option_a": "Contratar agentes humanos para tarefas de IA",
"option_b": "Lidar com tarefas de várias etapas em aplicações de IA",
"option_c": "Manter fisicamente o hardware de IA",
"option_d": "Reduzir o custo dos serviços de IA",
"correct_answers": ["B"],
"explanation_detailed": "Agentes para o Amazon Bedrock orquestram fluxos multi-etapas: interpretar instruções, chamar ferramentas/integrações, consultar bases (Kendra/OpenSearch) e executar ações antes de responder. Eles encapsulam raciocínio estruturado e function calling, reduzindo lógica customizada no cliente. Em uso corporativo, definem-se recursos, políticas e validações para manter rastreabilidade e segurança. O foco é decompor tarefas complexas (por exemplo, abrir ticket, buscar contexto, resumir, registrar) com consistência, não reduzir custos diretamente—embora automação eficiente possa impactar despesas.",
"incorrect_explanations": {
"A": "Não envolve contratação de pessoas; são orquestradores baseados em modelos.",
"C": "Hardware é gerenciado pela AWS; agentes atuam na camada de aplicação/orquestração.",
"D": "Podem otimizar operação, mas o objetivo principal é coordenação de passos e ferramentas."
}
},
{
"id": "aif-c01-ml_development-138",
"certification_id": "AIF-C01",
"domain": "ML_DEVELOPMENT",
"difficulty": "medium",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "Qual das seguintes opções é uma consideração chave ao selecionar um modelo pré-treinado?",
"option_a": "A popularidade do modelo nas redes sociais",
"option_b": "O tamanho físico do servidor que hospeda o modelo",
"option_c": "As capacidades de comprimento de entrada/saída do modelo",
"option_d": "O esquema de cores da documentação do modelo",
"correct_answers": ["C"],
"explanation_detailed": "A janela de contexto (tokens de entrada/saída) determina se o modelo comporta seus prompts e documentos. Para RAG, janelas maiores reduzem necessidade de truncar, mas elevam custo. Avalie também idiomas, ferramentas (function calling), segurança, desempenho e latência. Em AWS, compare FMs no Amazon Bedrock e alternativas no SageMaker JumpStart. Valide com testes no seu domínio e monitore métricas de negócio em produção.",
"incorrect_explanations": {
"A": "Popularidade não assegura adequação; avalie métricas e requisitos do caso.",
"B": "Tamanho de servidor é detalhe operacional; a janela de contexto é fator direto de viabilidade.",
"D": "Cores da documentação não afetam capacidade ou qualidade do modelo."
}
},
{
"id": "aif-c01-responsible_ai-139",
"certification_id": "AIF-C01",
"domain": "RESPONSIBLE_AI",
"difficulty": "medium",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "O que é sequestro de prompt no contexto da engenharia de prompt?",
"option_a": "Um método de otimização de prompts",
"option_b": "Uma técnica para roubar prompts de concorrentes",
"option_c": "Um ataque onde o modelo é enganado para ignorar o prompt pretendido",
"option_d": "Uma maneira de acelerar o processamento do prompt",
"correct_answers": ["C"],
"explanation_detailed": "Prompt hijacking ocorre quando entradas maliciosas injetam instruções que fazem o modelo ignorar as diretrizes originais (por exemplo, em conteúdo recuperado via RAG). Mitigue usando delimitação rigorosa de instruções, filtros de conteúdo, normalização de fontes, ‘context wrapping’ e validações pós-geração. Em AWS, combine Guardrails do Amazon Bedrock, verificação de origem (Kendra/OpenSearch), e monitoramento com CloudWatch/CloudTrail. Mantenha listas de bloqueio, sanitização e testes adversariais contínuos.",
"incorrect_explanations": {
"A": "Não é mera otimização; é um vetor de ataque que altera o comportamento esperado.",
"B": "Não se trata de ‘roubar’ prompts alheios, e sim de manipular o seu modelo via entradas.",
"D": "Não acelera processamento; compromete integridade e segurança das respostas."
}
},
{
"id": "aif-c01-ml_development-140",
"certification_id": "AIF-C01",
"domain": "ML_DEVELOPMENT",
"difficulty": "medium",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "Qual é o objetivo principal do ajuste de instrução em modelos de fundação?",
"option_a": "Ensinar o modelo a seguir instruções específicas",
"option_b": "Reduzir o tamanho do modelo",
"option_c": "Aumentar a velocidade de processamento do modelo",
"option_d": "Alterar a linguagem de programação do modelo",
"correct_answers": ["A"],
"explanation_detailed": "O ajuste de instrução treina o FM em pares instrução-resposta curados para que ele aprenda a obedecer pedidos em linguagem natural com formatos esperados. Isso melhora aderência a políticas, estilo e estrutura de saída. Em AWS, use pipelines no Amazon SageMaker para fine-tuning ou opções gerenciadas no Amazon Bedrock quando disponíveis. A técnica não reduz tamanho nem altera linguagem; refina o comportamento para tarefas específicas e padrões de resposta.",
"incorrect_explanations": {
"B": "Compressão/distilação podem reduzir tamanho, mas não é o objetivo do ajuste de instrução.",
"C": "Velocidade depende de hardware, otimizações e tamanho; ajuste de instrução foca comportamento.",
"D": "Não muda linguagem de programação; padroniza como o modelo segue instruções."
}
},
{
"id": "aif-c01-ml_development-141",
"certification_id": "AIF-C01",
"domain": "ML_DEVELOPMENT",
"difficulty": "medium",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "Para que serve o BERTScore na avaliação de modelos de fundação?",
"option_a": "Medir a eficiência energética do modelo",
"option_b": "Avaliar a qualidade do texto gerado",
"option_c": "Calcular a velocidade de processamento do modelo",
"option_d": "Determinar o valor de mercado do modelo",
"correct_answers": ["B"],
"explanation_detailed": "BERTScore mede similaridade semântica entre texto gerado e referência usando embeddings contextualizados (por exemplo, BERT). Ao invés de só contar n-grams (como ROUGE), captura relações semânticas mais sutis. Em AWS, você pode executar avaliações em jobs do Amazon SageMaker, salvar métricas no Amazon S3 e visualizar no CloudWatch. Combine BERTScore com avaliação humana e checagem factual (RAG + Kendra/OpenSearch) para uma visão robusta da qualidade.",
"incorrect_explanations": {
"A": "Eficiência energética requer métricas de hardware/consumo, não similaridade semântica.",
"C": "Velocidade é latência/throughput. BERTScore avalia qualidade textual.",
"D": "Valor de mercado não é métrica técnica de qualidade de geração."
}
},
{
"id": "aif-c01-ml_development-142",
"certification_id": "AIF-C01",
"domain": "ML_DEVELOPMENT",
"difficulty": "easy",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "Qual é um benefício chave de usar o aprendizado in-context para a personalização do modelo de fundação?",
"option_a": "Não requer dados de treinamento adicionais",
"option_b": "Sempre produz resultados perfeitos",
"option_c": "Reduz o tamanho do modelo",
"option_d": "Elimina a necessidade de prompts",
"correct_answers": ["A"],
"explanation_detailed": "In-context learning mostra poucos exemplos diretamente no prompt para guiar o comportamento do LLM, sem retreino. É útil quando há poucos dados e necessidade de resposta imediata. Em Amazon Bedrock, você estrutura o prompt com instruções, formato de saída e 2–5 exemplos, opcionalmente combinando RAG (Kendra/OpenSearch) para grounding. Monitore custo por token e latência, já que exemplos aumentam o contexto. Se a tarefa exigir consistência forte, evolua para fine-tuning no Amazon SageMaker.",
"incorrect_explanations": {
"B": "Nenhuma técnica garante perfeição; avalie e monitore.",
"C": "Não altera o tamanho do modelo; apenas guia comportamento via exemplos no prompt.",
"D": "Ele depende de prompts bem estruturados; não os elimina."
}
},
{
"id": "aif-c01-ml_development-143",
"certification_id": "AIF-C01",
"domain": "ML_DEVELOPMENT",
"difficulty": "medium",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "Qual é um risco potencial de usar o aprendizado zero-shot na engenharia de prompt?",
"option_a": "O modelo pode ter um desempenho ruim em tarefas para as quais não foi explicitamente treinado",
"option_b": "O modelo se recusará a gerar qualquer saída",
"option_c": "O modelo só funcionará com dados numéricos",
"option_d": "O modelo consumirá energia excessiva",
"correct_answers": ["A"],
"explanation_detailed": "Zero-shot pede ao modelo para realizar uma tarefa sem exemplos no prompt. Em domínios específicos ou formatos estritos, isso pode degradar a qualidade, levando a erros e alucinações. Para mitigar, adicione exemplos (few-shot), use RAG com Kendra ou OpenSearch para grounding, e valide com checagens automáticas. Em produção no Amazon Bedrock, monitore métricas de acerto e retrabalhe prompts/fluxos quando a performance cair.",
"incorrect_explanations": {
"B": "Modelos geralmente respondem; o problema é qualidade, não ausência total de saída.",
"C": "LLMs funcionam com texto, não apenas dados numéricos. A limitação é semântica e de instruções.",
"D": "Consumo depende de carga/infra; zero-shot não implica gasto energético excessivo."
}
},
{
"id": "aif-c01-ml_development-144",
"certification_id": "AIF-C01",
"domain": "ML_DEVELOPMENT",
"difficulty": "medium",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "Qual é o objetivo principal do aprendizado por reforço a partir do feedback humano (RLHF) no treinamento de modelos de fundação?",
"option_a": "Reduzir o consumo de energia do modelo",
"option_b": "Melhorar o desempenho do modelo com base em avaliações humanas",
"option_c": "Aumentar o tamanho do modelo",
"option_d": "Traduzir o modelo para diferentes idiomas",
"correct_answers": ["B"],
"explanation_detailed": "RLHF usa preferências humanas para treinar um modelo de recompensa e, depois, otimizar o FM via reforço para respostas mais alinhadas. Isso melhora utilidade, segurança e estilo. Em AWS, você pode treinar componentes no Amazon SageMaker, gerenciar dados rotulados em S3 e avaliar resultados com pipelines automatizados. Ainda requer curadoria e governança para evitar enviesar o modelo para padrões inadequados.",
"incorrect_explanations": {
"A": "O objetivo não é eficiência energética; é alinhamento com preferências humanas.",
"C": "RLHF não aumenta tamanho do modelo por si; ajusta o comportamento.",
"D": "Tradução envolve modelos/serviços como Amazon Translate, não RLHF."
}
},
{
"id": "aif-c01-ml_development-145",
"certification_id": "AIF-C01",
"domain": "ML_DEVELOPMENT",
"difficulty": "easy",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "Qual das seguintes opções NÃO é uma consideração típica ao preparar dados para o ajuste fino de um modelo de fundação?",
"option_a": "Curadoria de dados",
"option_b": "Tamanho dos dados",
"option_c": "Rotulagem de dados",
"option_d": "Codificação de cores dos dados",
"correct_answers": ["D"],
"explanation_detailed": "Preparação de dados envolve seleção, limpeza, balanceamento, deduplicação, anonimização e rotulagem consistente. O tamanho afeta generalização e custo; a curadoria define qualidade e cobertura de casos. Em AWS, use AWS Glue/SageMaker Processing para transformação e S3 como data lake. ‘Codificação de cores’ não é relevante para texto/código e raramente para imagem, onde o que importa são anotações e formatos corretos. Dados bem preparados resultam em ajustes mais estáveis e úteis.",
"incorrect_explanations": {
"A": "Curadoria decide o que entra no treino e garante representatividade e qualidade.",
"B": "Tamanho impacta desempenho e custo; pouco dado tende a overfitting.",
"C": "Rotulagem consistente é vital para instrução/fine-tuning com pares de alta qualidade."
}
},
{
"id": "aif-c01-ml_development-146",
"certification_id": "AIF-C01",
"domain": "ML_DEVELOPMENT",
"difficulty": "medium",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "O que é modelagem de prompt no contexto da engenharia de prompt?",
"option_a": "Um método de impressão física de prompts",
"option_b": "Uma técnica para criar estruturas de prompt reutilizáveis",
"option_c": "Uma maneira de criptografar prompts",
"option_d": "Um processo de tradução de prompts para diferentes idiomas",
"correct_answers": ["B"],
"explanation_detailed": "Modelagem de prompt define templates padronizados com seções claras: objetivo, contexto, dados de entrada, formato de saída, exemplos e restrições. Isso melhora consistência, facilita versionamento e mensuração A/B. Em AWS, armazene modelos no S3, gerencie versões via CodeCommit/CodePipeline, e teste em SageMaker ou Bedrock. Padronizar prompts reduz variação, acelera novos casos e apoia governança e auditoria de mudanças.",
"incorrect_explanations": {
"A": "Não trata de impressão; é design lógico de instruções para LLMs.",
"C": "Segurança é obtida com KMS/controle de acesso; modelagem de prompt não criptografa.",
"D": "Templates podem ser traduzidos, mas a técnica em si não é um processo de tradução."
}
},
{
"id": "aif-c01-ml_development-147",
"certification_id": "AIF-C01",
"domain": "ML_DEVELOPMENT",
"difficulty": "easy",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "Qual é a principal vantagem de usar o aprendizado few-shot na engenharia de prompt?",
"option_a": "Não requer exemplos no prompt",
"option_b": "Permite que o modelo aprenda com um pequeno número de exemplos",
"option_c": "Sempre produz resultados perfeitos",
"option_d": "Reduz o consumo de energia do modelo",
"correct_answers": ["B"],
"explanation_detailed": "Few-shot inclui alguns exemplos representativos no prompt para orientar estilo, formato e raciocínio sem retreinar o modelo. Isso melhora aderência a padrões específicos e reduz ambiguidades de zero-shot. Em Bedrock, combine few-shot com RAG para respostas fundamentadas, e controle custos limitando o número de exemplos e tokens. Registre versões dos prompts e avalie com métricas de qualidade para garantir estabilidade.",
"incorrect_explanations": {
"A": "Few-shot depende de exemplos; zero-shot é o caso sem exemplos.",
"C": "Não há garantia de perfeição; validação e monitoramento continuam necessários.",
"D": "Consumo está ligado a tokens/hardware; few-shot aumenta tokens no prompt."
}
},
{
"id": "aif-c01-ai_services-148",
"certification_id": "AIF-C01",
"domain": "AI_SERVICES",
"difficulty": "medium",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "Qual serviço da AWS é adequado para armazenar embeddings em um banco de dados relacional?",
"option_a": "Amazon DynamoDB",
"option_b": "Amazon S3",
"option_c": "Amazon Aurora",
"option_d": "Amazon EC2",
"correct_answers": ["C"],
"explanation_detailed": "Embora mecanismos vetoriais (como OpenSearch) sejam ideais, é possível persistir embeddings em um banco relacional para integrações transacionais. O Amazon Aurora (MySQL/PostgreSQL compatível) oferece escalabilidade e suporte a tipos numéricos e extensões que facilitam armazenar vetores e metadados, além de consultas combinando filtros relacionais com IDs de vetor pré-selecionados. Use Aurora para manter consistência com sistemas transacionais e OpenSearch quando precisar de busca k-NN em grande escala e baixa latência. Gere embeddings em SageMaker/Bedrock e gerencie dados no S3 como camada de origem.",
"incorrect_explanations": {
"A": "DynamoDB é NoSQL chave-valor/documento; pode armazenar vetores, mas não oferece SQL relacional.",
"B": "S3 armazena objetos; não oferece consultas relacionais nativas.",
"D": "EC2 é computação; não é um serviço de banco de dados gerenciado."
}
},
{
"id": "aif-c01-ai_fundamentals-149",
"certification_id": "AIF-C01",
"domain": "AI_FUNDAMENTALS",
"difficulty": "medium",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "Qual é uma consideração chave ao avaliar se um modelo de fundação atende efetivamente aos objetivos de negócios?",
"option_a": "A popularidade do modelo nas redes sociais",
"option_b": "O tamanho físico do servidor que hospeda o modelo",
"option_c": "O impacto do modelo no engajamento do usuário",
"option_d": "O esquema de cores da interface do usuário do modelo",
"correct_answers": ["C"],
"explanation_detailed": "Avalie resultados de negócio: engajamento, conversão, CSAT, LTV e economia de tempo. Métricas técnicas (latência, custo por 1.000 tokens) importam, mas precisam se conectar a KPIs. Em AWS, colete telemetria no CloudWatch, experimente com Evidently, armazene logs no S3 e visualize em dashboards (OpenSearch/Kibana). Execute testes A/B com prompts/modelos para validar impacto real e evite otimizações que não movem métricas de negócio.",
"incorrect_explanations": {
"A": "Popularidade não garante eficácia para seu contexto e metas.",
"B": "Dimensões físicas não refletem valor entregue ao usuário.",
"D": "Esquema de cores da UI não mede sucesso de negócio."
}
},
{
"id": "aif-c01-ai_fundamentals-150",
"certification_id": "AIF-C01",
"domain": "AI_FUNDAMENTALS",
"difficulty": "easy",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "Qual é o objetivo principal dos prompts negativos na engenharia de prompt?",
"option_a": "Fazer o modelo gerar emoções negativas",
"option_b": "Dizer ao modelo o que evitar em sua saída",
"option_c": "Reduzir o consumo de energia do modelo",
"option_d": "Diminuir a velocidade de processamento do modelo",
"correct_answers": ["B"],
"explanation_detailed": "Prompts negativos especificam o que não deve aparecer na resposta (por exemplo, ‘não inclua PII’, ‘não invente citações’, ‘sem HTML’). Em fluxos de produção no Amazon Bedrock, combine negativos com validações pós-processamento e guardrails para reforçar políticas. Isso reduz alucinações e conteúdo indesejado, melhora conformidade e torna a saída mais previsível. A técnica não altera diretamente consumo de energia ou latência; é um controle de conteúdo.",
"incorrect_explanations": {
"A": "Não define ‘emoções negativas’; define restrições de conteúdo e formato.",
"C": "Energia depende de carga e hardware; prompts negativos não impactam isso diretamente.",
"D": "Velocidade é função de modelo/infra; negativos são instruções sem efeito direto na latência."
}
},

{
"id": "aif-c01-ml_development-151",
"certification_id": "AIF-C01",
"domain": "ML_DEVELOPMENT",
"difficulty": "medium",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "O que é pré-treinamento contínuo no contexto de modelos de fundação?",
"option_a": "Um método de retreinamento constante do modelo em novos dados",
"option_b": "Uma técnica para treinar modelos 24 horas por dia, 7 dias por semana",
"option_c": "Uma maneira de treinar modelos usando matemática contínua",
"option_d": "Um processo de treinamento de modelos em uma superfície física contínua",
"correct_answers": ["A"],
"explanation_detailed": "Pré-treinamento contínuo (continual/pretraining refresh) atualiza um modelo de fundação já pré-treinado com dados adicionais do domínio para expandir cobertura e reduzir obsolescência, preservando o conhecimento geral. O foco é adaptar vocabulário, estilo e fatos novos sem “esquecer” o que já sabe (mitigar catastrophic forgetting com técnicas como regularização, misturas de dados e checkpoints). Em AWS, armazene corpus no Amazon S3, orquestre jobs de treinamento no Amazon SageMaker, acompanhe métricas no Amazon CloudWatch e gerencie versões com Model Registry. Use avaliações automáticas e humanas para verificar ganhos reais, e políticas de dados (IAM, KMS) para segurança. Não confunda com fine-tuning de instrução; aqui o objetivo é estender o pré-treino de base com textos adicionais amplos.",
"incorrect_explanations": {
"B": "Treinar 24x7 descreve cronograma, não a técnica de atualizar o conhecimento do modelo com dados adicionais.",
"C": "“Matemática contínua” não caracteriza o processo; trata-se de ampliar o corpus e refinar parâmetros.",
"D": "Não envolve superfícies físicas; é atualização estatística dos pesos com novos dados."
}
},
{
"id": "aif-c01-responsible_ai-152",
"certification_id": "AIF-C01",
"domain": "RESPONSIBLE_AI",
"difficulty": "medium",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "O que é envenenamento de prompt no contexto dos riscos da engenharia de prompt?",
"option_a": "Um método de otimização de prompts",
"option_b": "Uma técnica para melhorar a qualidade do prompt",
"option_c": "Um ataque onde conteúdo malicioso é inserido nos dados de treinamento ou prompts",
"option_d": "Uma maneira de acelerar o processamento do prompt",
"correct_answers": ["C"],
"explanation_detailed": "Envenenamento de prompt ocorre quando um atacante injeta instruções ou conteúdo malicioso no material que o modelo consome (por exemplo, documentos em RAG ou dados de ajuste), induzindo respostas incorretas, vazamento de dados ou descumprimento de políticas. Mitigue com validação de origem e saneamento de conteúdos, filtros de PII e de tópicos, isolamento entre instruções do sistema e contexto recuperado, checagens pós-geração e auditoria. No Amazon Bedrock, use Guardrails para bloquear classes de conteúdo, políticas de PII e termos proibidos; com Amazon Kendra/OpenSearch, controle relevância, metadados e proveniência. Registre eventos em CloudTrail/CloudWatch, e aplique IAM/KMS para proteger pipelines e repositórios.",
"incorrect_explanations": {
"A": "Otimização busca melhorar respostas; envenenamento visa manipulação hostil do comportamento.",
"B": "Melhoria de prompt não injeta vetores maliciosos; envenenamento é adversarial.",
"D": "Não acelera processamento; introduz risco de segurança e integridade."
}
},
{
"id": "aif-c01-ml_development-153",
"certification_id": "AIF-C01",
"domain": "ML_DEVELOPMENT",
"difficulty": "medium",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "Para que serve a pontuação BLEU na avaliação de modelos de fundação?",
"option_a": "Medir a eficiência energética do modelo",
"option_b": "Avaliar a qualidade das traduções automáticas",
"option_c": "Calcular a velocidade de processamento do modelo",
"option_d": "Determinar o valor de mercado do modelo",
"correct_answers": ["B"],
"explanation_detailed": "BLEU avalia tradução automática comparando n-grams da saída com traduções humanas de referência e penalizando frases muito curtas (brevity penalty). É objetiva e barata, mas limitada: não captura bem sinônimos ou fluência global. Use BLEU junto de métricas complementares (COMET, BERTScore) e revisão humana para decisões de produção. Em AWS, rode avaliações em jobs no Amazon SageMaker, armazene resultados no S3 e faça dashboards no CloudWatch ou OpenSearch. Para fluxos de tradução práticos, Amazon Translate fornece serviço gerenciado; para modelos personalizados, SageMaker JumpStart e Bedrock permitem experimentar alternativas que, depois, podem ser avaliadas com BLEU.",
"incorrect_explanations": {
"A": "Eficiência energética requer telemetria de hardware/uso; BLEU mede sobreposição textual.",
"C": "Velocidade/latência não é mensurada por BLEU; é métrica de desempenho de sistema.",
"D": "Valor de mercado não é métrica técnica de qualidade de tradução."
}
},
{
"id": "aif-c01-ml_development-154",
"certification_id": "AIF-C01",
"domain": "ML_DEVELOPMENT",
"difficulty": "medium",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "Qual é um benefício chave de usar a aprendizagem por transferência para personalização de modelo de fundação?",
"option_a": "Não requer treinamento adicional",
"option_b": "Permite que o modelo aproveite o conhecimento de um domínio para outro",
"option_c": "Sempre produz resultados perfeitos",
"option_d": "Reduz o tamanho do modelo para zero",
"correct_answers": ["B"],
"explanation_detailed": "Aprendizado por transferência reutiliza representações aprendidas no pré-treino para acelerar adaptação a novas tarefas/domínios com menos dados e custo. O modelo mantém conhecimento geral e ajusta camadas finais ou parâmetros específicos (p.ex., LoRA) para estilo/terminologia. Em AWS, use Amazon SageMaker para fine-tuning controlado e experimentos reprodutíveis; JumpStart fornece modelos e notebooks prontos. Valide ganhos com métricas específicas do negócio e avalie viés/robustez com SageMaker Clarify. Controle versões e políticas de acesso (Model Registry, IAM/KMS) e monitore drift pós-implantação com Model Monitor.",
"incorrect_explanations": {
"A": "Mesmo com transferência, há treino/adaptação, ainda que menor que do zero.",
"C": "Não há garantia de perfeição; depende de dados, avaliação e governança.",
"D": "Não elimina o modelo; reduz esforço de treino, não o tamanho a zero."
}
},
{
"id": "aif-c01-ai_fundamentals-155",
"certification_id": "AIF-C01",
"domain": "AI_FUNDAMENTALS",
"difficulty": "medium",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "Qual é o objetivo principal do espaço latente do modelo no contexto da engenharia de prompt?",
"option_a": "Armazenar fisicamente o modelo",
"option_b": "Representar a compreensão e o conhecimento internos do modelo",
"option_c": "Aumentar a velocidade de processamento do modelo",
"option_d": "Reduzir o consumo de energia do modelo",
"correct_answers": ["B"],
"explanation_detailed": "O espaço latente é a representação interna em que significados, relações e padrões são codificados em vetores/ativação. Prompts eficazes “navegam” esse espaço ao condicionar a geração para regiões desejadas (estilo, tom, formato), sem acesso direto aos vetores. Em modelos de imagem (difusão), o latente guia conteúdos/estética; em LLMs, estrutura o próximo token com base no histórico. Em AWS, você manipula esse comportamento via Amazon Bedrock (parâmetros de inferência, few-shot) e integra recuperação (Kendra/OpenSearch) para ancoragem factual. Métricas e testes A/B em SageMaker/CloudWatch confirmam se a orientação via prompt realmente move a saída para a região desejada do latente.",
"incorrect_explanations": {
"A": "Não é mídia física; é representação matemática nas ativações e pesos.",
"C": "Velocidade depende de hardware e otimizações; o latente representa semântica.",
"D": "Consumo energético não é função do latente; é de carga/infraestrutura."
}
},
{
"id": "aif-c01-responsible_ai-156",
"certification_id": "AIF-C01",
"domain": "RESPONSIBLE_AI",
"difficulty": "easy",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "Qual das seguintes opções NÃO é uma característica da IA responsável?",
"option_a": "Justiça",
"option_b": "Robustez",
"option_c": "Lucratividade",
"option_d": "Inclusividade",
"correct_answers": ["C"],
"explanation_detailed": "IA responsável está ancorada em justiça, transparência, segurança, privacidade, robustez, governança e inclusão. “Lucratividade” é objetivo de negócio, não princípio ético. Em AWS, combine Guardrails for Amazon Bedrock para filtros de conteúdo/PII, SageMaker Clarify para detectar vieses, Model Cards para documentação e A2I para revisão humana. Implemente auditoria (CloudTrail), controle de acesso (IAM/KMS), e monitoramento contínuo (Model Monitor). Tenha políticas de dados e avaliações de impacto. Foque efeitos distributivos, não apenas métricas agregadas.",
"incorrect_explanations": {
"A": "Justiça é central: tratamento equitativo entre grupos e contextos.",
"B": "Robustez garante desempenho sob variações e ataques.",
"D": "Inclusão mitiga exclusões sistêmicas e erros em grupos minoritários."
}
},
{
"id": "aif-c01-responsible_ai-157",
"certification_id": "AIF-C01",
"domain": "RESPONSIBLE_AI",
"difficulty": "medium",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "Qual é o objetivo principal do Guardrails for Amazon Bedrock?",
"option_a": "Proteger fisicamente o hardware de IA",
"option_b": "Identificar e aplicar recursos de IA responsável",
"option_c": "Aumentar o desempenho do modelo",
"option_d": "Reduzir o consumo de energia",
"correct_answers": ["B"],
"explanation_detailed": "Guardrails for Amazon Bedrock aplica políticas de uso responsável: filtros de PII, bloqueio de categorias sensíveis, limites de toxicidade, proteção contra prompt injection e conformidade de respostas. Opera na camada de aplicação, independentemente do modelo de fundação, com logs para auditoria e ajustes iterativos. Combine com IAM/KMS para segurança, CloudWatch para monitoramento e validações pós-processamento. Em fluxos RAG, envolva Kendra/OpenSearch com proveniência e saneamento. Guardrails não é ferramenta de desempenho ou hardware; é camada de políticas e segurança de conteúdo para manter saídas dentro de padrões definidos pelo negócio.",
"incorrect_explanations": {
"A": "Não trata de datacenter/hardware; foca conteúdo e política.",
"C": "Pode evitar respostas ruins, mas não é mecanismo de aceleração de modelo.",
"D": "Consumo energético depende de carga/infra; guardrails não o reduz diretamente."
}
},
{
"id": "aif-c01-responsible_ai-158",
"certification_id": "AIF-C01",
"domain": "RESPONSIBLE_AI",
"difficulty": "medium",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "Qual das seguintes opções é uma consideração chave na seleção responsável de modelos?",
"option_a": "A popularidade do modelo",
"option_b": "O impacto ambiental do modelo",
"option_c": "O país de origem do modelo",
"option_d": "O esquema de cores do modelo",
"correct_answers": ["B"],
"explanation_detailed": "Seleção responsável pondera desempenho, segurança, viés, privacidade, riscos de IP e pegada ambiental (energia/CO₂). Em AWS, escolha modelos no Amazon Bedrock ou no SageMaker JumpStart considerando janela de contexto, suporte a ferramentas, qualidade, custo por token e requisitos regulatórios. Avalie consumo e escalabilidade por região (data residency), e adote boas práticas do pilar de Sustentabilidade do Well-Architected. Documente justificativas em Model Cards e rode testes de justiça com Clarify. “Popularidade” e estética não substituem due diligence técnica e ética.",
"incorrect_explanations": {
"A": "Popularidade não indica adequação ao seu caso, riscos e custos.",
"C": "Origem geográfica não determina qualidade/ética por si; avalie evidências.",
"D": "Cores não têm relação com responsabilidade ou desempenho."
}
},
{
"id": "aif-c01-responsible_ai-159",
"certification_id": "AIF-C01",
"domain": "RESPONSIBLE_AI",
"difficulty": "medium",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "Qual é um risco legal potencial de trabalhar com IA generativa?",
"option_a": "Lesões físicas aos usuários",
"option_b": "Reivindicações de violação de propriedade intelectual",
"option_c": "Aumento nas contas de eletricidade",
"option_d": "Redução na velocidade da internet",
"correct_answers": ["B"],
"explanation_detailed": "Conteúdos gerados podem se assemelhar a obras protegidas, usar marcas ou incorporar materiais sensíveis. Mitigue com políticas de dados, filtros de conteúdo (Guardrails), checagem de similaridade, fontes autorizadas e proveniência em RAG (Kendra/OpenSearch). Documente usos, licenças e limitações em Model Cards. Em AWS, restrinja acesso com IAM/KMS, audite com CloudTrail e monitore fluxos em CloudWatch. Estabeleça revisão humana (A2I) quando houver alto risco e defina termos de uso claros a clientes internos. Trate também privacidade/PII e confidencialidade de prompts e outputs.",
"incorrect_explanations": {
"A": "IA generativa opera digitalmente; o risco primário é legal/compliance, não físico.",
"C": "Custo de energia é operacional, não risco legal direto.",
"D": "Largura de banda não define exposição legal de IP."
}
},
{
"id": "aif-c01-responsible_ai-160",
"certification_id": "AIF-C01",
"domain": "RESPONSIBLE_AI",
"difficulty": "easy",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "Qual das seguintes opções NÃO é uma característica de conjuntos de dados importantes para a IA responsável?",
"option_a": "Inclusividade",
"option_b": "Diversidade",
"option_c": "Tamanho",
"option_d": "Representação equilibrada",
"correct_answers": ["C"],
"explanation_detailed": "Tamanho ajuda, mas não garante justiça. Conjuntos responsáveis priorizam diversidade de fontes, representação equilibrada entre grupos e documentação de proveniência e consentimento. Em AWS, gerencie dados no S3/Lake Formation, catalogue no Glue Data Catalog, aplique IAM/KMS e rode avaliações de viés com SageMaker Clarify por subgrupos. Registre decisões e limitações em Model Cards e mantenha processos de atualização para corrigir lacunas. Valide qualidade de rótulos e cobertura, não apenas volume.",
"incorrect_explanations": {
"A": "Inclusividade reduz lacunas de cobertura e erros sistemáticos.",
"B": "Diversidade mitiga viés de amostragem e melhora generalização.",
"D": "Balanceamento evita que o modelo favoreça classes/grupos dominantes."
}
},
{
"id": "aif-c01-ai_fundamentals-161",
"certification_id": "AIF-C01",
"domain": "AI_FUNDAMENTALS",
"difficulty": "easy",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "O que é overfitting no contexto de modelos de IA?",
"option_a": "Quando um modelo tem um desempenho muito bom nos dados de treinamento, mas ruim em novos dados",
"option_b": "Quando um modelo é muito grande para caber na memória",
"option_c": "Quando um modelo gera saídas que são muito longas",
"option_d": "Quando um modelo consome muita energia",
"correct_answers": ["A"],
"explanation_detailed": "Overfitting ocorre quando o modelo aprende ruídos e peculiaridades do treino, falhando em generalizar. Sintomas: grande gap entre métricas de treino e validação, previsões instáveis. Mitigações: regularização, early stopping, data augmentation, validação cruzada e mais dados representativos. Em AWS, use SageMaker Experiments para rastrear métricas, Debugger para detectar problemas de treino, e Model Monitor para observar drift após implantação. Documente limitações em Model Cards e avalie por subgrupos com Clarify para entender onde a generalização é fraca.",
"incorrect_explanations": {
"B": "Limitação de memória é restrição de infraestrutura, não definição de overfitting.",
"C": "Comprimento de saída é escolha de inferência; não caracteriza overfitting.",
"D": "Consumo de energia não define generalização do modelo."
}
},
{
"id": "aif-c01-ai_services-162",
"certification_id": "AIF-C01",
"domain": "AI_SERVICES",
"difficulty": "medium",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "Qual serviço da AWS é projetado para ajudar a detectar e monitorar vieses em modelos de aprendizado de máquina?",
"option_a": "Amazon EC2",
"option_b": "Amazon S3",
"option_c": "Amazon SageMaker Clarify",
"option_d": "Amazon RDS",
"correct_answers": ["C"],
"explanation_detailed": "SageMaker Clarify detecta viés em dados e modelos, gera relatórios por atributos (ex.: gênero, região), produz explicações (SHAP) e suporta monitoramento em produção. Integre Clarify a pipelines de ML no SageMaker para avaliar antes do deploy e usar Model Monitor para continuar a checagem após a implantação. Combine com Model Cards para registrar contexto, métricas e limitações, e com A2I quando a decisão exigir revisão humana. EC2, S3 e RDS não fornecem essa análise de viés nativamente.",
"incorrect_explanations": {
"A": "EC2 provê computação, não ferramenta nativa de viés/explicabilidade.",
"B": "S3 é armazenamento de objetos; não avalia viés.",
"D": "RDS é banco relacional; não mede viés de modelo."
}
},
{
"id": "aif-c01-responsible_ai-163",
"certification_id": "AIF-C01",
"domain": "RESPONSIBLE_AI",
"difficulty": "easy",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "Qual é a principal diferença entre modelos de IA transparentes e não transparentes?",
"option_a": "Modelos transparentes são sempre mais precisos",
"option_b": "Modelos transparentes permitem a compreensão de seu processo de tomada de decisão",
"option_c": "Modelos transparentes são sempre menores em tamanho",
"option_d": "Modelos transparentes consomem menos energia",
"correct_answers": ["B"],
"explanation_detailed": "Transparência significa explicar como entradas influenciam saídas, quais atributos pesam e sob quais condições o modelo falha. Técnicas: modelos intrinsecamente interpretáveis (árvores lineares), explicações locais (LIME/SHAP), relatórios (Model Cards) e documentação de dados. Em AWS, Clarify gera explicações e detecção de viés; Model Cards registram contexto de uso; CloudTrail/Config auditam mudanças. Transparência não implica maior acurácia nem menor custo; é um atributo de governança e confiança.",
"incorrect_explanations": {
"A": "Transparência não garante acurácia superior; pode até trade-off com complexidade.",
"C": "Tamanho não define interpretabilidade; depende da estrutura e técnicas de explicação.",
"D": "Consumo energético é tema de infraestrutura, não de interpretabilidade."
}
},
{
"id": "aif-c01-responsible_ai-164",
"certification_id": "AIF-C01",
"domain": "RESPONSIBLE_AI",
"difficulty": "medium",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "Qual ferramenta pode ser usada para documentar informações do modelo para transparência?",
"option_a": "Amazon SageMaker Model Cards",
"option_b": "Amazon EC2",
"option_c": "Amazon S3",
"option_d": "Amazon RDS",
"correct_answers": ["A"],
"explanation_detailed": "SageMaker Model Cards consolida propósito, dados de treinamento, métricas, avaliações de viés/robustez, riscos e recomendações de uso, promovendo transparência e governança. Integre com Clarify para anexar resultados de justiça/explicabilidade e com Model Registry para versionamento e trilha de auditoria. Armazene artefatos no S3 e monitore em produção com Model Monitor. EC2, S3 e RDS são úteis em infraestrutura, mas não substituem documentação padronizada do ciclo de vida do modelo.",
"incorrect_explanations": {
"B": "EC2 fornece compute; não é ferramenta de documentação de modelos.",
"C": "S3 armazena artefatos, mas não estrutura fichas de modelo.",
"D": "RDS é banco de dados; não provê cards de modelos."
}
},
{
"id": "aif-c01-responsible_ai-165",
"certification_id": "AIF-C01",
"domain": "RESPONSIBLE_AI",
"difficulty": "medium",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "Qual é uma possível compensação entre segurança e transparência do modelo?",
"option_a": "Modelos mais seguros são sempre menos transparentes",
"option_b": "Modelos transparentes são sempre menos seguros",
"option_c": "Maior transparência pode revelar vulnerabilidades",
"option_d": "Não há compensações entre segurança e transparência",
"correct_answers": ["C"],
"explanation_detailed": "Explicações detalhadas e documentação extensa podem expor limites, features sensíveis ou protocolos, o que facilita ataques (por exemplo, extração de modelo, evasão, prompt injection). Mitigue fornecendo explicações úteis ao usuário sem vazar sinal de exploração, agregando informações e controlando acesso. Em AWS, use IAM/KMS para segregar ambientes, CloudTrail para auditoria, Guardrails para políticas de conteúdo e testes adversariais contínuos. Transparência e segurança são compatíveis, mas requerem design deliberado.",
"incorrect_explanations": {
"A": "Não é regra geral; bons projetos alcançam segurança e transparência juntas.",
"B": "Transparência não implica insegurança inevitável; depende de escopo e controles.",
"D": "Há trade-offs práticos; negar isso ignora risco operacional."
}
},
{
"id": "aif-c01-responsible_ai-166",
"certification_id": "AIF-C01",
"domain": "RESPONSIBLE_AI",
"difficulty": "easy",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "O que é design centrado no ser humano no contexto da IA explicável?",
"option_a": "Projetar sistemas de IA que se pareçam com humanos",
"option_b": "Criar sistemas de IA que priorizem as necessidades e a compreensão humanas",
"option_c": "Usar humanos em vez de IA para todas as tarefas",
"option_d": "Projetar sistemas de IA que só podem ser usados por humanos",
"correct_answers": ["B"],
"explanation_detailed": "Design centrado no ser humano adapta interfaces, explicações e controles para decisões seguras e auditáveis. Explique em linguagem clara, mostre fontes (RAG), permita contestação e feedback, e forneça confiança calibrada (scores, limites). Em AWS, combine Bedrock + Kendra/OpenSearch para citabilidade, Clarify para importar explicações, A2I para revisão humana e Model Cards para orientar uso. O objetivo é tornar a IA útil e compreensível, não antropomórfica.",
"incorrect_explanations": {
"A": "Aparência humana é irrelevante para explicabilidade.",
"C": "Não elimina IA; equilibra automação e intervenção humana.",
"D": "Uso não é exclusividade; é acessibilidade e entendimento."
}
},
{
"id": "aif-c01-responsible_ai-167",
"certification_id": "AIF-C01",
"domain": "RESPONSIBLE_AI",
"difficulty": "easy",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "Qual das seguintes opções NÃO é um efeito típico de viés em sistemas de IA?",
"option_a": "Tratamento injusto de certos grupos demográficos",
"option_b": "Melhor precisão geral",
"option_c": "Possíveis problemas legais",
"option_d": "Perda de confiança do usuário",
"correct_answers": ["B"],
"explanation_detailed": "Viés tende a reduzir precisão em subgrupos, gerar injustiças, criar risco legal e corroer confiança. Comportamentos incluem falsos positivos/negativos desbalanceados e respostas inconsistentes entre populações. Em AWS, use Clarify para medir viés, Model Monitor para monitorar drift de distribuição e A2I para revisão humana em decisões críticas. Mitigue com dados diversos, reamostragem, reponderação e restrições de fairness, registrando resultados em Model Cards.",
"incorrect_explanations": {
"A": "Tratamento desigual é consequência típica quando dados são enviesados.",
"C": "Falhas de justiça expõem a litígios e fiscalização regulatória.",
"D": "Usuários perdem confiança quando percebem resultados injustos/incoerentes."
}
},
{
"id": "aif-c01-responsible_ai-168",
"certification_id": "AIF-C01",
"domain": "RESPONSIBLE_AI",
"difficulty": "medium",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "Qual é o objetivo principal da análise de subgrupo na IA responsável?",
"option_a": "Dividir a equipe de desenvolvimento em subgrupos",
"option_b": "Analisar o desempenho do modelo em diferentes grupos demográficos",
"option_c": "Reduzir o tamanho do modelo",
"option_d": "Aumentar a velocidade de processamento do modelo",
"correct_answers": ["B"],
"explanation_detailed": "Análise de subgrupo mede desempenho por grupos (ex.: idade, região, idioma) para detectar assimetrias e injustiças. Compare métricas (precision/recall, MAE) por grupo, aplique testes estatísticos e defina metas mínimas de qualidade. Em AWS, use Clarify para relatórios por atributo, e Model Monitor para observar desvios ao longo do tempo. Registre limitações em Model Cards e implemente ações corretivas (recoleta de dados, reponderação, novas features).",
"incorrect_explanations": {
"A": "Não trata de organograma; é avaliação estatística por população.",
"C": "Tamanho do modelo não resolve por si desequilíbrios por grupos.",
"D": "Velocidade não evidencia justiça; foque métricas por subgrupo."
}
},
{
"id": "aif-c01-responsible_ai-169",
"certification_id": "AIF-C01",
"domain": "RESPONSIBLE_AI",
"difficulty": "easy",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "Qual das seguintes opções é uma consideração chave para a diversidade do conjunto de dados na IA responsável?",
"option_a": "Usar dados de apenas uma fonte",
"option_b": "Garantir a representação de vários grupos demográficos",
"option_c": "Usar o maior conjunto de dados disponível, independentemente do conteúdo",
"option_d": "Usar apenas os dados mais recentes",
"correct_answers": ["B"],
"explanation_detailed": "Diversidade de dados reduz viés de amostragem e melhora generalização. Busque origem variada, cenários e linguagens/dialetos, com documentação de proveniência, consentimento e políticas de retenção. Em AWS, catalogue com Glue Data Catalog, controle acesso com Lake Formation/IAM, e use Clarify para medir cobertura por grupo. Priorize qualidade e representatividade, não apenas volume ou recência.",
"incorrect_explanations": {
"A": "Fonte única aumenta risco de viés e baixa cobertura.",
"C": "Maior volume sem curadoria não assegura justiça.",
"D": "Recência ajuda, mas não substitui diversidade e cobertura."
}
},
{
"id": "aif-c01-responsible_ai-170",
"certification_id": "AIF-C01",
"domain": "RESPONSIBLE_AI",
"difficulty": "easy",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "O que é veracidade no contexto da IA responsável?",
"option_a": "A velocidade em que o sistema de IA opera",
"option_b": "A veracidade e precisão das saídas do sistema de IA",
"option_c": "O tamanho do modelo de IA",
"option_d": "O custo de operação do sistema de IA",
"correct_answers": ["B"],
"explanation_detailed": "Veracidade exige respostas corretas, verificáveis e consistentes. Em soluções RAG, inclua fontes citáveis (Kendra/OpenSearch) no prompt para aumentar fidelidade factual. Aplique guardrails contra informações proibidas e processos de validação humana (A2I) quando necessário. Em produção, acompanhe acurácia, taxas de correção e relatórios de erro no CloudWatch, e mantenha Model Cards descrevendo limites de uso e confiabilidade.",
"incorrect_explanations": {
"A": "Velocidade é latência; não mede verdade factual.",
"C": "Tamanho não implica maior veracidade.",
"D": "Custo é operacional; veracidade é qualidade informacional."
}
},
{
"id": "aif-c01-responsible_ai-171",
"certification_id": "AIF-C01",
"domain": "RESPONSIBLE_AI",
"difficulty": "medium",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "Qual das seguintes opções NÃO é um método típico para melhorar a interpretabilidade do modelo?",
"option_a": "Usar modelos mais simples",
"option_b": "Fornecer classificações de importância de atributos",
"option_c": "Aumentar o tamanho do modelo",
"option_d": "Gerar explicações legíveis por humanos",
"correct_answers": ["C"],
"explanation_detailed": "Interpretabilidade melhora com estruturas mais simples (árvores, regressões), explicações locais (LIME/SHAP) e relatórios claros (Model Cards). Aumentar tamanho tende a reduzir transparência. Em AWS, Clarify gera importâncias/SHAP e Model Cards documenta propósito, dados e limites. Em aplicações de alto risco, combine revisão humana (A2I) com explicações e regras para decisões auditáveis.",
"incorrect_explanations": {
"A": "Modelos simples facilitam entendimento e auditoria.",
"B": "Importâncias ajudam a ver drivers de decisão.",
"D": "Explicações legíveis conectam técnica ao usuário final."
}
},
{
"id": "aif-c01-ai_services-172",
"certification_id": "AIF-C01",
"domain": "AI_SERVICES",
"difficulty": "easy",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "Qual é o objetivo principal do Amazon Augmented AI (A2I) na IA responsável?",
"option_a": "Substituir trabalhadores humanos por IA",
"option_b": "Facilitar a revisão humana das previsões de IA",
"option_c": "Aumentar o tamanho do modelo de IA",
"option_d": "Reduzir o consumo de energia dos sistemas de IA",
"correct_answers": ["B"],
"explanation_detailed": "Amazon A2I insere humanos no loop para revisar previsões de IA em cenários de baixa confiança, amostragens de auditoria ou exceções. Integra com SageMaker endpoints, Model Monitor e fluxos de labeling. Defina políticas de roteamento (limiares), colete feedback e use-o para corrigir dados, refinar prompts ou ajustar modelos. Em aplicações reguladas, A2I ajuda a demonstrar governança e controle humano efetivo.",
"incorrect_explanations": {
"A": "Objetivo não é substituir humanos; é supervisionar e validar a IA.",
"C": "Tamanho do modelo não é afetado por A2I.",
"D": "Energia não é foco; é qualidade e conformidade."
}
},
{
"id": "aif-c01-responsible_ai-173",
"certification_id": "AIF-C01",
"domain": "RESPONSIBLE_AI",
"difficulty": "medium",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "Qual das seguintes opções é uma consideração chave ao avaliar a justiça de um sistema de IA?",
"option_a": "A velocidade de processamento do sistema",
"option_b": "O consumo de energia do sistema",
"option_c": "O impacto do sistema em diferentes grupos demográficos",
"option_d": "A popularidade do sistema entre os usuários",
"correct_answers": ["C"],
"explanation_detailed": "Justiça exige medir desempenho por subgrupos, investigar disparidades, entender causas (dados/algoritmo) e aplicar mitigação. Em AWS, use Clarify para relatórios por atributo, Model Monitor para vigilância contínua e Model Cards para documentar limites. Conecte métricas técnicas a consequências reais (erros custosos para grupos específicos) e adote políticas de revisão humana quando a incerteza for alta.",
"incorrect_explanations": {
"A": "Latência não mede tratamento equitativo.",
"B": "Energia é sustentabilidade, não justiça.",
"D": "Popularidade não revela viés ou impacto desigual."
}
},
{
"id": "aif-c01-ai_fundamentals-174",
"certification_id": "AIF-C01",
"domain": "AI_FUNDAMENTALS",
"difficulty": "easy",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "O que é underfitting no contexto de modelos de IA?",
"option_a": "Quando um modelo é muito pequeno para caber na memória",
"option_b": "Quando um modelo tem um desempenho ruim tanto nos dados de treinamento quanto em novos dados",
"option_c": "Quando um modelo gera saídas que são muito curtas",
"option_d": "Quando um modelo consome pouca energia",
"correct_answers": ["B"],
"explanation_detailed": "Underfitting ocorre quando o modelo é simples demais para capturar padrões, rendendo erro alto em treino e validação. Mitigue com modelos mais expressivos, features melhores e mais épocas. Em AWS, use SageMaker Experiments para comparar arquiteturas/hiperparâmetros e Debugger para identificar gargalos. Monitore métricas em CloudWatch e registre resultados e limitações em Model Cards.",
"incorrect_explanations": {
"A": "Memória insuficiente não define underfitting; é limitação operacional.",
"C": "Comprimento de saída não indica falta de capacidade de modelagem.",
"D": "Baixo consumo energético não caracteriza subajuste."
}
},
{
"id": "aif-c01-responsible_ai-175",
"certification_id": "AIF-C01",
"domain": "RESPONSIBLE_AI",
"difficulty": "medium",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "Qual das seguintes opções NÃO é um benefício típico do uso de modelos de código aberto para transparência?",
"option_a": "Capacidade de inspecionar o código do modelo",
"option_b": "Melhorias impulsionadas pela comunidade",
"option_c": "Desempenho perfeito garantido",
"option_d": "Potencial para auditorias independentes",
"correct_answers": ["C"],
"explanation_detailed": "Modelos open-source permitem inspeção, auditorias externas, reprodutibilidade e customização, o que ajuda transparência e confiança. Não garantem acurácia perfeita. Em AWS, hospede treinamentos no SageMaker/EC2, controle acesso via IAM, registre experimentos e métricas, e crie Model Cards. Avalie licenças e riscos de IP e aplique Clarify para medições de viés e explicações, independentemente de serem abertos ou proprietários.",
"incorrect_explanations": {
"A": "Código aberto viabiliza inspeção detalhada e revisões.",
"B": "Comunidade pode descobrir bugs e melhorar desempenho.",
"D": "Código disponível facilita auditorias independentes e governança."
}
},
{
"id": "aif-c01-ml_development-176",
"certification_id": "AIF-C01",
"domain": "ML_DEVELOPMENT",
"difficulty": "medium",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "Qual é o objetivo principal de analisar a qualidade do rótulo na IA responsável?",
"option_a": "Melhorar a aparência visual dos rótulos",
"option_b": "Garantir a precisão e consistência dos rótulos de dados",
"option_c": "Reduzir o número de rótulos usados",
"option_d": "Aumentar a velocidade de processamento do modelo",
"correct_answers": ["B"],
"explanation_detailed": "Rótulos de baixa qualidade geram ruído e viés. Avalie concordância entre anotadores, distribuições por classe e exemplos ambíguos. Em AWS, use SageMaker Ground Truth para rotulagem com controle de qualidade, A2I para revisão humana, e Clarify para checar impactos de rótulos em métricas por subgrupo. Monitore drift de rótulo após o deploy com Model Monitor e atualize datasets conforme surgem novos padrões.",
"incorrect_explanations": {
"A": "Estética é irrelevante; importa precisão/consistência.",
"C": "Menos rótulos não resolve; qualidade e cobertura são críticas.",
"D": "Velocidade vem de infraestrutura; qualidade de rótulo afeta acurácia."
}
},
{
"id": "aif-c01-responsible_ai-177",
"certification_id": "AIF-C01",
"domain": "RESPONSIBLE_AI",
"difficulty": "easy",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "Qual das seguintes opções é uma consequência potencial do uso de conjuntos de dados enviesados no treinamento de IA?",
"option_a": "Melhor desempenho do modelo para todos os grupos",
"option_b": "Resultados injustos ou discriminatórios para certos grupos",
"option_c": "Consumo de energia reduzido",
"option_d": "Tempos de treinamento de modelo mais rápidos",
"correct_answers": ["B"],
"explanation_detailed": "Dados enviesados levam o modelo a replicar assimetrias, produzindo decisões injustas para grupos sub-representados. Em AWS, use Clarify para medir viés e simular impactos, A2I para revisão humana de casos críticos e Model Cards para registrar riscos. Atualize datasets com fontes diversas e execute reponderações/reamostragem. Monitore continuamente métricas por subgrupo em produção com Model Monitor.",
"incorrect_explanations": {
"A": "Desempenho pode piorar para segmentos inteiros, não melhorar para todos.",
"C": "Energia não é consequência direta de viés nos dados.",
"D": "Tempo de treino não é determinado por justiça dos dados."
}
},
{
"id": "aif-c01-responsible_ai-178",
"certification_id": "AIF-C01",
"domain": "RESPONSIBLE_AI",
"difficulty": "medium",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "Qual é o objetivo principal das práticas responsáveis na seleção de modelos?",
"option_a": "Escolher sempre o maior modelo disponível",
"option_b": "Selecionar modelos com base unicamente nas métricas de desempenho",
"option_c": "Equilibrar o desempenho com considerações éticas e sustentabilidade",
"option_d": "Escolher o modelo mais caro",
"correct_answers": ["C"],
"explanation_detailed": "Seleção responsável pondera acurácia, custo, privacidade, viés, robustez e pegada ambiental. Em AWS, compare FMs do Bedrock e opções no SageMaker JumpStart avaliando janela de contexto, ferramentas, segurança, custo por token e requisitos legais. Documente decisões em Model Cards e teste justiça com Clarify. Operacionalize governança com CloudTrail/Config e revisões periódicas.",
"incorrect_explanations": {
"A": "Maior nem sempre é melhor; custo/latência podem inviabilizar.",
"B": "Desempenho isolado ignora riscos e compliance.",
"D": "Preço não é proxy de adequação ou responsabilidade."
}
},
{
"id": "aif-c01-responsible_ai-179",
"certification_id": "AIF-C01",
"domain": "RESPONSIBLE_AI",
"difficulty": "medium",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "Qual das seguintes opções NÃO é uma característica típica de uma fonte de dados com curadoria para IA responsável?",
"option_a": "Precisão verificada",
"option_b": "Proveniência conhecida",
"option_c": "Maior tamanho possível",
"option_d": "Métodos de coleta éticos",
"correct_answers": ["C"],
"explanation_detailed": "Curadoria prioriza qualidade, consentimento, licenças, rastreabilidade e representatividade. Tamanho por si não garante justiça nem factualidade. Em AWS, armazene no S3 com políticas de Lake Formation, catalogue no Glue, proteja com KMS/IAM e documente em data dictionaries. Audite fluxos com CloudTrail e avalie viés/impactos com Clarify. Prefira menos dados de alta qualidade a mais dados sem governança.",
"incorrect_explanations": {
"A": "Verificação de precisão é requisito para confiabilidade.",
"B": "Proveniência clara viabiliza auditoria e conformidade.",
"D": "Coleta ética reduz riscos legais e reputacionais."
}
},
{
"id": "aif-c01-responsible_ai-180",
"certification_id": "AIF-C01",
"domain": "RESPONSIBLE_AI",
"difficulty": "medium",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "Qual é o objetivo principal das auditorias humanas em sistemas de IA responsáveis?",
"option_a": "Substituir sistemas de IA por trabalhadores humanos",
"option_b": "Verificar e validar as saídas e processos do sistema de IA",
"option_c": "Aumentar a velocidade de processamento do sistema de IA",
"option_d": "Reduzir o consumo de energia do sistema de IA",
"correct_answers": ["B"],
"explanation_detailed": "Auditorias humanas revisam processos, dados, modelos e saídas para verificar conformidade, ética e desempenho real. Incluem amostragem de casos, replicação de resultados, checagem de logs e validação de políticas. Em AWS, extraia trilhas com CloudTrail, métricas no CloudWatch, relatórios do Clarify e documentação de Model Cards; colete revisões via A2I. Auditorias independentes aumentam confiança e cumprem requisitos regulatórios.",
"incorrect_explanations": {
"A": "Objetivo é fiscalizar e validar, não substituir toda automação.",
"C": "Velocidade não é alvo; é qualidade e conformidade.",
"D": "Energia não é foco de auditoria de IA por si."
}
},
{
"id": "aif-c01-responsible_ai-181",
"certification_id": "AIF-C01",
"domain": "RESPONSIBLE_AI",
"difficulty": "easy",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "Qual serviço da AWS é usado principalmente para gerenciar acesso e permissões para sistemas de IA?",
"option_a": "Amazon S3",
"option_b": "AWS IAM",
"option_c": "Amazon EC2",
"option_d": "Amazon RDS",
"correct_answers": ["B"],
"explanation_detailed": "AWS Identity and Access Management (IAM) define usuários, funções, políticas e permissões granulares para recursos (S3, SageMaker, Bedrock, KMS). É base de segurança “na” nuvem. Combine com KMS para chaves, CloudTrail para auditoria e SCPs/Organizations para controle em escala. Controle acesso a dados de treino, endpoints e artefatos de modelo. S3/EC2/RDS são serviços de dados/compute, não o mecanismo central de autorização.",
"incorrect_explanations": {
"A": "S3 armazena objetos; permissões são regidas via IAM/buckets policies.",
"C": "EC2 é compute; não gerencia identidades/permissões.",
"D": "RDS é banco relacional; acesso é mediado por IAM/credenciais."
}
},
{
"id": "aif-c01-responsible_ai-182",
"certification_id": "AIF-C01",
"domain": "RESPONSIBLE_AI",
"difficulty": "medium",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "Qual é o objetivo principal do Amazon Macie na segurança de IA?",
"option_a": "Gerar modelos de IA",
"option_b": "Descobrir e proteger dados confidenciais",
"option_c": "Aumentar o desempenho do modelo",
"option_d": "Reduzir o consumo de energia",
"correct_answers": ["B"],
"explanation_detailed": "Amazon Macie identifica automaticamente PII e dados sensíveis em buckets S3 usando ML e correspondência de padrões, sinalizando exposições e mudanças de política. Integre com CloudWatch/CloudTrail para alertas, aplique KMS/IAM/Lake Formation para correções e registre remediações. Em pipelines de IA, Macie ajuda a garantir que dados usados em treino/inferência estejam adequadamente protegidos.",
"incorrect_explanations": {
"A": "Macie não treina modelos para você; foca descoberta/proteção de dados.",
"C": "Desempenho de modelo não é escopo do Macie.",
"D": "Energia não é alvo; é segurança de dados."
}
},
{
"id": "aif-c01-responsible_ai-183",
"certification_id": "AIF-C01",
"domain": "RESPONSIBLE_AI",
"difficulty": "easy",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "A que se refere o modelo de responsabilidade compartilhada da AWS?",
"option_a": "Compartilhamento de modelos de IA entre clientes",
"option_b": "Divisão de responsabilidades de segurança entre a AWS e o cliente",
"option_c": "Compartilhamento de custos entre a AWS e o cliente",
"option_d": "Divisão de tarefas de IA entre humanos e máquinas",
"correct_answers": ["B"],
"explanation_detailed": "AWS protege a infraestrutura global (“da” nuvem); clientes protegem dados, configurações, identidades e aplicações (“na” nuvem). Para IA, clientes definem políticas de dados, chaves (KMS), acessos (IAM), guardrails e auditoria (CloudTrail). A AWS oferece serviços seguros, mas configuração e operação correta são do cliente. Alinhe responsabilidades em documentação e Model Cards quando modelos expõem riscos específicos.",
"incorrect_explanations": {
"A": "Não trata de troca de modelos entre clientes.",
"C": "Custeio não define segurança e governança.",
"D": "Humano vs. máquina é design de produto, não o modelo de responsabilidade."
}
},
{
"id": "aif-c01-responsible_ai-184",
"certification_id": "AIF-C01",
"domain": "RESPONSIBLE_AI",
"difficulty": "easy",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "Qual das seguintes opções NÃO é um método típico para proteger sistemas de IA?",
"option_a": "Criptografia",
"option_b": "Controle de acesso",
"option_c": "Compartilhamento público de dados",
"option_d": "Gerenciamento de vulnerabilidades",
"correct_answers": ["C"],
"explanation_detailed": "Proteção inclui criptografia (KMS), controle de acesso (IAM/Lake Formation), varredura de vulnerabilidades (Inspector), monitoração (GuardDuty/CloudTrail), e segmentação de rede (VPC/WAF). Compartilhar dados publicamente amplifica risco. Em IA, aplique guardrails no Bedrock, sanitização em RAG e revisão humana (A2I) em decisões críticas. Mantenha trilha de auditoria e respostas a incidentes documentadas.",
"incorrect_explanations": {
"A": "Criptografia é base para confidencialidade e compliance.",
"B": "Acesso mínimo necessário reduz superfícies de ataque.",
"D": "Vulnerabilidades não tratadas expõem dados e pipelines."
}
},
{
"id": "aif-c01-responsible_ai-185",
"certification_id": "AIF-C01",
"domain": "RESPONSIBLE_AI",
"difficulty": "medium",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "O que é linhagem de dados no contexto da segurança de IA?",
"option_a": "Um método de criptografia de dados",
"option_b": "Rastrear a origem e as transformações dos dados",
"option_c": "Um tipo de arquitetura de modelo de IA",
"option_d": "Uma maneira de aumentar a velocidade de processamento de dados",
"correct_answers": ["B"],
"explanation_detailed": "Linhagem de dados mapeia de onde os dados vieram, quais transformações sofreram e quem acessou/alterou. Essencial para auditoria, reproduzibilidade e investigação de incidentes. Em AWS, catalogue no Glue Data Catalog, governe com Lake Formation, registre jobs no Glue/Athena e use CloudTrail/CloudWatch para trilhas de acesso e execução. Para IA, vincule datasets/versões no SageMaker Model Registry e documente em Model Cards.",
"incorrect_explanations": {
"A": "Criptografia protege; linhagem explica trajetória e uso.",
"C": "Arquitetura de modelo não é rastreamento de dados.",
"D": "Velocidade é tema de performance, não de linhagem."
}
},
{
"id": "aif-c01-responsible_ai-186",
"certification_id": "AIF-C01",
"domain": "RESPONSIBLE_AI",
"difficulty": "medium",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "Qual serviço da AWS é usado para detectar ameaças à segurança em sistemas de IA?",
"option_a": "Amazon Macie",
"option_b": "Amazon S3",
"option_c": "Amazon EC2",
"option_d": "Amazon RDS",
"correct_answers": ["A"],
"explanation_detailed": "Macie encontra PII e dados sensíveis expostos em S3 e alerta sobre configurações arriscadas, o que ajuda a prevenir vazamentos e abusos em pipelines de IA. Combine com CloudWatch/CloudTrail para alarmes e com Lake Formation/IAM/KMS para correções. Para detecção de ameaças em contas e tráfego, complementa-se com GuardDuty; para vulnerabilidades em workloads, use Inspector. O foco aqui é proteção de dados sensíveis usados pela IA.",
"incorrect_explanations": {
"B": "S3 armazena; não detecta ameaças por si.",
"C": "EC2 é computação, não um serviço de detecção de ameaças.",
"D": "RDS é banco relacional; não faz varredura de segurança."
}
},
{
"id": "aif-c01-responsible_ai-187",
"certification_id": "AIF-C01",
"domain": "RESPONSIBLE_AI",
"difficulty": "medium",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "O que é injeção de prompt no contexto da segurança de IA?",
"option_a": "Um método de otimização de prompts",
"option_b": "Uma vulnerabilidade de segurança onde uma entrada maliciosa manipula o comportamento da IA",
"option_c": "Uma técnica para acelerar o processamento da IA",
"option_d": "Uma maneira de reduzir o consumo de energia da IA",
"correct_answers": ["B"],
"explanation_detailed": "Injeção de prompt engana o modelo a ignorar instruções originais, seguindo comandos maliciosos presentes na entrada ou em documentos recuperados (RAG). Mitigue com delimitação clara de papéis, saneamento de conteúdo, allow/deny lists, validações pós-geração, e Guardrails for Amazon Bedrock para filtrar instruções proibidas/PII. Monitore uso e incidentes via CloudTrail/CloudWatch e restrinja fontes confiáveis com Kendra/OpenSearch com metadados.",
"incorrect_explanations": {
"A": "Otimizar prompt visa qualidade; injeção é ataque.",
"C": "Não acelera; altera o comportamento para fins maliciosos.",
"D": "Energia não é afetada diretamente; é risco de integridade."
}
},
{
"id": "aif-c01-responsible_ai-188",
"certification_id": "AIF-C01",
"domain": "RESPONSIBLE_AI",
"difficulty": "easy",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "Qual das seguintes opções NÃO é um padrão típico de conformidade regulatória para sistemas de IA?",
"option_a": "ISO",
"option_b": "SOC",
"option_c": "HTML",
"option_d": "Leis de responsabilidade de algoritmo",
"correct_answers": ["C"],
"explanation_detailed": "Conformidade envolve padrões como ISO/IEC (ex.: 27001), relatórios SOC e legislações de responsabilidade algorítmica. HTML é linguagem de marcação, não norma de compliance. Em AWS, acesse relatórios em AWS Artifact, registre auditorias com CloudTrail, monitore configurações com Config e colete evidências com Audit Manager. Mantenha Model Cards, políticas e trilhas para demonstrar conformidade contínua.",
"incorrect_explanations": {
"A": "ISO abrange práticas de segurança/gestão; é relevante.",
"B": "SOC avalia controles organizacionais; é padrão aceito.",
"D": "Regulações algorítmicas são cada vez mais exigidas."
}
},
{
"id": "aif-c01-responsible_ai-189",
"certification_id": "AIF-C01",
"domain": "RESPONSIBLE_AI",
"difficulty": "easy",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "Qual serviço da AWS é usado para monitoramento e avaliação contínuos de recursos?",
"option_a": "Amazon EC2",
"option_b": "AWS Config",
"option_c": "Amazon S3",
"option_d": "Amazon RDS",
"correct_answers": ["B"],
"explanation_detailed": "AWS Config rastreia configurações de recursos e as compara a regras/políticas, gerando histórico e notificações de desvio. É útil para governança de pipelines de IA (S3, SageMaker, KMS, IAM), assegurando que ambientes permaneçam dentro de padrões definidos. Integre com CloudWatch Events/Rules para automações e com CloudTrail para auditorias completas.",
"incorrect_explanations": {
"A": "EC2 executa cargas; monitoramento de conformidade é do Config.",
"C": "S3 armazena; não avalia compliance de recursos.",
"D": "RDS é banco; não audita configurações globais."
}
},
{
"id": "aif-c01-responsible_ai-190",
"certification_id": "AIF-C01",
"domain": "RESPONSIBLE_AI",
"difficulty": "easy",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "Qual é o objetivo principal do AWS Artifact na governança de IA?",
"option_a": "Gerar modelos de IA",
"option_b": "Fornecer acesso a relatórios de conformidade da AWS",
"option_c": "Aumentar o desempenho do modelo",
"option_d": "Reduzir o consumo de energia",
"correct_answers": ["B"],
"explanation_detailed": "AWS Artifact disponibiliza relatórios de conformidade (ISO, SOC, PCI, etc.) e acordos para apoiar auditorias e due diligence. Combine com Audit Manager para coletar evidências de controles internos, CloudTrail para trilhas de API e Config para postura de recursos. Use esses artefatos para demonstrar governança de soluções de IA que dependem da infraestrutura AWS.",
"incorrect_explanations": {
"A": "Artifact não treina modelos; é repositório de compliance.",
"C": "Desempenho de modelo não é objetivo do Artifact.",
"D": "Energia não é foco; é conformidade."
}
},
{
"id": "aif-c01-responsible_ai-191",
"certification_id": "AIF-C01",
"domain": "RESPONSIBLE_AI",
"difficulty": "easy",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "Qual das seguintes opções NÃO faz parte normalmente de uma estratégia de governança de dados?",
"option_a": "Gerenciamento do ciclo de vida dos dados",
"option_b": "Políticas de retenção de dados",
"option_c": "Políticas de compartilhamento público de dados",
"option_d": "Monitoramento de dados",
"correct_answers": ["C"],
"explanation_detailed": "Governança de dados define papéis, políticas de acesso, retenção, qualidade e monitoramento, priorizando segurança e conformidade. Compartilhamento público indiscriminado contraria princípios de proteção. Em AWS, implemente Lake Formation/IAM/KMS, catálogos no Glue e auditoria via CloudTrail/Config. Documente processos e mantenha evidências com Audit Manager.",
"incorrect_explanations": {
"A": "Ciclo de vida (criação, uso, retenção, exclusão) é central.",
"B": "Retenção atende leis e minimiza risco.",
"D": "Monitoramento detecta desvios e incidentes."
}
},
{
"id": "aif-c01-responsible_ai-192",
"certification_id": "AIF-C01",
"domain": "RESPONSIBLE_AI",
"difficulty": "easy",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "Qual é o objetivo principal do AWS CloudTrail na governança de IA?",
"option_a": "Gerar modelos de IA",
"option_b": "Registrar chamadas de API e atividade da conta",
"option_c": "Aumentar o desempenho do modelo",
"option_d": "Reduzir o consumo de energia",
"correct_answers": ["B"],
"explanation_detailed": "CloudTrail registra quem fez o quê, quando e de onde, criando trilha de auditoria para recursos (S3, SageMaker, Bedrock, IAM). Essencial para investigações, compliance e controles de mudança. Integre com CloudWatch Logs/Events para alertas e respostas automáticas. Para governança, combine com Config, Audit Manager e Model Cards para visão completa do ciclo de vida do modelo.",
"incorrect_explanations": {
"A": "Não treina modelos; fornece auditoria de API.",
"C": "Desempenho não é foco do CloudTrail.",
"D": "Consumo energético não é monitorado por CloudTrail."
}
},
{
"id": "aif-c01-responsible_ai-193",
"certification_id": "AIF-C01",
"domain": "RESPONSIBLE_AI",
"difficulty": "medium",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "Qual das seguintes opções é uma consideração chave na engenharia de dados segura para IA?",
"option_a": "Maximizar a coleta de dados sem considerar a qualidade",
"option_b": "Implementar tecnologias de melhoria de privacidade",
"option_c": "Tornar todos os dados publicamente acessíveis",
"option_d": "Usar apenas armazenamento de dados não criptografado",
"correct_answers": ["B"],
"explanation_detailed": "Engenharia segura inclui criptografia (KMS), controle de acesso (IAM/Lake Formation), mascaramento/anonimização, minimização de dados e segregação de ambientes. Em AWS, catalogue no Glue, armazene no S3 com políticas, audite com CloudTrail e monitore postura com Config. Tecnologias de melhoria de privacidade reduzem risco e viés de exposição sem bloquear casos de uso legítimos.",
"incorrect_explanations": {
"A": "Coletar sem critério aumenta risco e custo, sem garantir qualidade.",
"C": "Acesso público é o oposto de segurança de dados.",
"D": "Armazenar sem criptografia viola melhores práticas."
}
},
{
"id": "aif-c01-responsible_ai-194",
"certification_id": "AIF-C01",
"domain": "RESPONSIBLE_AI",
"difficulty": "medium",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "Qual é o objetivo principal da Matriz de Escopo de Segurança de IA Generativa?",
"option_a": "Gerar modelos de IA",
"option_b": "Fornecer uma estrutura para avaliar os riscos de segurança de IA",
"option_c": "Aumentar o desempenho do modelo",
"option_d": "Reduzir o consumo de energia",
"correct_answers": ["B"],
"explanation_detailed": "Uma matriz de escopo de segurança de IA generativa mapeia riscos por camadas (dados, modelo, orquestração, interface) e por responsabilidades (equipe, fornecedor, cliente), guiando controles: guardrails, PII, proveniência, detecção de abuso e auditoria. Em AWS, aplique Bedrock Guardrails, IAM/KMS, CloudTrail/Config, Kendra/OpenSearch com metadados e A2I para revisão humana. O objetivo é avaliar e priorizar mitigação sistematicamente.",
"incorrect_explanations": {
"A": "Não cria modelos; estrutura avaliação de risco.",
"C": "Desempenho não é foco primário; é segurança.",
"D": "Energia não é o alvo da matriz de segurança."
}
},
{
"id": "aif-c01-responsible_ai-195",
"certification_id": "AIF-C01",
"domain": "RESPONSIBLE_AI",
"difficulty": "easy",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "Qual serviço da AWS é usado para avaliações automatizadas de segurança?",
"option_a": "Amazon EC2",
"option_b": "Amazon Inspector",
"option_c": "Amazon S3",
"option_d": "Amazon RDS",
"correct_answers": ["B"],
"explanation_detailed": "Amazon Inspector avalia continuamente workloads (EC2, ECR, Lambda) para vulnerabilidades conhecidas e exposição de rede. Em ambientes de IA, ele ajuda a manter imagens limpas e hosts seguros para pipelines de dados/treino. Combine com GuardDuty para detecção de ameaças, Config para postura e CloudTrail para auditoria. Corrija achados com automações e controle de mudanças.",
"incorrect_explanations": {
"A": "EC2 é compute; Inspector é quem avalia vulnerabilidades.",
"C": "S3 armazena dados; não realiza checagens de CVEs.",
"D": "RDS não faz varreduras de segurança de workloads."
}
},
{
"id": "aif-c01-responsible_ai-196",
"certification_id": "AIF-C01",
"domain": "RESPONSIBLE_AI",
"difficulty": "easy",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "O que é residência de dados no contexto da governança de IA?",
"option_a": "O local físico onde os dados são armazenados",
"option_b": "A duração pela qual os dados são mantidos",
"option_c": "A velocidade na qual os dados são processados",
"option_d": "O formato no qual os dados são armazenados",
"correct_answers": ["A"],
"explanation_detailed": "Residência de dados especifica a região/país onde os dados ficam armazenados e sob quais leis se enquadram. Em AWS, escolha regiões, use políticas do S3/Lake Formation e restrições de replicação, e documente requisitos em Model Cards e políticas. Para IA, confirme que dados de treino/inferência cumprem regras de localização e transferência.",
"incorrect_explanations": {
"B": "Isso é retenção, não residência.",
"C": "Velocidade é performance; residência é localização legal.",
"D": "Formato descreve esquema/serialização, não local."
}
},
{
"id": "aif-c01-responsible_ai-197",
"certification_id": "AIF-C01",
"domain": "RESPONSIBLE_AI",
"difficulty": "easy",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "Qual das seguintes opções NÃO é uma consideração típica na segurança de aplicações de IA?",
"option_a": "Detecção de ameaças",
"option_b": "Gerenciamento de vulnerabilidades",
"option_c": "Maximização do compartilhamento público de dados",
"option_d": "Proteção da infraestrutura",
"correct_answers": ["C"],
"explanation_detailed": "Segurança de aplicações de IA exige monitorar ameaças (GuardDuty), gerenciar vulnerabilidades (Inspector), proteger infraestrutura (VPC, WAF, IAM/KMS) e controlar dados (Macie, Lake Formation). Compartilhar dados publicamente vai contra confidencialidade e compliance. Adicione guardrails no Bedrock e trilhas de auditoria (CloudTrail).",
"incorrect_explanations": {
"A": "Detecção de ameaças é pilar essencial.",
"B": "Vulnerabilidades não tratadas expõem o ambiente.",
"D": "Infraestrutura protegida reduz superfície de ataque."
}
},
{
"id": "aif-c01-responsible_ai-198",
"certification_id": "AIF-C01",
"domain": "RESPONSIBLE_AI",
"difficulty": "easy",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "Qual é o objetivo principal do AWS Trusted Advisor na governança de IA?",
"option_a": "Gerar modelos de IA",
"option_b": "Fornecer orientação em tempo real para melhorar o ambiente da AWS",
"option_c": "Aumentar o desempenho do modelo",
"option_d": "Reduzir o consumo de energia",
"correct_answers": ["B"],
"explanation_detailed": "Trusted Advisor recomendações para custos, desempenho, segurança, tolerância a falhas e limites de serviço. Em ambientes de IA, ajuda a manter contas saudáveis: permissões excessivas, recursos subutilizados, backups e cotas. Combine com Config/CloudTrail para governança contínua e com Guardrails/Clarify para camadas de responsabilidade específicas de IA.",
"incorrect_explanations": {
"A": "Não treina modelos; avalia melhores práticas de conta.",
"C": "Desempenho de modelo não é alvo; é postura de ambiente.",
"D": "Energia não é foco direto do Trusted Advisor."
}
},
{
"id": "aif-c01-responsible_ai-199",
"certification_id": "AIF-C01",
"domain": "RESPONSIBLE_AI",
"difficulty": "easy",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "Qual das seguintes opções é um aspecto chave da integridade de dados em sistemas de IA?",
"option_a": "Garantir que os dados permaneçam inalterados e não corrompidos",
"option_b": "Tornar todos os dados publicamente acessíveis",
"option_c": "Usar apenas os maiores conjuntos de dados disponíveis",
"option_d": "Armazenar todos os dados em um único local",
"correct_answers": ["A"],
"explanation_detailed": "Integridade garante que dados não sejam alterados indevidamente. Use versionamento (S3), checksums, logs de acesso (CloudTrail), criptografia (KMS) e políticas de alteração controladas (Lake Formation/IAM). Para IA, vincule datasets e modelos no Model Registry e documente transformações no Glue para rastrear coerência entre dados e resultados.",
"incorrect_explanations": {
"B": "Acesso público compromete confidencialidade e integridade.",
"C": "Tamanho não assegura dados íntegros.",
"D": "Concentrar em um local cria risco; integridade requer controle, não centralização cega."
}
},
{
"id": "aif-c01-responsible_ai-200",
"certification_id": "AIF-C01",
"domain": "RESPONSIBLE_AI",
"difficulty": "easy",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "Qual é o objetivo principal da criptografia em repouso na segurança de IA?",
"option_a": "Proteger os dados enquanto estão sendo transmitidos",
"option_b": "Proteger os dados armazenados",
"option_c": "Aumentar a velocidade de processamento de dados",
"option_d": "Reduzir o consumo de energia",
"correct_answers": ["B"],
"explanation_detailed": "Criptografia em repouso protege dados armazenados (S3, EBS, RDS) usando chaves gerenciadas pelo AWS KMS. Em pipelines de IA, garanta que datasets, artefatos de treino e modelos estejam criptografados. Combine com criptografia em trânsito (TLS), IAM para acesso mínimo necessário, e auditoria via CloudTrail. Isso reduz risco de exposição em perdas de mídia ou acessos indevidos.",
"incorrect_explanations": {
"A": "Em trânsito é TLS; repouso é armazenamento.",
"C": "Criptografia não acelera processamento; protege confidencialidade.",
"D": "Energia não é alvo da criptografia; é controle de segurança."
}
},
{
"id": "aif-c01-responsible_ai-201",
"certification_id": "AIF-C01",
"domain": "RESPONSIBLE_AI",
"difficulty": "medium",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "Qual das seguintes opções NÃO faz parte normalmente dos protocolos de governança para sistemas de IA?",
"option_a": "Revisões regulares de políticas",
"option_b": "Requisitos de treinamento de equipe",
"option_c": "Maximização da complexidade do modelo",
"option_d": "Padrões de transparência",
"correct_answers": ["C"],
"explanation_detailed": "Governança define políticas, forma equipes, cria padrões de transparência (Model Cards), executa auditorias periódicas e mede riscos. Complexidade do modelo não é meta de governança. Em AWS, use Audit Manager, Artifact, CloudTrail, Config e registros de Clarify/Model Monitor para evidências. Revise periodicamente guardrails e fluxos de revisão humana.",
"incorrect_explanations": {
"A": "Revisões mantêm políticas aderentes a riscos reais.",
"B": "Treinamento capacita operação segura e ética.",
"D": "Transparência é pilar para confiança e auditoria."
}
},
{
"id": "aif-c01-responsible_ai-202",
"certification_id": "AIF-C01",
"domain": "RESPONSIBLE_AI",
"difficulty": "medium",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "Qual é o objetivo principal do AWS Audit Manager na governança de IA?",
"option_a": "Gerar modelos de IA",
"option_b": "Auditar continuamente o uso da AWS para conformidade",
"option_c": "Aumentar o desempenho do modelo",
"option_d": "Reduzir o consumo de energia",
"correct_answers": ["B"],
"explanation_detailed": "AWS Audit Manager coleta evidências automaticamente de múltiplos serviços (CloudTrail, Config, etc.) para avaliar conformidade contra frameworks. Em IA, isso suporta auditorias de dados, acesso, treinamento e inferência. Combine com Artifact para relatórios oficiais, com Model Cards para documentação de modelos e com Clarify/Model Monitor para métricas anexas.",
"incorrect_explanations": {
"A": "Audit Manager não treina modelos; coleta evidências.",
"C": "Desempenho de modelo não é escopo do serviço.",
"D": "Energia não é foco; é compliance."
}
},
{
"id": "aif-c01-responsible_ai-203",
"certification_id": "AIF-C01",
"domain": "RESPONSIBLE_AI",
"difficulty": "medium",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "Qual das seguintes opções é uma consideração chave na proteção da infraestrutura de IA?",
"option_a": "Maximizar o acesso público aos sistemas de IA",
"option_b": "Implementar medidas de segurança de rede",
"option_c": "Usar apenas os maiores modelos disponíveis",
"option_d": "Armazenar todos os dados em um único local",
"correct_answers": ["B"],
"explanation_detailed": "Segurança de rede inclui segmentação (VPC), listas de controle (NACLs), Security Groups, WAF e proteção contra DDoS. Em pipelines de IA, isole endpoints SageMaker/Bedrock atrás de camadas de API e autenticação, registre tráfego (VPC Flow Logs) e audite com CloudTrail. Combine com IAM/KMS e monitoramento contínuo para reduzir superfícies de ataque.",
"incorrect_explanations": {
"A": "Acesso público aumenta risco de exploração.",
"C": "Tamanho do modelo não protege infraestrutura.",
"D": "Centralizar tudo eleva risco; prefira controles e redundância."
}
},
{
"id": "aif-c01-responsible_ai-204",
"certification_id": "AIF-C01",
"domain": "RESPONSIBLE_AI",
"difficulty": "easy",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "Qual é o objetivo principal do catálogo de dados na governança de IA?",
"option_a": "Tornar todos os dados publicamente acessíveis",
"option_b": "Organizar e inventariar ativos de dados",
"option_c": "Aumentar a velocidade de processamento de dados",
"option_d": "Reduzir os custos de armazenamento de dados",
"correct_answers": ["B"],
"explanation_detailed": "Um catálogo (ex.: AWS Glue Data Catalog) descreve conjuntos de dados, esquemas, localizações, proprietários e classificações de sensibilidade. Facilita descoberta, controle de acesso (Lake Formation/IAM) e auditoria (CloudTrail). Para IA, vincule datasets a modelos no Model Registry e documente em Model Cards para rastreabilidade ponta a ponta.",
"incorrect_explanations": {
"A": "Catálogo não implica acesso público; ajuda governança.",
"C": "Velocidade depende de arquitetura e compute, não do catálogo.",
"D": "Custos podem melhorar indiretamente, mas não é o objetivo central."
}
},
{
"id": "aif-c01-responsible_ai-205",
"certification_id": "AIF-C01",
"domain": "RESPONSIBLE_AI",
"difficulty": "easy",
"tier": "FREE",
"required_selection_count": 1,
"active": true,
"question_text": "Qual das seguintes opções NÃO é tipicamente um componente de uma estratégia de gerenciamento do ciclo de vida dos dados?",
"option_a": "Criação de dados",
"option_b": "Retenção de dados",
"option_c": "Exclusão de dados",
"option_d": "Compartilhamento público de dados",
"correct_answers": ["D"],
"explanation_detailed": "Gerenciamento do ciclo de vida abrange criação, ingestão, classificação, armazenamento, uso, retenção e exclusão segura. Compartilhar publicamente não é componente padrão. Em AWS, combine S3 Lifecycle, Lake Formation/IAM, KMS, Glue Catalog e auditorias com CloudTrail/Config. Documente processos para auditorias e conformidade.",
"incorrect_explanations": {
"A": "Criação/ingestão iniciam o ciclo e precisam de governança.",
"B": "Retenção cumpre leis e políticas.",
"C": "Exclusão controlada evita retenção indevida e riscos."
}
},
{
"id": "aif-c01-ml_development-206",
"certification_id": "AIF-C01",
"domain": "ML_DEVELOPMENT",
"difficulty": "medium",
"tier": "FREE",
"required_selection_count": 2,
"active": true,
"question_text": "Uma empresa implantou um modelo de detecção de fraudes e quer garantir precisão contínua e revisão humana quando necessário. Quais serviços ou recursos da AWS devem ser usados? (Selecione DUAS.)",
"option_a": "Amazon SageMaker Model Monitor",
"option_b": "Amazon A2I (Amazon Augmented AI)",
"option_c": "Amazon Bedrock",
"option_d": "Amazon SageMaker Ground Truth",
"correct_answers": ["A", "B"],
"explanation_detailed": "SageMaker Model Monitor detecta desvios de dados e de qualidade em endpoints em produção, gerando métricas e alertas quando distribuições mudam ou quando a performance degrada. Isso permite acionar reentreinos ou revisões. Amazon A2I insere revisão humana sob critérios (baixa confiança, amostragem periódica, casos de risco) para validar previsões, coletar feedback e criar conjuntos de erro para correção. Bedrock é útil para IA generativa, mas não é o serviço específico de monitoramento desse cenário. Ground Truth é para rotulagem de dados; relevante para criar datasets, mas não monitora endpoints em produção nem orquestra revisão de predições em tempo real.",
"incorrect_explanations": {
"C": "Bedrock orquestra FMs; não monitora métricas de um modelo de fraude existente.",
"D": "Ground Truth rotula dados; não monitora drift de endpoint nem dispara revisão humana em produção."
}
}
]

